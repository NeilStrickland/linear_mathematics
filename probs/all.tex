\documentclass[a4paper]{amsart}
\usepackage{fullpage}
\usepackage{verbatim}
\usepackage{tikz}
\usepackage{xr}

\externaldocument{../notes/MAS201}
\usepackage[type={CC},modifier={by-nc-sa},version={3.0}]{doclicense}

\input{../macros/macros}
\input{soldefs}

\title{Problems for MAS201 (Linear Mathematics for Applications)}

\def\SOLS{1}

\ifx\SOLS\undefined\else
\renewenvironment{solution}{\SolutionInline}{\endSolutionInline}
\fi

\begin{document}

\maketitle

\begin{center}
 This work is licensed under a 
 \href{https://creativecommons.org/licenses/by-nc-sa/3.0/deed.en}{
  Creative Commons Attribution-NonCommercial-ShareAlike license}.
 
 \bigskip

 \doclicenseImage 
\end{center}

\newpage

\section{Lecture 1}

\begin{exercise}\label{ex-mat-prod-i}
 Calculate $AB$, where 
 \[ A = \bbm 2&0&0&0 \\ 3&2&0&0 \\ 3&6&2&0 \\ 3&6&3&2 \ebm 
    \hspace{4em}
    B = \bbm 4&2&2&2 \\ 0&1&-1&-1 \\ 0&0&4&2 \\ 0&0&0&1 \ebm
 \]
\end{exercise}
\begin{solution}
 \[ AB = \bbm  8 &  4 &  4 &  4 \\
              12 &  8 &  4 &  4 \\
              12 & 12 &  8 &  4 \\
              12 & 12 & 12 &  8.
         \ebm 
 \]
\end{solution}

\begin{exercise}\label{ex-mat-prod-ii}
 Consider the following matrices:
 \[ A = \bbm 1&2&3&4 \\ 4&3&2&1 \ebm \qquad
    B = \bbm 1 & 10 \\ 100 & 1000 \ebm \qquad
    C = \bbm 1 & 0 \\ 11 & 0 \\ 111 & 0 \ebm.
 \]
 For each of the following products, either evaluate the product or
 explain why it is undefined:
 \[ A^2 \qquad AB \qquad AC \qquad
    BA \qquad B^2 \qquad BC \qquad
    CA \qquad CB \qquad C^2
 \]
\end{exercise}
\begin{solution}
 The products that are defined are as follows:
 \begin{align*}
  BA &= \bbm 41 & 32 & 23 & 14 \\ 4100 & 3200 & 2300 & 1400 \ebm \\
  B^2 &= \bbm 1001 & 10010 \\ 100100 & 1001000 \ebm \\
  CA &= \bbm 1 & 2 & 3 & 4 \\
             11 & 22 & 33 & 44 \\
             111 & 222 & 333 & 444 \ebm \\
  CB &= \bbm 1 & 10 \\ 11 & 110 \\ 111 & 1110 \ebm.
 \end{align*}
 The other products are undefined.  For example, $A$ is a $2\tm 4$
 matrix (with $4$ columns) and $B$ is a $2\tm 2$ matrix ( with $2$
 rows).  As the numer of columns in $A$ is different from the number
 of rows in $B$, we cannot define the product $AB$.  
\end{solution}

\begin{exercise}\label{ex-mat-prod-eg-i}
 Find examples as follows.
 \begin{itemize}
  \item[(a)] Matrices $A$ and $B$ such that $AB$ is defined but $BA$
   is not.
  \item[(b)] Matrices $C$ and $D$ such that $CD$ and $DC$ are both
   defined but have different sizes.
  \item[(c)] Matrices $E$ and $F$ such that $EF$ and $FE$ are both
   defined and have the same size but are not equal.
  \item[(d)] Matrices $G$ and $H$ such that $GH$ and $HG$ are both
   defined and have the same size and are equal.
 \end{itemize}
\end{exercise}
\begin{solution}
 In each case there are many possible examples.  We will give a
 selection.
 \begin{itemize}
  \item[(a)] Here $A$ must be an $m\tm n$ matrix and $B$ must be an
   $n\tm p$ matrix where $m$ and $p$ are different.  We could take
   $A=\bbm 1&2\\3&4\ebm$ (a $2\tm 2$ matrix) and $B=\bbm
   1&2&3\\4&5&6\ebm$ (a $2\tm 3$ matrix).  The entries in these
   matrices are not really relevant, only the shape matters.  We could
   therefore simplify things by taking $A=\bbm 0&0\\0&0\ebm$ and
   $B=\bbm 0&0&0\\0&0&0\ebm$.  For an even more minimalist example, we
   could take $A=\bbm 0 \ebm$ (a $1\tm 1$ matrix) and $B=\bbm 0&0\ebm$
   (a $1\tm 2$ matrix).
  \item[(b)] Here $C$ must be an $m\tm n$ matrix and $D$ must be an
   $n\tm m$ matrix for some integers $m$ and $n$ with $m\neq n$.
   For a realistic example, we can take
   \[ C=\bbm 1&1&1\\2&2&2\ebm \hspace{4em}
      D=\bbm 3&4\\3&4\\3&4\ebm
   \]
   giving
   \[ CD = \bbm 9 & 12 \\ 18 & 24 \ebm \hspace{4em}
      DC = \bbm 11&11&11 \\ 11&11&11 \\ 11&11&11 \ebm.
   \]
   For a minimalist example we can take
   \[ C = \bbm 0 & 0 \ebm \qquad
      D = \bbm 0 \\ 0 \ebm \qquad
      CD = \bbm 0 \ebm \qquad
      DC = \bbm 0 & 0 \\ 0 & 0 \ebm.
   \]
  \item[(c)] Here $E$ and $F$ must be square matrices of shape
   $n\tm n$ for some $n>1$.  If we choose a pair of $2\tm 2$ matrices
   at random then it will usually work.  For example, we could have 
   \[ E = \bbm 1&5\\3&2 \ebm \qquad
      F = \bbm 3&1\\4&6 \ebm \qquad
      EF = \bbm 23 & 31 \\ 17 & 15 \ebm \qquad
      FE = \bbm 6 & 17 \\ 22 & 32 \ebm.
   \]
   For a minimal example, we have
   \[ E  = \bbm 1&0\\0&0 \ebm \qquad
      F  = \bbm 0&1\\0&0 \ebm \qquad
      EF = \bbm 0&1\\0&0 \ebm \qquad
      FE = \bbm 0&0\\0&0 \ebm.
   \]
  \item[(d)] Here $G$ and $H$ must be square matrices of the same
   size, say $n\tm n$.  We can take $G$ to be the zero matrix and $H$
   to be any $n\tm n$ matrix, and then we have $GH=0=HG$, so this
   gives an example.  Alternatively, we can take $G$ to be the
   identity matrix $I_n$ and $H$ to be any $n\tm n$ matrix, and then
   we have $GH=H=HG$, so this gives another example.  Yet another
   possibility is to let $H$ be any $n\tm n$ matrix and then take
   $G=H$, so that $GH=HG=H^2$.  For a minimal example, we can take
   $n=1$ and $G=H=\bbm 0 \ebm$.
 \end{itemize}
\end{solution}

\begin{exercise}\label{ex-mat-prod-eg-ii}
 Find a nonzero matrix $A$ such that $A^2$ is defined and is zero.
\end{exercise}
\begin{solution}
 We could take $A=\bbm 0&1\\0&0\ebm$ or $A=\bbm 1&1\\-1&-1\ebm$.
\end{solution}

\begin{exercise}\label{ex-commutator-trace}
 The \emph{trace} of a square matrix is the sum of the diagonal
 entries.  Show that if $A=\bbm a&b\\c&d\ebm$ and
 $B=\bbm p&q\\r&s\ebm$ then the trace of $AB-BA$ is zero.
\end{exercise}
\begin{solution}
 \begin{align*}
  AB &= \bbm a&b\\c&d\ebm \bbm p&q\\r&s\ebm
      = \bbm ap+br & aq+bs \\ cp+dr & cq+ds \ebm \\
  BA &= \bbm p&q\\r&s\ebm \bbm a&b\\c&d\ebm
      = \bbm ap+cq & bp+dq \\ ar+cs & br+ds \ebm \\
  AB-BA &= \bbm ap+br & aq+bs \\ cp+dr & cq+ds \ebm -
           \bbm ap+cq & bp+dq \\ ar+cs & br+ds \ebm \\
        &= \bbm br-cq & aq+bs-bp-dq \\ cp+dr-ar-cs & cq-br \ebm \\
  \text{trace}(AB-BA) &= (br-cq) + (cq-br) = 0. 
 \end{align*}
\end{solution}

\section{Lecture 2}

\begin{exercise}\label{ex-which-rref-i}
 Which of the following matrices are in reduced row-echelon form?
 \[ 
  A = \bbm 0&0&0&0 \\ 0&1&2&0 \\ 0&0&0&1 \ebm \hspace{3em}
  B = \bbm 0&0&0&1 \\ 0&1&0&0 \\ 0&0&0&0 \ebm \hspace{3em}
  C = \bbm 1&3&0&2 \\ 0&0&1&2 \\ 0&0&0&0 \ebm
 \] \[
  D = \bbm 3&1&0&2 \\ 0&0&2&1 \\ 0&0&0&0 \ebm \hspace{3em}
  E = \bbm 1&0&0&1 \\ 0&1&0&0 \\ 0&0&0&1 \ebm
 \]
\end{exercise}
\begin{solution}
 \begin{itemize}
  \item $A$ is not in RREF because the row of zeros occurs at the top,
   instead of the bottom.
  \item $B$ is not in RREF because the pivot in the second row is to
   the left of the pivot in the first row, not to the right.
  \item $C$ is in RREF.
  \item $D$ is not in RREF because the first nonzero entry in the
   first row is equal to $3$, not $1$.  Similarly, the first nonzero
   entry in the second row is not equal to $1$.
  \item $E$ is not in RREF because the last column contains a nonzero
   entry above the pivot in the third row.
 \end{itemize}
\end{solution}

\begin{exercise}\label{ex-rref-eg-i}
 Give an example of a $4\tm 7$ matrix in RREF with pivots in columns
 $2$, $5$ and $7$ (and no other columns) and with precisely six
 nonzero entries.
\end{exercise}
\begin{solution}
 Every $4\tm 7$ matrix with pivots in the specified columns has the
 form 
 \[ A = 
    \bbm 0 & 1 & a & b & 0 & c & 0 \\
         0 & 0 & 0 & 0 & 1 & d & 0 \\
         0 & 0 & 0 & 0 & 0 & 0 & 1 \\
         0 & 0 & 0 & 0 & 0 & 0 & 0 \ebm
 \]
 for some scalars $a$, $b$, $c$ and $d$.  If all of these scalars are
 nonzero then (together with the three pivots) we would have seven
 nonzero entries in the matrix.  We want to have only six nonzero
 entries, so we can choose $a=b=c=42$ and $d=0$ (for example) giving 
 \[ A = 
    \bbm 0 & 1 & 42 & 42 & 0 & 42 & 0 \\
         0 & 0 &  0 &  0 & 1 &  0 & 0 \\
         0 & 0 &  0 &  0 & 0 &  0 & 1 \\
         0 & 0 &  0 &  0 & 0 &  0 & 0 \ebm.
 \]
\end{solution}

\begin{exercise}\label{ex-solve-i}
 Use the augmented matrix method to solve the following system of
 linear equations, or prove that there is no solution.
 \begin{align*}
  10a &= 10b+c \\
  10c+b &= 10a-9 \\
  a+100c &= 100b+11.
 \end{align*}
\end{exercise}
\begin{solution}
 We can tidy up the equations as follows:
 \[ \begin{array}{rrrl}
     10a & -10b & -c &= 0 \\
     10a & -b & -10c &= 9 \\
     a & -100b & +100c &= 11. 
    \end{array}
 \]
 Using this we can write down the augmented matrix and row reduce it
 as follows:
 \[ 
  \left[\begin{array}{ccc|c}
   10 & -10 & -1 & 0 \\
   10 & -1 & -10 & 9 \\
   1 & -100 & 100 & 11 
  \end{array}\right]
  \xra{}
  \left[\begin{array}{ccc|c}
   1 & -1 & -0.1 & 0 \\
   10 & -1 & -10 & 9 \\
   1 & -100 & 100 & 11 
  \end{array}\right]
  \xra{}
  \left[\begin{array}{ccc|c}
   1 & -1 & -0.1 & 0 \\
   0 & 9 & -9 & 9 \\
   0 & -99 & 100.1 & 11 
  \end{array}\right]
  \xra{}
  \left[\begin{array}{ccc|c}
   1 & -1 & -0.1 & 0 \\
   0 & 1 & -1 & 1 \\
   0 & -99 & 100.1 & 11 
  \end{array}\right]
 \] \[
  \xra{}
  \left[\begin{array}{ccc|c}
   1 & 0 & -1.1 & 1 \\
   0 & 1 & -1 & 1 \\
   0 & 0 & 1.1 & 110 
  \end{array}\right]
  \xra{}
  \left[\begin{array}{ccc|c}
   1 & 0 & -1.1 & 1 \\
   0 & 1 & -1 & 1 \\
   0 & 0 & 1 & 100 
  \end{array}\right]
  \xra{}
  \left[\begin{array}{ccc|c}
   1 & 0 & 0 & 111 \\
   0 & 1 & 0 & 101 \\
   0 & 0 & 1 & 100 
  \end{array}\right]
 \]
 We conclude that there is a unique solution, namely $a=111$ and
 $b=101$ and $c=100$.
\end{solution}

\begin{exercise}\label{ex-solve-ii}
 Use the augmented matrix method to solve the following system of
 linear equations, or prove that there is no solution.
 \begin{align*}
  2w - x - y - 2z &= 1 \\
  3w - 2x - 2y - 3z &= -1 \\
  5w - 3x - 3y - 5z &= 0.
 \end{align*}
\end{exercise}
\begin{solution}
 We can write down the augmented matrix and row-reduce it as follows: 
 \[ \left[\begin{array}{cccc|c}
     2 & -1 & -1 & -2 & 1 \\
     3 & -2 & -2 & -3 & -1 \\
     5 & -3 & -3 & -5 & 0 
    \end{array}\right]
    \xra{}
    \left[\begin{array}{cccc|c}
     2 & -1 & -1 & -2 & 1 \\
    -1 &  0 &  0 &  1 & -3 \\
    -1 &  0 &  0 &  1 & -3 
    \end{array}\right]
    \xra{}
 \] \[
    \left[\begin{array}{cccc|c}
     0 & -1 & -1 &  0 & -5 \\
     1 &  0 &  0 & -1 & 3 \\
     0 &  0 &  0 &  0 & 0 
    \end{array}\right]
    \xra{}
    \left[\begin{array}{cccc|c}
     1 &  0 &  0 & -1 & 3 \\
     0 &  1 &  1 &  0 & 5 \\
     0 &  0 &  0 &  0 & 0 
    \end{array}\right]
 \]
 The final matrix corresponds to the system 
 \begin{align*}
  w-z &= 3 \\
  x+y &= 5 \\
  0 &= 0.
 \end{align*}
 There are pivots in columns $1$ and $2$, corresponding to the dependent
 variables $w$ and $x$.  After rearranging the equations to give the
 dependent variables in terms of the independent variables, we get
 $w=z+3$ and $x=5-y$ with $y$ and $z$ arbitrary.  Thus, we have an
 infinite family of solutions.
\end{solution}

\begin{exercise}\label{ex-solve-iii}
 Use the augmented matrix method to solve the following system of
 linear equations, or prove that there is no solution.
 \begin{align*}
  p+q+r &= 30 \\
  p+q-r &= 16 \\
  p-q+r &= 24 \\
  p-q-r &= 11
 \end{align*}
\end{exercise}
\begin{solution}
 We can write down the augmented matrix and row-reduce it as follows:
 \[ 
  \left[\begin{array}{ccc|c}
    1 &  1 &  1 & 30 \\
    1 &  1 & -1 & 16 \\
    1 & -1 &  1 & 24 \\
    1 & -1 & -1 & 12
  \end{array}\right]
  \xra{}
  \left[\begin{array}{ccc|c}
    1 &  1 &  1 &  30 \\
    0 &  0 & -2 & -14 \\
    0 & -2 &  0 &  -6 \\
    0 & -2 & -2 & -18
  \end{array}\right]
  \xra{}
  \left[\begin{array}{ccc|c}
    1 &  1 &  1 &  30 \\
    0 &  0 &  1 &   7 \\
    0 &  1 &  0 &   3 \\
    0 &  1 &  1 &   9
  \end{array}\right]
 \] \[
  \xra{}
  \left[\begin{array}{ccc|c}
    1 &  0 &  0 &  20 \\
    0 &  1 &  0 &   3 \\
    0 &  0 &  1 &   7 \\
    0 &  0 &  0 &   2
  \end{array}\right]
  \xra{}
  \left[\begin{array}{ccc|c}
    1 &  0 &  0 &  0 \\
    0 &  1 &  0 &  0 \\
    0 &  0 &  1 &  0 \\
    0 &  0 &  0 &  1
  \end{array}\right]
 \]
 The final matrix has a pivot in the last column, which means that the
 original system of equations has no solution.
\end{solution}

\section{Lecture 3}

\begin{exercise}\label{ex-combination-i}
 Put 
 \[ p_1 = \bbm 1 \\ 2 \ebm \qquad
    p_2 = \bbm 3 \\ 6 \ebm \qquad
    p_3 = \bbm 2 \\ 4 \ebm.
 \]
 Describe geometrically which vectors in $\R^2$ can be expressed as a
 linear combination of $p_1$, $p_2$ and $p_3$.  Give an example of a
 vector that cannot be described as such a linear combination.
\end{exercise}
\begin{solution}
 Any linear combination of the vectors $p_i$ has the form 
 \[ \lm_1p_1 + \lm_2p_2 + \lm_3p_3 =
     \bbm \lm_1+3\lm_2+2\lm_3 \\
          2\lm_1+6\lm_2+4\lm_3 \ebm = 
     (\lm_1+3\lm_2+2\lm_3)\bbm 1 \\ 2 \ebm.
 \]
 Thus, these linear combinations are just the multiples of the vector
 $\bbm 1\\ 2\ebm$, so they form the line with equation $y=2x$.  This
 means that any vector $\bbm x\\ y\ebm$ with $y\neq 2x$ cannot be
 expressed as a linear combination of the vectors $p_i$.  For example,
 the vector $e_1=\bbm 1\\ 0\ebm$ cannot be expressed as a linear
 combination of the vectors $p_i$.
\end{solution}

\begin{exercise}\label{ex-combination-ii}
 Put 
 \[ u_1 = \bbm 1 \\ 1 \\ 7 \ebm \qquad
    u_2 = \bbm 2 \\ 2 \\ 3 \ebm \qquad
    u_3 = \bbm 3 \\ 3 \\ 1 \ebm \qquad
    u_4 = \bbm 4 \\ 4 \\ 5 \ebm \qquad
    u_5 = \bbm 5 \\ 5 \\ 2 \ebm 
 \]
 Give an example of a vector $v\in\R^3$ that cannot be expressed as a
 linear combination of $u_1,\dotsc,u_5$.
\end{exercise}
\begin{solution}
 Each of the vectors $u_i$ has the first two components the same, so
 every linear combination of $u_1,\dotsc,u_5$ will also have the first
 two components the same.  Thus, if we choose any vector $v$ whose
 first two components are not the same, then it will not be a linear
 combination of $u_1,\dotsc,u_5$.  The simplest example is to take
 $v=e_1=\bbm 1&0&0\ebm^T$.
\end{solution}

\begin{exercise}\label{ex-combination-iii}
 Consider the vectors
 \[
  a_1 = \bbm 1 \\  1 \\ 1 \\ 1 \ebm \qquad
  a_2 = \bbm 1 \\  1 \\ 2 \\ 2 \ebm \qquad
  a_3 = \bbm 2 \\  2 \\ 1 \\ 1 \ebm \qquad
  a_4 = \bbm 1 \\  2 \\ 2 \\ 1 \ebm \qquad
  b   = \bbm 3 \\ -2 \\ 0 \\ 5 \ebm.
 \]
 You may assume the row-reduction
 \[ 
  \left[\begin{array}{cccc|c}
   1 & 1 & 2 & 1 &  3 \\
   1 & 1 & 2 & 2 & -2 \\
   1 & 2 & 1 & 2 &  0 \\
   1 & 2 & 1 & 1 &  5
  \end{array}\right]
  \to 
  \left[\begin{array}{cccc|c}
   1 & 0 &  3 & 0 &  6 \\
   0 & 1 & -1 & 0 &  2 \\
   0 & 0 &  0 & 1 & -5 \\
   0 & 0 &  0 & 0 &  0 
  \end{array}\right].
 \]
 Use this to give a formula expressing $b$ as a linear combination of
 $a_1,\dotsc,a_4$. 
\end{exercise}
\begin{solution}
 The left hand matrix is $[a_1|a_2|a_3|a_4|b]$, so the row-reduction
 tells us that the equation $\lm_1a_1+\dotsb+\lm_4a_4=b$ is equivalent
 to the system of equations corresponding to the right hand matrix,
 namely 
 \begin{align*}
  \lm_1+3\lm_3 &= 6 \\
  \lm_2-\lm_3  &= 2 \\
  \lm_4        &= -5.
 \end{align*}
 Here $\lm_3$ is independent so it can take arbitrary values.  We can
 choose $\lm_3=0$, giving $\lm_1=6$ and $\lm_2=2$ and $\lm_4=-5$.
 This means that we have
 \[ b = \sum_i\lm_ia_i = 6a_1+2a_2-5a_4. \]
\end{solution}

\begin{exercise}\label{ex-combination-iv}
 Consider the vectors
 \begin{align*}
  u_1 &= \bbm 1 &  2 & -1 & 0 \ebm^T &
  u_2 &= \bbm 3 & -1 & 4 & -2 \ebm^T &
  u_3 &= \bbm -1 & 5 & -6 & 2 \ebm^T \\
  v   &= \bbm 5 & -4 & 9 & -4 \ebm^T &&&
  w   &= \bbm 4 & -2 & 3 & 1 \ebm^T 
 \end{align*}
 and the matrix
 \[ A = \left[\begin{array}{c|c|c|c|c}
          &&&& \\
          u_1 & u_2 & u_3 & v & w \\
          &&&&
        \end{array}\right].
 \]
 \begin{itemize}
  \item[(a)] Row-reduce $A$.
  \item[(b)] Is $v$ a linear combination of $u_1$, $u_2$ and $u_3$?
  \item[(c)] Is $w$ a linear combination of $u_1$, $u_2$ and $u_3$?
 \end{itemize}
 (Note that you do not need any additional row-reductions for
 parts~(b) and~(c).  Remark~\ref{rem-delete-cols} in the notes is
 relevant here.)
\end{exercise}
\begin{solution}
 \begin{itemize}
  \item[(a)] We have
   \begin{align*}
    A  & =
     \left[\begin{array}{c|c|c|c|c}
      &&&&\\u_1&u_2&u_3&v&w\\&&&&
     \end{array}\right]
     =
     \left[\begin{array}{ccccc}
      1&3&-1&5&4\\2&-1&5&-4&-2\\-1&4&-6&9&3\\0&-2&2&-4&1
     \end{array}\right]\\
   & \to
     \left[\begin{array}{ccccc}
      1&3&-1&5&4\\0&-7&7&-14&-10\\0&7&-7&14&7\\0&-2&2&-4&1
     \end{array}\right]
     \to
     \left[\begin{array}{ccccc}
      1&3&-1&5&4\\0&1&-1&2&1\\0&-7&7&-14&-10\\0&-2&2&-4&1
     \end{array}\right]\\
   & \to
     \left[\begin{array}{ccccc}
      1&3&-1&5&4\\0&1&-1&2&1\\0&0&0&0&-3\\0&0&0&0&3
     \end{array}\right]
     \to
     \left[\begin{array}{ccccc}
      1&0&2&-1&1\\0&1&-1&2&1\\0&0&0&0&1\\0&0&0&0&0
     \end{array}\right]\\
   & \to
     \left[\begin{array}{ccccc}
      1&0&2&-1&0\\0&1&-1&2&0\\0&0&0&0&1\\0&0&0&0&0
     \end{array}\right]
   \end{align*}
  \item[(b)] As in Remark~\ref{rem-delete-cols} we can delete the last
   column and we still have a valid row-reduction
   \[
     \left[\begin{array}{c|c|c|c}
      &&&\\u_1&u_2&u_3&v\\ &&&
     \end{array}\right]
     =
     \left[\begin{array}{cccc}
      1&3&-1&5\\2&-1&5&-4\\-1&4&-6&9\\0&-2&2&-4
     \end{array}\right]\\
     \to
     \left[\begin{array}{cccc}
      1&0&2&-1\\0&1&-1&2\\0&0&0&0\\0&0&0&0
     \end{array}\right]
   \]
   The matrix on the right is in RREF with no pivot in the last
   column, which means (by Method~\ref{meth-find-lincomb}) that $v$ is
   indeed a linear combination of $u_1,u_2$ and $u_3$.  More
   specifically, we see that the equation
   $\lm_1u_1+\lm_2u_2+\lm_3u_3=v$ is equivalent to the system of
   equations corresponding to the above matrix, namely
   \begin{align*}
    \lm_1 + 2\lm_3 &= -1 \\
    \lm_2-\lm_3 &= 2 \\
    0 &= 0 \\
    0 &= 0.
   \end{align*}
   The solution is $\lm_1=-1-2\lm_3$ and $\lm_2=2+\lm_3$ with $\lm_3$
   arbitrary.  We can take $\lm_3=0$ giving $\lm_1=-1$ and $\lm_2=2$,
   which means that $v=-u_1+2u_2$.
  \item[(b)] As in Remark~\ref{rem-delete-cols} we can delete the fourth
   column and we still have a valid row-reduction
   \[
     \left[\begin{array}{c|c|c|c}
      &&& \\ u_1&u_2&u_3&w \\ &&&
     \end{array}\right]
     =
     \left[\begin{array}{cccc}
      1&3&-1&4\\2&-1&5&-2\\-1&4&-6&3\\0&-2&2&1
     \end{array}\right]
     \to
     \left[\begin{array}{cccc}
      1&0&2&0\\0&1&-1&0\\0&0&0&1\\0&0&0&0
     \end{array}\right]
   \]
   Here we have a pivot in the last column, indicating that $w$ cannot
   be expressed as a linear combination of $u_1$, $u_2$ and $u_3$.
 \end{itemize}
\end{solution}

\begin{exercise}\label{ex-combination-v}
 Let $u_1$ and $u_2$ be vectors in $\R^n$, and put $v_1=u_1+u_2$ and
 $v_2=u_1-u_2$.  
 \begin{itemize}
  \item[(a)] Show that if a vector $w$ can be expressed as a linear
   combination of $v_1$ and $v_2$, then it can also be expressed as a
   linear combination of $u_1$ and $u_2$.
  \item[(b)] Give a formula for $u_1$ in terms of $v_1$ and $v_2$, and
   also a formula for $u_2$ in terms of $v_1$ and $v_2$.
  \item[(c)] As a converse to~(a), show that if a vector $w$ can be
   expressed as a linear combination of $u_1$ and $u_2$, then it can
   also be expressed as a linear combination of $v_1$ and $v_2$.
 \end{itemize}
\end{exercise}
\begin{solution}
 \begin{itemize}
  \item[(a)] Suppose that $w$ can be expressed as a linear combination
   of $v_1$ and $v_2$.  This means that $w=\lm_1v_1+\lm_2v_2$ for some
   scalars $\lm_1$ and $\lm_2$.  After substituting in the definition
   of $v_1$ and $v_2$, we get 
   \[ w = \lm_1(u_1+u_2) + \lm_2(u_1-u_2) 
        = (\lm_1+\lm_2)u_1 + (\lm_1-\lm_2)u_2.
   \]
   Thus, if we define scalars $\mu_i$ by $\mu_1=\lm_1+\lm_2$ and
   $\mu_2=\lm_1-\lm_2$, we have $w=\mu_1u_1+\mu_2u_2$.  This expresses
   $w$ as a linear combination of $u_1$ and $u_2$, as required.
  \item[(b)] By adding the equations $v_1=u_1+u_2$ and $v_2=u_1-u_2$
   we get $2u_1=v_1+v_2$ and so $u_1=v_1/2+v_2/2$.  Similarly, we have
   $u_2=v_1/2-v_2/2$.
  \item[(c)] Suppose that $w$ can be expressed as a linear combination
   of $u_1$ and $u_2$.  This means that $w=\lm_1u_1+\lm_2u_2$ for some
   scalars $\lm_1$ and $\lm_2$.  After substituting in the equations
   from~(b) we get 
   \[ w = \lm_1(v_1/2+v_2/2) + \lm_2(v_1/2-v_2/2) 
        = (\lm_1/2+\lm_2/2)v_1 + (\lm_1/2-\lm_2/2)v_2.
   \]
   Thus, if we define scalars $\mu_i$ by $\mu_1=\lm_1/2+\lm_2/2$ and
   $\mu_2=\lm_1/2-\lm_2/2$, we have $w=\mu_1v_1+\mu_2v_2$.  This expresses
   $w$ as a linear combination of $v_1$ and $v_2$, as required.
 \end{itemize}
\end{solution}

\begin{exercise}\label{ex-independent-i}
 Decide whether the following lists are linearly dependent.
 \begin{itemize}
  \item[(a)] $\displaystyle
              a_1=\bbm 1\\4\ebm,\quad 
              a_2=\bbm 5\\3\ebm,\quad 
              a_3=\bbm 4\\2\ebm,\quad 
              a_4=\bbm 6\\6\ebm
             $.
  \item[(b)] $\displaystyle
              b_1=\bbm 5\\0\\0\\3\ebm,\quad
              b_2=\bbm 6\\4\\0\\0\ebm,\quad
              b_3=\bbm 7\\0\\5\\0\ebm
             $
  \item[(c)] $\displaystyle
              c_1=\bbm 5\\4\\3\ebm,\quad
              c_2=\bbm 4\\5\\4\ebm,\quad
              c_3=\bbm 5\\3\\2\ebm
             $
 \end{itemize}
\end{exercise}
\begin{solution}
 \begin{itemize}
  \item[(a)] Here we have a list of $4$ vectors in $\R^2$, and any
   such list is automatically linearly dependent.  (In general, any
   linearly independent list in $\R^n$ has length at most $n$, so any
   list of length greater than $n$ must be dependent.)  As an example
   of a nontrivial linear relation, we have 
   \[ 4a_1 + 14a_2 - 8a_3 - 7a_4 = 0. \]
   However, we do not need this in order to answer the question as
   asked.
  \item[(b)] The list $b_1,b_2,b_3$ is easily seen to be linearly
   independent.  Indeed, any linear relation
   $\lm_1b_1+\lm_2b_2+\lm_3b_3=0$ can be expanded as 
   \[ \bbm 5\lm_1+6\lm_2+7\lm_3 \\ 4\lm_2 \\ 5\lm_3 \\ 3\lm_1 \ebm
        = \bbm 0 \\ 0 \\ 0 \\ 0 \ebm. 
   \]
   By looking at the fourth entry we see that $3\lm_1=0$ so
   $\lm_1=0$.  Similarly, the second and third entries give
   $\lm_2=\lm_3=0$, so all the $\lm_i$ are zero, so our linear
   relation is the trivial one.  As there is only the trivial linear
   relation, the list is independent.

   We can reach the same conclusion by row-reducing the matrix
   $[b_1|b_2|b_3]$: 
   \[ 
     \bbm 5 & 6 & 7 \\
          0 & 4 & 0 \\
          0 & 0 & 5 \\
          3 & 0 & 0
     \ebm 
     \to
     \bbm 5 & 6 & 7 \\
          0 & 1 & 0 \\
          0 & 0 & 1 \\
          1 & 0 & 0
     \ebm 
     \to
     \bbm 1 & 0 & 0 \\
          0 & 1 & 0 \\
          0 & 0 & 1 \\
          5 & 6 & 7
     \ebm 
     \to
     \bbm 1 & 0 & 0 \\
          0 & 1 & 0 \\
          0 & 0 & 1 \\
          0 & 0 & 0
     \ebm 
   \]
   At the end we have a pivot in every column, so the original list is
   independent.
  \item[(c)] Here there is no obvious shortcut so we just row-reduce
   the matrix $[c_1|c_2|c_3]$:
   \[
    \bbm 5 & 4 & 5 \\ 
         4 & 5 & 3 \\
         3 & 4 & 2 \ebm 
    \to
    \bbm 5 & 4 & 5 \\ 
         1 & 1 & 1 \\
         3 & 4 & 2 \ebm 
    \to
    \bbm 0 &-1 & 0 \\ 
         1 & 1 & 1 \\
         0 & 1 &-1 \ebm 
    \to
   \] \[
    \bbm 0 & 1 & 0 \\ 
         1 & 0 & 1 \\
         0 & 0 &-1 \ebm 
    \to
    \bbm 1 & 0 & 1 \\
         0 & 1 & 0 \\ 
         0 & 0 &-1 \ebm 
    \to
    \bbm 1 & 0 & 0 \\
         0 & 1 & 0 \\ 
         0 & 0 & 1 \ebm 
   \]
   Again, we have a pivot in every column, so the list $c_1,c_2,c_3$ is
   independent.
 \end{itemize}
\end{solution}

\begin{exercise}\label{ex-independent-ii}
 Consider the vectors $u=\bbm 1\\2\\3\ebm$ and $v=\bbm 3\\2\\1\ebm$.
 Give an example of a nonzero vector $w$ such that the list $u,w$ is
 independent and the list $v,w$ is independent but the list $u,v,w$ is
 dependent. 
\end{exercise}
\begin{solution}
 The simplest example is to put $w=u+v=\bbm 4\\4\\4\ebm$.  To see that
 this works, recall that a list of two nonzero vectors is independent
 iff the two vectors are not multiples of each other.  As $w$ is not a
 multiple of $u$, we see that the list $u,w$ is independent.
 Similarly, as $w$ is not a multiple of $v$ we see that the list $v,w$
 is independent.  However, we have a nontrivial linear relation
 $u+v-w=0$, which proves that the list $u,v,w$ is dependent.
\end{solution}


\section{Lecture 4}

\begin{exercise}\label{ex-independent-iii}
 Find examples as follows.  All your vectors should be nonzero, and
 all your lists should have length at least two and not contain the
 same vector twice.
 \begin{itemize}
  \item[(a)] A list of vectors in $\R^3$ that is linearly dependent
   and does not span $\R^3$.
  \item[(b)] A list of vectors in $\R^3$ that is linearly dependent
   and spans $\R^3$.
  \item[(c)] A list of vectors in $\R^3$ that is linearly independent
   and does not span $\R^3$.
  \item[(d)] A list of vectors in $\R^3$ that is linearly independent
   and does not span $\R^3$.
 \end{itemize}
\end{exercise}
\begin{solution}
 There are many possible correct solutions.  Here is one.
 \begin{itemize}
  \item[(a)] Put $a_1=e_1=\bbm 1\\0\\0\ebm$ and $a_2=-a_1$.  Then the
   list $a_1,a_2$ is linearly dependent (because we have a nontrivial
   linear relation $a_1+a_2=0$) and does not span (because $e_2$
   cannot be written as a linear combination of $a_1$ and $a_2$).
  \item[(b)] Put $b_1=e_1$ and $b_2=e_2$ and $b_3=e_3$ and
   $b_4=-e_3$.  The list $b_1,\dotsc,b_4$ is linearly dependent,
   because we have the nontrivial linear relation
   $0b_1+0b_2+b_3+b_4=0$.  It spans $\R^3$, because any vector
   $v=\bbm x&y&z\ebm^T\in\R^3$ can be written as
   $v=xb_1+yb_2+zb_3+0b_4$, which expresses $v$ as a linear
   combination of $b_1,\dotsc,b_4$.
  \item[(c)] Put $c_1=e_1$ and $c_2=e_2$.  The list $c_1,c_2$ is
   clearly linearly independent: a linear relation
   $\lm_1c_1+\lm_2c_2=0$ expands to give
   $\bbm \lm_1\\\lm_2\\0\ebm=\bbm 0\\0\\0\ebm$, so $\lm_1=\lm_2=0$, so
   the linear relation is trivial.  However, $e_3$ cannot be expressed
   as a linear combination of $c_1$ and $c_2$, so the list $c_1,c_2$
   does not span.
  \item[(d)] The list $e_1,e_2,e_3$ is linearly independent and spans.
 \end{itemize}
\end{solution}

\begin{exercise}\label{ex-independent-iv}
 Decide whether the following statements are true or false.  Justify
 your answers, and give explicit counterexamples for any statements
 that are false.
 \begin{itemize}
  \item[(a)] Every list of $4$ vectors in $\R^3$ spans $\R^3$.
  \item[(b)] Every list of $4$ vectors in $\R^3$ is linearly
   independent.
  \item[(c)] If $\CA$ is a list that spans $\R^4$ and $\CB$ is a
   linearly independent list in $\R^4$ then $\CA$ is at least as long
   as $\CB$.
  \item[(d)] There is a linearly independent list of length $5$ in
   $\R^6$. 
 \end{itemize}
\end{exercise}
\begin{solution}
 \begin{itemize}
  \item[(a)] This is false.  For example, the list $e_1,e_1,e_1,e_1$
   is a list of four vectors in $\R^4$ that does not span.
  \item[(b)] This is also false, and in fact is the opposite of the
   truth: every list of $4$ vectors in $\R^3$ is linearly dependent,
   not linearly independent.
  \item[(c)] This is true.  As $\CA$ spans $\R^4$ it must contain at
   least $4$ vectors, and as $\CB$ is linearly independent in $\R^4$
   it must contain at most $4$ vectors. 
   Thus $\text{length}(\CB)\leq 4\leq\text{length}(\CA)$.
  \item[(d)] This is true.  The list $e_1,e_2,e_3,e_4,e_5$ is the most
   obvious example.
 \end{itemize}
\end{solution}

\begin{exercise}\label{ex-fullspan-i}
 Consider the list
 \[ u_1 = \bbm 2 \\ -1 \\ 0 \ebm, \quad
    u_2 = \bbm 0 \\ 3 \\ -2 \ebm, \quad
    u_3 = \bbm 1 \\ -2 \\ 1 \ebm, \quad
    u_4 = \bbm 3 \\ 0 \\ -1 \ebm. 
 \]
 Does this span $\R^3$?
\end{exercise}
\begin{solution}
 We use Method~\ref{meth-check-span}, which tells us to perform the
 following row-reduction:
 \[ 
   \left[\begin{array}{ccc}
    & u_1^T & \\ \hline
    & u_2^T & \\ \hline
    & u_3^T & \\ \hline
    & u_4^T & \\ \hline
   \end{array}\right]
   = 
   \bbm 
    2 & -1 &  0 \\
    0 &  3 & -2 \\
    1 & -2 &  1 \\
    3 &  0 & -1 
   \ebm 
   \to 
   \bbm 
    0 &  3 & -2 \\
    0 &  3 & -2 \\
    1 & -2 &  1 \\
    0 &  6 & -4 
   \ebm 
   \to 
 \] \[
   \bbm 
    1 & -2 &  1 \\
    0 &  3 & -2 \\
    0 &  3 & -2 \\
    0 &  6 & -4 
   \ebm 
   \to 
   \bbm 
    1 & -2 &  1 \\
    0 &  1 & -2/3 \\
    0 &  0 & 0 \\
    0 &  0 & 0 
   \ebm 
   \to 
   \bbm 
    1 &  0 & -1/3 \\
    0 &  1 & -2/3 \\
    0 &  0 & 0 \\
    0 &  0 & 0 
   \ebm. 
 \]
 In the final matrix we do not have a pivot in every column, so the
 specified list does not span $\R^3$.  
\end{solution}

\begin{exercise}\label{ex-fullspan-ii}
 Put $a=\bbm 1 & 3 & 5 & 7 \ebm\in\R^4$.
 \begin{itemize}
  \item[(a)] Suppose we have vectors $u_1,\dotsc,u_4\in\R^4$ with
   $a.u_1=a.u_2=a.u_3=a.u_4=0$.  Prove that the list $u_1,\dotsc,u_4$
   does not span $\R^4$.
  \item[(b)] Give an example of a list $v_1,\dotsc,v_4$ that 
   satisfies $a.v_1=a.v_2=a.v_3=a.v_4=1$ and also spans $\R^4$.
  \item[(c)] Give an example of a list $w_1,\dotsc,w_4$ that 
   satisfies $a.w_1=a.w_2=a.w_3=a.w_4=1$ and does not span $\R^4$.
 \end{itemize}
\end{exercise}
\begin{solution}
 \begin{itemize}
  \item[(a)] If $x$ is a linear combination of the vectors $u_i$, we
   have $x=\lm_1u_1+\dotsb+\lm_4u_4$ for some scalars
   $\lm_1,\dotsc,\lm_4$, so 
   \[ a.x = a.(\lm_1u_1+\dotsb+\lm_4u_4) =
       \lm_1(a.u_1) + \dotsb + \lm_4(a.u_4),
   \]
   but $a.u_1=a.u_2=a.u_3=a.u_4=0$ so $a.x=0$.  On the other hand, we
   have $a.e_1=1\neq 0$, so $e_1$ cannot be a linear combination of
   the vectors $u_i$.  This means that the $u_i$ do not span $\R^4$.
  \item[(b)] The obvious example is 
   \[ v_1 = \bbm 1 \\ 0 \\ 0 \\ 0 \ebm \hspace{3em} 
      v_2 = \bbm 0 \\ 1/3 \\ 0 \\ 0 \ebm \hspace{3em} 
      v_3 = \bbm 0 \\ 0 \\ 1/5 \\ 0 \ebm \hspace{3em} 
      v_4 = \bbm 0 \\ 0 \\ 0 \\ 1/7 \ebm.
   \]
   To see that this spans, note that an arbitrary vector
   $x=\bbm a&b&c&d\ebm^T$ in $\R^4$ can be expressed as 
   \[ x = a v_1 + 3b v_2 + 5c v_3 + 7d v_4, \]
   which is a linear combination of the list $v_1,\dotsc,v_4$.
  \item[(c)] The most obvious solution is to take
   $w_1=w_2=w_3=w_4=e_1$.  If we prefer to avoid repetitions, we can
   instead use 
   \[ w_1 = \bbm  1 \\  0 \\ 0 \\ 0 \ebm \hspace{3em} 
      w_2 = \bbm  4 \\ -1 \\ 0 \\ 0 \ebm \hspace{3em} 
      w_3 = \bbm  7 \\ -2 \\ 0 \\ 0 \ebm \hspace{3em} 
      w_4 = \bbm 10 \\ -3 \\ 0 \\ 0 \ebm.
   \]
   It is clear that any linear combination of $w_1,\dotsc,w_4$ has
   zeros in the third and fourth places.  In particular, the standard
   vector $e_4$ is not a linear combination of the list
   $w_1,\dotsc,w_4$, so the list does not span $\R^4$.
 \end{itemize}
\end{solution}

\begin{exercise}\label{ex-fullspan-iii}
 The vectors 
 \[ u_1 = \bbm 1 \\ 0 \\ 0 \\ 0 \ebm \qquad
    u_2 = \bbm 1 \\ 1 \\ 0 \\ 0 \ebm \qquad
    u_3 = \bbm 1 \\ 1 \\ 1 \\ 0 \ebm \qquad
    u_4 = \bbm 1 \\ 1 \\ 1 \\ 1 \ebm \qquad
    u_5 = \bbm 0 \\ 1 \\ 1 \\ 1 \ebm \qquad
    u_6 = \bbm 0 \\ 0 \\ 1 \\ 1 \ebm \qquad
    u_7 = \bbm 0 \\ 0 \\ 0 \\ 1 \ebm \qquad
 \]
 span $\R^4$, because an arbitrary vector
 $x=\bbm a & b & c & d\ebm^T$ can be expressed as a linear combination
 of $u_i$ by the formula 
 \[ x = (a-b)u_1 + b u_2 + c u_6 + (d-c)u_7, \]
 or alternatively by the formula
 \[ x = -bu_1+bu_2-du_3+(a+d)u_4 -a u_5 + cu_6 -cu_7. \]
 \begin{itemize}
  \item[(a)] Check the above formulae.
  \item[(b)] Give a similar explicit formula to prove that the
   following vectors span $\R^4$:
   \[ v_1 = \bbm 1 \\ 1 \\ 1 \\ 0 \ebm \hspace{3em}
      v_2 = \bbm 1 \\ 1 \\ 0 \\ 1 \ebm \hspace{3em}
      v_3 = \bbm 1 \\ 1 \\ 1 \\ 1 \ebm \hspace{3em}
      v_4 = \bbm 1 \\ 0 \\ 1 \\ 1 \ebm \hspace{3em}
      v_5 = \bbm 0 \\ 1 \\ 1 \\ 1 \ebm
   \]
  \item[(c)] Use the row-reduction method to show again that the
   vectors $v_i$ span $\R^4$.
 \end{itemize}
\end{exercise}
\begin{solution}
 \begin{itemize}
  \item[(a)] For the first formula we have
   \begin{align*}
    & (a-b)u_1 + b u_2 + c u_6 + (d-c)u_7 \\
    =& \bbm a-b \\ 0 \\ 0 \\ 0 \ebm + 
       \bbm b \\ b \\ 0 \\ 0 \ebm +
       \bbm 0 \\ 0 \\ c \\ c \ebm + 
       \bbm 0 \\ 0 \\ 0 \\ d-c \ebm 
    = \bbm a \\ b \\ c \\ d \ebm. 
   \end{align*}
   For the second, we have
   \begin{align*}
    & -bu_1+bu_2-du_3+(a+d)u_4 -a u_5 + cu_6 -cu_7 \\
    &=  \bbm  -b \\   0 \\   0 \\   0 \ebm +
        \bbm   b \\   b \\   0 \\   0 \ebm +
        \bbm  -d \\  -d \\  -d \\   0 \ebm +
        \bbm a+d \\ a+d \\ a+d \\ a+d \ebm +
        \bbm   0 \\  -a \\  -a \\  -a \ebm +
        \bbm   0 \\   0 \\   c \\   c \ebm +
        \bbm   0 \\   0 \\   0 \\  -c \ebm 
      = \bbm   a \\   b \\   c \\   d \ebm.
   \end{align*}
  \item[(b)] One possible formula is as follows: if
   $x=\bbm a & b & c & d\ebm^T$, then
   \[ x = -dv_1 -cv_2 +(a+b+c+d)v_3 - bv_4 -av_5. \]
   This can be found as follows: we note that
   \[ e_1 = v_3 - v_5 \hspace{4em}
      e_2 = v_3 - v_4 \hspace{4em}
      e_3 = v_3 - v_2 \hspace{4em}
      e_4 = v_3 - v_1,
   \]
   and it follows that
   \begin{align*}
    x &= a e_1 + b e_2 + c e_3 + d e_4 \\
      &= a(v_3-v_5) + b(v_3-v_4) + c(v_3-v_2) + d(v_3-v_1) \\
      &= -dv_1 -cv_2 +(a+b+c+d)v_3 - bv_4 -av_5.
   \end{align*}
  \item[(c)] The general method for these kinds of questions is to
   construct a matrix $A$ whose rows are the vectors $v_i^T$, and then
   row-reduce it:
   \[ A = \bbm 1 & 1 & 1 & 0 \\ 
               1 & 1 & 0 & 1 \\
               1 & 1 & 1 & 1 \\
               1 & 0 & 1 & 1 \\
               0 & 1 & 1 & 1 \ebm \to
          \bbm 1 & 1 & 1 & 0 \\ 
               0 & 0 &-1 & 1 \\
               0 & 0 & 0 & 1 \\
               0 &-1 & 0 & 1 \\
               0 & 1 & 1 & 1 \ebm \to
          \bbm 1 & 1 & 1 & 0 \\ 
               0 & 0 & 1 &-1 \\
               0 & 0 & 0 & 1 \\
               0 & 0 & 1 & 2 \\
               0 & 1 & 1 & 1 \ebm \to
   \] \[
          \bbm 1 & 1 & 1 & 0 \\ 
               0 & 0 & 1 &-1 \\
               0 & 0 & 0 & 1 \\
               0 & 0 & 0 & 3 \\
               0 & 1 & 1 & 1 \ebm \to
          \bbm 1 & 1 & 1 & 0 \\ 
               0 & 1 & 1 & 1 \\
               0 & 0 & 1 &-1 \\
               0 & 0 & 0 & 1 \\
               0 & 0 & 0 & 3 \ebm \to
          \bbm 1 & 0 & 0 & 0 \\ 
               0 & 1 & 0 & 0 \\
               0 & 0 & 1 & 0 \\
               0 & 0 & 0 & 1 \\
               0 & 0 & 0 & 0 \ebm
   \]
   The final matrix has a pivot in every column, so the vectors $v_i$
   span $\R^4$. 
 \end{itemize}
\end{solution}

\section{Lecture 5}

\begin{exercise}\label{ex-basis-i}
 \begin{itemize}
  \item[(a)] Is the list
   $a_1=\bbm 3\\5\ebm, a_2=\bbm 2\\7\ebm, a_3=\bbm 4\\4\ebm$
   a basis for $\R^2$?
  \item[(b)] Is the list
   $b_1=\bbm 9\\8\\7\ebm, b_2=\bbm 8\\7\\6\ebm, b_3=\bbm 3\\2\\1\ebm$
   a basis for $\R^3$?
  \item[(c)] Is the list 
   $c_1=\bbm 1\\8\\5\\4\ebm,
    c_2=\bbm 7\\3\\9\\5\ebm,
    c_3=\bbm 5\\1\\9\\9\ebm$
   a basis for $\R^4$?
 \end{itemize}
\end{exercise}
\begin{solution}
 Any basis for $\R^n$ must contain exactly $n$ vectors.  In
 particular, a basis for $\R^2$ must contain precisely $2$ vectors, so
 $a_1,a_2,a_3$ cannot be a basis for $\R^2$.  (In fact, there is a
 linear relation $-20a_1+8a_2+11a_3=0$, showing that the list is
 linearly dependent and so cannot form a basis.  However, it is not
 strictly necessary to work this out.)  Similarly, as the list
 $c_1,c_2,c_3$ does not have length $4$, it cannot form a basis for
 $\R^4$.  This just leaves part~(b).  Here we can observe that
 \begin{align*} 
  b_1-b_2 &= \bbm 1&1&1\ebm^T \\
  b_2-b_3 &= \bbm 5&5&5\ebm^T = 5(b_1-b_2),
 \end{align*}
 and this rearranges to give a nontrivial linear relation
 $6b_1-5b_2+b_3=0$.  This proves that the list $b_1,b_2,b_3$ is
 linearly dependent, so again we do not have a basis.  This can also
 be seen by row reducing the matrix $[b_1|b_2|b_3]$:
 \[ 
  \bbm 9 & 8 & 3 \\
       8 & 7 & 2 \\
       7 & 6 & 1 \ebm \to
  \bbm 1 & 8/9 & 1/3 \\
       8 &   7 &   2 \\ 
       7 &   6 &   1 \ebm \to
  \bbm 1 &  8/9 &  1/3 \\
       0 & -1/9 & -2/3 \\ 
       0 & -2/9 & -4/3 \ebm \to
  \bbm 1 &  8/9 &  1/3 \\
       0 &    1 &    6 \\ 
       0 & -2/9 & -4/3 \ebm \to
  \bbm 1 &    0 &  -5 \\
       0 &    1 &   6 \\ 
       0 &    0 &   0 \ebm
 \]
 As the final result is not the identity matrix, we see that the list
 $b_1,b_2,b_3$ is not a basis.
\end{solution}

\begin{exercise}\label{ex-basis-ii}
 Consider the list 
 \[ 
  a_1 = \bbm 1\\1\\1\\2 \ebm, \quad
  a_2 = \bbm 1\\1\\3\\4 \ebm, \quad
  a_3 = \bbm 1\\4\\5\\6 \ebm, \quad
  a_4 = \bbm 7\\8\\9\\10\ebm.
 \]
 Is this a basis for $\R^4$?
\end{exercise}
\begin{solution}
 We can check this by row-reducing the matrix $[a_1|a_2|a_3|a_4]$:
 \[ 
  \bbm 1 & 1 & 1 & 7 \\ 
       1 & 1 & 4 & 8 \\
       1 & 4 & 5 & 6 \\
       7 & 8 & 9 & 10 \ebm 
  \to 
  \bbm 1 & 1 & 1 &   7 \\ 
       0 & 0 & 3 &   1 \\
       0 & 3 & 4 &  -1 \\
       0 & 1 & 2 & -39 \ebm 
  \to 
  \bbm 1 & 0 &-1 &  46 \\ 
       0 & 0 & 3 &   1 \\
       0 & 0 &-2 & 116 \\
       0 & 1 & 2 & -39 \ebm 
  \to 
  \bbm 1 & 0 &-1 &  46 \\ 
       0 & 0 & 3 &   1 \\
       0 & 0 & 1 & -58 \\
       0 & 1 & 2 & -39 \ebm 
  \to 
 \] \[
  \bbm 1 & 0 & 0 & -12 \\ 
       0 & 0 & 0 & 175 \\
       0 & 0 & 1 & -58 \\
       0 & 1 & 0 &  77 \ebm 
  \to 
  \bbm 1 & 0 & 0 & -12 \\ 
       0 & 0 & 0 &   1 \\
       0 & 0 & 1 & -58 \\
       0 & 1 & 0 &  77 \ebm 
  \to 
  \bbm 1 & 0 & 0 &   0 \\ 
       0 & 0 & 0 &   1 \\
       0 & 0 & 1 &   0 \\
       0 & 1 & 0 &   0 \ebm 
  \to 
  \bbm 1 & 0 & 0 &   0 \\ 
       0 & 1 & 0 &   0 \\
       0 & 0 & 1 &   0 \\
       0 & 0 & 0 &   1 \ebm
 \]
 As we end up with the identity matrix, the original list is a basis.
\end{solution}

\begin{exercise}\label{ex-basis-iii}
 Put $u_1=\bbm 1\\2\\3\ebm$ and $u_2=\bbm 2\\3\\4\ebm$.  Find a vector
 $u_3$ such that the list $u_1,u_2,u_3$ is a basis for $\R^3$.
\end{exercise}
\begin{solution}
 Any vector will do provided that it does not lie in the plane spanned
 by $u_1$ and $u_2$, so if you choose $u_3$ randomly then it will
 probably work.  The simplest choice is to take
 $u_3=e_1=\bbm 1\\0\\0\ebm$.  To check that $u_1,u_2,u_3$ is a basis
 we can row-reduce the matrix $U=[u_1|u_2|u_3]$ and check that we get
 the identity:
 \[ 
  \bbm 1 & 2 & 1 \\
       2 & 3 & 0 \\
       3 & 4 & 0 \ebm 
  \to 
  \bbm 1 & 2 & 1 \\
       0 &-1 &-2 \\
       0 &-2 &-3 \ebm 
  \to 
  \bbm 1 & 0 &-3 \\
       0 & 1 & 2 \\
       0 & 0 & 1 \ebm 
  \to 
  \bbm 1 & 0 & 0 \\
       0 & 1 & 0 \\
       0 & 0 & 1 \ebm. 
 \]
\end{solution}

\begin{exercise}\label{ex-basis-iv}
 Suppose that the list $a_1,a_2,a_3,a_4,a_5$ is a basis for $\R^5$.
 Show that the list $a_1,a_3,a_5$ is linearly independent.
\end{exercise}
\begin{solution}
 Suppose we have a linear relation $\lm a_1 + \mu a_3 + \nu a_5=0$.
 This gives a linear relation
 \[ \lm a_1 + 0 a_2 + \mu a_3 + 0 a_4 + \nu a_5 = 0 \]
 on the whole list.  However, the whole list is a basis for $\R^5$, so
 in particular it is linearly independent.  Thus, the above linear
 relation must be the trivial one, so the coefficients
 $\lm,0,\mu,0,\nu,0$ must all be zero.  As $\lm,\mu$ and $\nu$ are
 zero, we see that the original relation on the list $a_1,a_3,a_5$ is
 the trivial relation.  This means that the list $a_1,a_3,a_5$ is
 linearly independent, as claimed.
\end{solution}

\section{Lecture 6}

\begin{exercise}\label{ex-inverse-i}
 Find the inverse of the matrix 
 \[ A = \bbm 0&0&0&1 \\ 0&0&1&1 \\ 0&1&1&1 \\ 1&1&1&1 \ebm. \]
\end{exercise}
\begin{solution}
 We row-reduce the matrix $[A|I_4]$:
 \[ 
  \left[\begin{array}{cccc|cccc}
    0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 \\ 
    0 & 0 & 1 & 1 & 0 & 1 & 0 & 0 \\ 
    0 & 1 & 1 & 1 & 0 & 0 & 1 & 0 \\ 
    1 & 1 & 1 & 1 & 0 & 0 & 0 & 1
  \end{array}\right]
  \xra{1}
  \left[\begin{array}{cccc|cccc}
    0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 \\ 
    0 & 0 & 1 & 1 & 0 & 1 & 0 & 0 \\ 
    0 & 1 & 1 & 1 & 0 & 0 & 1 & 0 \\ 
    1 & 0 & 0 & 0 & 0 & 0 &-1 & 1
  \end{array}\right]
  \xra{2}
  \left[\begin{array}{cccc|cccc}
    0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 \\ 
    0 & 0 & 1 & 1 & 0 & 1 & 0 & 0 \\ 
    0 & 1 & 0 & 0 & 0 &-1 & 1 & 0 \\ 
    1 & 0 & 0 & 0 & 0 & 0 &-1 & 1
  \end{array}\right]
 \] \[
  \xra{3}
  \left[\begin{array}{cccc|cccc}
    0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 \\ 
    0 & 0 & 1 & 0 &-1 & 1 & 0 & 0 \\ 
    0 & 1 & 0 & 0 & 0 &-1 & 1 & 0 \\ 
    1 & 0 & 0 & 0 & 0 & 0 &-1 & 1
  \end{array}\right]
  \xra{4}
  \left[\begin{array}{cccc|cccc}
    1 & 0 & 0 & 0 & 0 & 0 &-1 & 1 \\
    0 & 1 & 0 & 0 & 0 &-1 & 1 & 0 \\ 
    0 & 0 & 1 & 0 &-1 & 1 & 0 & 0 \\ 
    0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 
  \end{array}\right].
 \] 
 The conclusion is that 
 \[ 
  A^{-1} = \bbm 
    0 &  0 & -1 &  1 \\
    0 & -1 &  1 &  0 \\
   -1 &  1 &  0 &  0 \\
    1 &  0 &  0 &  0 
  \ebm.
 \]
\end{solution}

\begin{exercise}\label{ex-elementary-i}
 Consider the matrix
 \[ A_0 = \bbm 0 & 10 & 100 & -1 &  10 \\
               0 & 11 & 110 & -1 &  21 \\
               0 & -1 & -10 &  0 & -11
          \ebm
 \]
 \begin{itemize}
  \item[(a)] Find a row reduction 
   \[ A_0 \to A_1 \to A_2 \to A_3 \to A_4 \to A_5 \to A_6 \]
   where each step uses only a single row-operation and $A_6$ is in
   RREF.
  \item[(b)] Find elementary matrices $U_1,\dotsc,U_6$ such that
   $A_i=U_iA_{i-1}$.
  \item[(c)] Hence find an invertible matrix $U$ such that
   $A_6=UA_0$.  (Be careful about the order of multiplication.)
 \end{itemize}
\end{exercise}
\begin{solution}
 The relevant matrices are as follows:
 \begin{align*}
  A_1 &= 
    \bbm 0 & 10 & 100 & -1 &   10 \\
         0 & 11 & 110 & -1 &   21 \\
         0 &  1 &  10 &  0 &   11
    \ebm &
  U_1 &= D_3(-1) = \bbm 1&0&0 \\ 0&1&0 \\ 0&0&-1 \ebm \\
  A_2 &= 
    \bbm 0 &  0 &   0 & -1 & -100 \\
         0 & 11 & 110 & -1 &   21 \\
         0 &  1 &  10 &  0 &   11
    \ebm &
  U_2 &= E_{13}(-10) = \bbm 1&0&-10 \\ 0&1&0 \\ 0&0&1 \ebm \\
  A_3 &= 
    \bbm 0 &  0 &   0 & -1 & -100 \\
         0 &  0 &   0 & -1 & -100 \\
         0 &  1 &  10 &  0 &   11
    \ebm &
  U_3 &= E_{23}(-11) = \bbm 1&0&0 \\ 0&1&-11 \\ 0&0&1 \ebm \\
  A_4 &= 
    \bbm 0 &  1 &  10 &  0 &   11 \\
         0 &  0 &   0 & -1 & -100 \\
         0 &  0 &   0 & -1 & -100
    \ebm &
  U_4 &= F_{13} = \bbm 0&0&1 \\ 0&1&0 \\ 1&0&0 \ebm \\
  A_5 &= 
    \bbm 0 &  1 &  10 &  0 &   11 \\
         0 &  0 &   0 &  1 &  100 \\
         0 &  0 &   0 & -1 & -100
    \ebm &
  U_5 &= D_2(-1) = \bbm 1&0&0 \\ 0&-1&0 \\ 0&0&1 \ebm \\
  A_6 &= 
    \bbm 0 &  1 &  10 &  0 &   11 \\
         0 &  0 &   0 &  1 &  100 \\
         0 &  0 &   0 &  0 &    0
    \ebm &
  U_6 &= E_{32}(1) = \bbm 1&0&0 \\ 0&1&0 \\ 0&1&1 \ebm.
 \end{align*}
 Indeed, the reduction steps are as follows:
 \begin{itemize}
  \item[(1)] Multiply row $3$ by $-1$.
  \item[(2)] Add $-10$ times row $3$ to row $1$.
  \item[(3)] Add $-11$ times row $3$ ro row $2$.
  \item[(4)] Swap rows $1$ and $3$.
  \item[(5)] Multiply row $3$ by $-1$.
  \item[(6)] Add row $2$ to row $3$.
 \end{itemize}
 The matrices $U_i$ correspond to these row operations as in
 Proposition~\ref{prop-ro-elem}.  It follows that
 \begin{align*}
  A_1 &= U_1A_0 \\
  A_2 &= U_2A_1 = U_2U_1A_0 \\
  A_3 &= U_3A_2 = U_3U_2U_1A_0
 \end{align*}
 and so on, so $A_6=UA_0$ where $U=U_6U_5U_4U_3U_2U_1$.
 Here 
 \begin{align*}
  U_6U_5U_4 &= \bbm 1&0&0 \\ 0&1&0 \\ 0&1&1 \ebm
               \bbm 1&0&0 \\ 0&-1&0 \\ 0&0&1 \ebm
               \bbm 0&0&1 \\ 0&1&0 \\ 1&0&0 \ebm
             = \bbm 0&0&1 \\ 0&-1&0 \\1&-1&0 \ebm \\
  U_3U_2U_1 &= \bbm 1&0&0 \\ 0&1&-11 \\ 0&0&1 \ebm
               \bbm 1&0&-10 \\ 0&1&0 \\ 0&0&1 \ebm
               \bbm 1&0&0 \\ 0&1&0 \\ 0&0&-1 \ebm
             = \bbm 1&0&10 \\ 0&1&11 \\ 0&0&-1 \ebm \\
  U         &= \bbm 0&0&1 \\ 0&-1&0 \\1&-1&0 \ebm
               \bbm 1&0&10 \\ 0&1&11 \\ 0&0&-1 \ebm 
             = \bbm 0&0&-1 \\ 0&-1&-11 \\ 1&-1&-1 \ebm.
 \end{align*}
 As a check, we can verify directly that
 \[ UA_0 = 
    \bbm 0&0&-1 \\ 0&-1&-11 \\ 1&-1&-1 \ebm
    \bbm 0 & 10 & 100 & -1 &  10 \\
         0 & 11 & 110 & -1 &  21 \\
         0 & -1 & -10 &  0 & -11
    \ebm = 
    \bbm 0 &  1 &  10 &  0 &   11 \\
         0 &  0 &   0 &  1 &  100 \\
         0 &  0 &   0 &  0 &    0
    \ebm =
    A_6.
 \]
\end{solution}

\begin{exercise}\label{ex-invertible-i}
 Which of the following matrices are invertible?  Justify your
 answers. 
 \[ A = \bbm 1&2&2&1 \\ 2&3&3&2 \\ 2&3&3&2 \\ 1&2&2&1 \ebm
    \quad
    B = \bbm 2&2&2&2 \\ 1&2&2&2 \\ 1&1&2&2 \\ 1&1&1&2 \ebm
    \quad
    C = \bbm 2&1&1&1 \\ 2&2&1&1 \\ 2&2&2&1 \\ 2&2&2&2 \ebm
    \quad
    D = \bbm 1&2&3&4 \\ 0&0&5&6 \\ 0&0&7&8 \\ 0&0&0&9 \ebm
    \quad
    E = \bbm 1 & 1 \\ 10 & 11 \\ 100 & 111 \\ 1000 & 1111 \ebm
 \]
\end{exercise}
\begin{solution}
 \begin{itemize}
  \item[(a)] The matrix $A$ is not invertible.  Indeed, the first and
   last rows are the same, as are the middle two rows.  Thus, we can
   perform row operations on $A$ to get a matrix $A'$ with two rows of
   zeros.  It follows that $A$ cannot row-reduce to the identity.
   Alternatively, we can say that there are only two distinct columns,
   which means that the columns cannot possibly form a basis for
   $\R^4$, which again means that the matrix is not invertible.
  \item[(b)] We can start row-reducing $B$ as follows:
   \[ B = 
       \bbm 2&2&2&2 \\ 1&2&2&2 \\ 1&1&2&2 \\ 1&1&1&2 \ebm \to
       \bbm 1&1&1&1 \\ 1&2&2&2 \\ 1&1&2&2 \\ 1&1&1&2 \ebm \to
       \bbm 1&1&1&1 \\ 0&1&1&1 \\ 0&0&1&1 \\ 0&0&0&1 \ebm = B'.
   \]
   As $B'$ is upper-triangular with ones on the diagonal we have
   $\det(B')=1$, and it follows that $\det(B)\neq 0$, so $B$ is
   invertible.  More specifically, only the first of our row
   operations (where we multiplied row $1$ by $1/2$) affects the
   determinant, so $\det(B)=\det(B')/(1/2)=2$.  Alternatively, we can
   just carry out a few more row operations to see that $B'\to I_4$.
  \item[(c)] We have $C=B^T$ and it is clear from
   Theorem~\ref{thm-inverse} that the transpose of any invertible
   matrix is invertible, so $C$ is invertible.
  \item[(d)] As $D$ is upper triangular, the determinant is the
   product of the diagonal entries, which is zero because $D_{22}=0$.
   It follows that $D$ is not invertible.  This can also be seen from
   the row-reduction
   \[ 
      \bbm 1&2&3&4 \\ 0&0&5&6 \\ 0&0&7&8 \\ 0&0&0&9 \ebm \to
      \bbm 1&2&3&4 \\ 0&0&1&6/5 \\ 0&0&1&8/7 \\ 0&0&0&1 \ebm \to
      \bbm 1&2&3&0 \\ 0&0&1&0 \\ 0&0&1&0 \\ 0&0&0&1 \ebm \to
      \bbm 1&2&0&0 \\ 0&0&1&0 \\ 0&0&0&1 \\ 0&0&0&0 \ebm.
   \]
  \item[(e)] The matrix $E$ is not invertible, just because
   invertibility only makes sense for square matrices.
 \end{itemize}
\end{solution}

\begin{exercise}\label{ex-inverse-ii}
 Find the inverse of the following matrix, either by creative
 experimentation or by row-reduction.
 \[ A = \bbm 0&0&1&0 \\ 0&0&0&1 \\ 1&0&a&b \\ 0&1&c&d \ebm \]
\end{exercise}
\begin{solution}
 The answer is
 \[ A^{-1} =
     \bbm -a&-b&1&0 \\ -c&-d&0&1 \\ 1&0&0&0 \\ 0&1&0&0 \ebm.
 \]
 There are various ways to see this.  Perhaps the most conceptual is
 as follows.  We can put $B=\bbm a&b\\ c&d\ebm$ and divide $A$ into
 $2\tm 2$ blocks.  We then have
 $A=\left[\begin{array}{c|c} 0&I\\ \hline I&B\end{array}\right]$, and
 the claim is that 
 $A^{-1}=\left[\begin{array}{c|c} -B&I\\ \hline I&0\end{array}\right]$.
 To check this we just need the equation
 \[ \left[\begin{array}{c|c}  0&I \\ \hline I&B\end{array}\right]
    \left[\begin{array}{c|c} -B&I \\ \hline I&0\end{array}\right] = 
    \left[\begin{array}{c|c}  I&0 \\ \hline 0&I\end{array}\right].
 \]
 This is clear provided that we believe that we can treat the $2\tm 2$
 blocks as though they were just numbers when we perform the above
 matrix product.  This is not completely obvious, but it can be
 justified. 

 For a more pedestrian approach, we row-reduce the matrix $[A|I_4]$:
 \[ \left[\begin{array}{cccc|cccc}
     0&0&1&0 & 1&0&0&0 \\
     0&0&0&1 & 0&1&0&0 \\
     1&0&a&b & 0&0&1&0 \\
     0&1&c&d & 0&0&0&1
    \end{array}\right]
    \to 
    \left[\begin{array}{cccc|cccc}
     0&0&1&0 &  1&0&0&0 \\
     0&0&0&1 &  0&1&0&0 \\
     1&0&0&b & -a&0&1&0 \\
     0&1&0&d & -c&0&0&1
    \end{array}\right]
    \to 
 \] \[
    \left[\begin{array}{cccc|cccc}
     0&0&1&0 &  1& 0&0&0 \\
     0&0&0&1 &  0& 1&0&0 \\
     1&0&0&0 & -a&-b&1&0 \\
     0&1&0&0 & -c&-d&0&1
    \end{array}\right]
    \to 
    \left[\begin{array}{cccc|cccc}
     1&0&0&0 & -a&-b&1&0 \\
     0&1&0&0 & -c&-d&0&1 \\
     0&0&1&0 &  1& 0&0&0 \\
     0&0&0&1 &  0& 1&0&0 
    \end{array}\right]    
 \]
 (Subtract multiples of row $1$ from rows $3$ and $4$; subtract
 multiples of row $2$ from rows $3$ and $4$; swap rows $1$ and $3$,
 and also swap rows $2$ and $4$.)  The matrix $A^{-1}$ appears as the
 right hand half of the final result.
\end{solution}

\section{Lecture 7}

\begin{exercise}\label{ex-det-i}
 Calculate the determinant of the matrix
 \[ A = 
     \bbm 
      a & 0 & b & c \\
      d & 0 & 0 & 0 \\
      e & f & g & h \\
      i & 0 & 0 & j 
     \ebm
 \]
\end{exercise}
\begin{solution}
 The most obvious approach is to expand along the top row.  This gives 
 \[ \det(A) = a\det(B_1) - 0\det(B_2) + b\det(B_3) + c\det(B_4), \]
 where 
 \[ 
  B_1 = \bbm 0&0&0 \\ f&g&h \\ 0&0&j \ebm \qquad
  B_2 = \bbm d&0&0 \\ e&g&h \\ i&0&j \ebm \qquad
  B_3 = \bbm d&0&0 \\ e&f&h \\ i&0&j \ebm \qquad
  B_4 = \bbm d&0&0 \\ e&f&g \\ i&0&0 \ebm
 \]
 As $B_1$ has a row of zeros we have $\det(B_1)=0$.  As $\det(B_2)$
 gets multiplied by zero, we need not evaluate it.  Straightforward
 expansion gives $\det(B_3)=dfj$ and $\det(B_4)=0$.  Putting this
 together, we get $\det(A)=bdfj$.

 Alternatively, we can expand $\det(A)$ down the second column, and
 then along the second row, giving 
 \[ 
  \det(A) = (-1)^{3+2}f \det\bbm a&b&c \\ d&0&0 \\ i&0&j \ebm 
    = (-1)^{3+2} (-1)^{2+1} fd\det\bbm b&c \\ 0&j\ebm 
    = fdbj = bdfj.
 \]
\end{solution}

\begin{exercise}\label{ex-det-ii}
 Consider the matrix
 \[ A = \bbm a & b & c & d \\
             e & 0 & 0 & f \\
             g & 0 & 0 & h \\
             i & j & k & l \ebm.
 \]
 Prove that $\det(A)=\det\bbm e&f\\g&h\ebm\det\bbm b&c\\j&k\ebm$.
 (You can reduce the work involved if you choose carefully how to
 expand the determinant.)
\end{exercise}
\begin{solution}
 We expand along the second row.  Note that $e$ occurs in the $(2,1)$
 position and so comes with a sign $(-1)^{2+1}=-1$, whereas $f$ occurs
 in the $(2,4)$ position with a sign $(-1)^{2+4}=+1$.  We thus have
 \[ \det(A) = -e\det\bbm b&c&d \\ 0&0&h \\ j&k&l \ebm
              +f\det\bbm a&b&c \\ g&0&0 \\ i&j&k \ebm.
 \]
 We now expand out these two $3\tm 3$ determinants along the middle
 row.  Note that $h$ is in the $(2,3)$ position of the first $3\tm 3$
 matrix and so comes with a sign $-1$, and $g$ is in the $(2,1)$
 position of the second $3\tm 3$ matrix and so also comes with a sign
 $-1$.  This gives 
 \begin{align*}
  \det\bbm b&c&d \\ 0&0&h \\ j&k&l \ebm
   &= -h \det\bbm b&c\\ j&k \ebm = -h(bk-cj) \\
  \det\bbm a&b&c \\ e&0&0 \\ i&j&k \ebm
   &= -g \det\bbm b&c\\ j&k \ebm = -g(bk-cj).
 \end{align*}
 Putting this together we get 
 \[ \det(A) = (-e)(-h)(bk-cj) +f(-g)(bk-cj)
            = (eh-fg)(bk-cj) 
            = \det\bbm e&f\\g&h\ebm\det\bbm b&c\\j&k\ebm.
 \]
\end{solution}

\begin{exercise}\label{ex-det-iii}
 Calculate the determinant of the matrix
 \[ 
  A = \bbm
       a & a & a & a \\
       a & b & b & b \\
       a & b & c & c \\
       a & b & c & d 
      \ebm.
 \]
 (The easiest method is to start with some carefully chosen row
 operations as in Method~\ref{meth-det-ro}.) 
\end{exercise}
\begin{solution}
 We subtract the third row from the fourth row, the second row from
 the third row, and the first row from the second row to get a new
 matrix $B$:
 \[ 
  A = \bbm
       a & a & a & a \\
       a & b & b & b \\
       a & b & c & c \\
       a & b & c & d 
      \ebm
  \to 
      \bbm
       a & a   & a   & a   \\
       0 & b-a & b-a & b-a \\
       0 & 0   & c-b & c-b \\
       0 & 0   & 0   & d-c 
      \ebm
  = B.
 \]
 As we have not swapped any rows or multiplied any rows by a constant,
 there are no correcting factors and Method~\ref{meth-det-ro} just
 tells us that $\det(A)=\det(B)$.  As $B$ is upper triangular, the
 determinant is just the product of the diagonal entries, giving
 \[ \det(A) = a(b-a)(c-b)(d-c). \]
\end{solution}

\begin{exercise}\label{ex-adjugate-circulant}
 Find the adjugate, determinant and inverse of the matrix
 $C=\bbm a & b & c \\
         b & c & a \\
         c & a & b \ebm$.  \\
 (Note that the intermediate calculations that you need for $\det(C)$
 are a subset of those that you need for $\adj(C)$.  Try not to repeat
 work unnecessarily.)
\end{exercise}
\begin{solution}
 The minors are 
 \begin{align*}
  m_{11} &= \det\bbm c & a \\ a & b \ebm = bc-a^2 &
  m_{12} &= \det\bbm b & a \\ c & b \ebm = b^2-ac &
  m_{13} &= \det\bbm b & c \\ c & a \ebm = ab-c^2 \\
  m_{21} &= \det\bbm b & c \\ a & b \ebm = b^2-ac &
  m_{22} &= \det\bbm a & c \\ c & b \ebm = ab-c^2 &
  m_{23} &= \det\bbm a & b \\ c & a \ebm = a^2-bc \\
  m_{31} &= \det\bbm b & c \\ c & a \ebm = ab-c^2 &
  m_{32} &= \det\bbm a & c \\ b & a \ebm = a^2-bc &
  m_{33} &= \det\bbm a & b \\ b & c \ebm = ac-b^2. 
 \end{align*}
 This gives
 \begin{align*} 
  \adj(C) &= \bbm  m_{11} & -m_{21} &  m_{31} \\
                  -m_{12} &  m_{22} & -m_{32} \\
                   m_{13} & -m_{23} &  m_{33} \ebm 
           = \bbm  bc-a^2 & ac-b^2 & ab-c^2 \\
                   ac-b^2 & ab-c^2 & bc-a^2 \\
                   ab-c^2 & bc-a^2 & ac-b^2 \ebm
                   \\
  \det(C) &= C_{11}m_{11} - C_{12}m_{12} + C_{13}m_{13} 
           = a(bc-a^2) - b(b^2-ac) + c(ab-c^2) \\
          &= 3abc-a^3-b^3-c^3 \\
  C^{-1}  &= \frac{\adj(C)}{\det(C)} 
           = \frac{1}{3abc-a^3-b^3-c^3}
             \bbm  bc-a^2 & ac-b^2 & ab-c^2 \\
                   ac-b^2 & ab-c^2 & bc-a^2 \\
                   ab-c^2 & bc-a^2 & ac-b^2 \ebm
 \end{align*}
\end{solution}

\begin{exercise}\label{ex-adjugate-hilbert}
 Find the adjugate, determinant and inverse of the matrix
 $H=\bbm 1&1/2&1/3 \\ 1/2&1/3&1/4 \\ 1/3&1/4&1/5 \ebm$.  \\
 (Note again that the intermediate calculations that you need for
 $\det(H)$ are a subset of those that you need for $\adj(H)$.)
\end{exercise}
\begin{solution}
 The minors are 
 \begin{align*}
  m_{11} &= \det\bbm 1/3&1/4 \\ 1/4&1/5 \ebm = \frac{1}{240} &
  m_{12} &= \det\bbm 1/2&1/4 \\ 1/3&1/5 \ebm = \frac{1}{60} &
  m_{13} &= \det\bbm 1/2&1/3 \\ 1/3&1/4 \ebm = \frac{1}{72} \\
  m_{21} &= \det\bbm 1/2&1/3 \\ 1/4&1/5 \ebm = \frac{1}{60} &
  m_{22} &= \det\bbm 1  &1/3 \\ 1/3&1/5 \ebm = \frac{4}{45} &
  m_{23} &= \det\bbm 1  &1/2 \\ 1/3&1/4 \ebm = \frac{1}{12} \\
  m_{31} &= \det\bbm 1/2&1/3 \\ 1/3&1/4 \ebm = \frac{1}{72} &
  m_{32} &= \det\bbm 1  &1/3 \\ 1/2&1/4 \ebm = \frac{1}{12} &
  m_{33} &= \det\bbm 1  &1/2 \\ 1/2&1/3 \ebm = \frac{1}{12}.
 \end{align*}
 This gives
 \begin{align*} 
  \adj(H) &= \bbm  m_{11} & -m_{21} &  m_{31} \\
                  -m_{12} &  m_{22} & -m_{32} \\
                   m_{13} & -m_{23} &  m_{33} \ebm 
           = \renewcommand{\arraystretch}{1.3} \bbm 
                   \frac{1}{240} & -\frac{1}{60} & \frac{1}{72} \\
                  -\frac{1}{60} &  \frac{4}{45} & -\frac{1}{12} \\
                   \frac{1}{72} & -\frac{1}{12} &  \frac{1}{12} \ebm
                   \\
  \det(H) &= H_{11}m_{11} - H_{12}m_{12} + H_{13}m_{13} 
           = \frac{1}{240} - \frac{1}{2}\tm\frac{1}{60} +
             \frac{1}{3}\tm\frac{1}{72} \\
          &= \frac{1}{2160} \\
  H^{-1} &= \adj(H)/\det(H) 
           = \renewcommand{\arraystretch}{1.3} \bbm 
                   \frac{2160}{240} & -\frac{2160}{60} & \frac{2160}{72} \\
                  -\frac{2160}{60} &  \frac{4\tm 2160}{45} & -\frac{2160}{12} \\
                   \frac{2160}{72} & -\frac{2160}{12} &
                   \frac{2160}{12} \ebm \\
         &= \bbm   9 &  -36 &   30 \\
                 -36 &  192 & -180 \\
                  30 & -180 &  180 \ebm.
 \end{align*}
\end{solution}


\section{Lecture 8}

\begin{exercise}\label{ex-char-companion}
 Find the characteristic polynomial of the matrix
 \[ A = \bbm
     0 & 0 & 0 & -d \\
     1 & 0 & 0 & -c \\
     0 & 1 & 0 & -b \\
     0 & 0 & 1 & -a
    \ebm
 \]
\end{exercise}
\begin{solution}
 The characteristic polynomial is the determinant of the matrix
 \[ A-tI = \bbm
    -t &  0 &  0 & -d \\
     1 & -t &  0 & -c \\
     0 &  1 & -t & -b \\
     0 &  0 &  1 & -a-t
    \ebm.
 \]
 Expanding along the top row, we get
 \[ \det(A-tI) = 
    -t\det\bbm -t &  0 & -c \\
                1 & -t & -b \\
                0 &  1 & -a-t \ebm 
    +d\det\bbm  1 & -t &  0 \\
                0 &  1 & -t \\
                0 &  0 &  1 \ebm. 
 \]
 The second matrix above is upper triangular and so the determinant is
 easily seen to be one.  For the first matrix we have
 \[ \det\bbm -t &  0 & -c \\
              1 & -t & -b \\
              0 &  1 & -a-t \ebm = 
    -t \det\bbm -t & -b \\ 1 & -a-t \ebm 
    -c \det\bbm 1 & -t \\ 0 & 1 \ebm =
    -t(t^2+at+b) - c = -(t^3+at^2+bt+c).
 \]
 Putting this together, we get 
 \[ \det(A-tI) =
     t(t^3+at^2+bt+c)+d = t^4+at^3+bt^2+ct+d.
 \]
\end{solution}

\begin{exercise}\label{ex-evectors-i}
 Find the characteristic polynomial, eigenvalues and all the corresponding
 eigenvectors of the matrix
 \[ A = \bbm 1 & 4 & 6 \\ 0 & 2 & 5 \\ 0 & 0 & 3 \ebm \]
\end{exercise}
\begin{solution}
 The characteristic polynomial is
    $$ \det (A - tI_3) =
    \det \bbm
    1-t & 4 & 6 \\
    0 & 2-t & 5 \\
    0 & 0 & 3-t
    \ebm=-(t-1)(t-2)(t-3).
    $$
 (Recall that the determinant of an upper triangular $3 \times 3$
 matrix is the product of the diagonal entries.) Hence the
 eigenvalues of $A$ are $1$, $2$ and $3$.

 To find the eigenvectors corresponding to the eigenvalue $1$, we
 solve the system of linear equations $(A - 1I_3)\left[
 \begin{array}{c}x \\ y\\z
 \end{array} \right] = 0$. The augmented matrix of this system
 \begin{align*}
 \big(A - I_3|0\big) & = \bbm 0& 4&
 6&0\\0&1&5&0\\0&0&2&0\ebm \sim \bbm 0& 2&
 3&0\\0&1&5&0\\0&0&2&0\ebm\\& \sim \bbm 0& 2&
 3&0\\0&0&\frac{7}{2}&0\\0&0&2&0\ebm \sim \bbm
 0& 2& 3&0\\0&0&2&0\\0&0&0&0\ebm,
 \end{align*}
 in row echelon form. We therefore see by back substitution that
 the general solution of the system is given by $z = 0$, $y = 0$,
 $x = \mu$ where $\mu$ can be any number; therefore the set of
 eigenvectors of $A$ corresponding to the eigenvalue $1$ is
 $$
 \left\{ \mu \left[ \begin{array}{c} 1\\0\\0
 \end{array} \right] : 0 \neq \mu \in \R \right\}.
 $$

 To find the eigenvectors corresponding to the eigenvalue $2$, we
 solve the system of linear equations $(A - 2I_3)\left[
 \begin{array}{c}x \\ y\\z
 \end{array} \right] = 0$. The augmented matrix of this system
 $$
 \big(A - 2I_3|0\big) = \bbm -1& 4&
 6&0\\0&0&5&0\\0&0&1&0\ebm \sim \bbm -1& 4&
 6&0\\0&0&1&0\\0&0&0&0\ebm,
 $$
 in row echelon form. We therefore see by back substitution that
 the general solution of the system is given by $z = 0$, $y = \mu$,
 $x = 4\mu$ where $\mu$ can be any number; therefore the set of
 eigenvectors of $A$ corresponding to the eigenvalue $2$ is
 $$
 \left\{ \mu \left[ \begin{array}{c} 4\\1\\0
 \end{array} \right] : 0 \neq \mu \in \R \right\}.
 $$

 To find the eigenvectors corresponding to the eigenvalue $3$, we
 solve the system of linear equations $(A - 3I_3)\left[
 \begin{array}{c}x \\ y\\z
 \end{array} \right] = 0$. The augmented matrix of this system
 $$
 \big(A - 3I_3|0\big) = \bbm -2& 4&
 6&0\\0&-1&5&0\\0&0&0&0\ebm
 $$
 is in row echelon form. We therefore see by back substitution that
 the general solution of the system is given by $z = \mu$, $y =
 5\mu$, $x = \frac{1}{2}(4(5\mu) + 6\mu) = 13\mu$,  where $\mu$ can
 be any number; therefore the set of eigenvectors of $A$
 corresponding to the eigenvalue $3$ is
 $$
 \left\{ \mu \left[ \begin{array}{c} 13\\5\\1
 \end{array} \right] : 0 \neq \mu \in \R \right\}.
 $$
\end{solution}

\begin{exercise}\label{ex-evectors-ii}
 Find the characteristic polynomial, eigenvalues and all the corresponding
 eigenvectors of the matrix
 \[ B = \bbm 3 & 2 & 1 \\ 0 & 1 & 2 \\ 0 & 1 & -1 \ebm \]
\end{exercise}
\begin{solution}
 The characteristic polynomial is
    \begin{align*} \det (B - tI_3) & =
    \det \bbm
    3-t & 2 & 1 \\
 0 & 1-t & 2 \\
 0 & 1 & -1-t
    \ebm=-(t-3)((1-t)(-1-t) - 2)\\ & = -(t-3)(t^2-3)
    = -(t-3)(t - \sqrt{3})(t+\sqrt{3}).
    \end{align*}
 Hence the eigenvalues of $B$ are $3$, $\sqrt{3}$ and $-\sqrt{3}$.

 To find the eigenvectors corresponding to the eigenvalue $3$, we
 solve the system of linear equations $(B - 3I_3)\left[
 \begin{array}{c}x \\ y\\z
 \end{array} \right] = 0$. The augmented matrix of this system
 $$
 \big(B - 3I_3|0\big) = \bbm 0& 2&
 1&0\\0&-2&2&0\\0&1&-4&0\ebm \sim \bbm 0& 2&
 1&0\\0&0&3&0\\0&0&-\frac{9}{2}&0\ebm \sim \bbm
 0& 2& 1&0\\0&0&3&0\\0&0&0&0\ebm,
 $$
 in row echelon form. We therefore see by back substitution that
 the general solution of the system is given by $z = 0$, $y = 0$,
 $x = \mu$ where $\mu$ can be any number; therefore the set of
 eigenvectors of $B$ corresponding to the eigenvalue $1$ is
 $$
 \left\{ \mu \left[ \begin{array}{c} 1\\0\\0
 \end{array} \right] : 0 \neq \mu \in \R \right\}.
 $$

 To find the eigenvectors corresponding to the eigenvalue
 $\sqrt{3}$, we solve the system of linear equations $(B -
 \sqrt{3}I_3)\left[
 \begin{array}{c}x \\ y\\z
 \end{array} \right] = 0$. The augmented matrix of this system
 $$
 \big(B - \sqrt{3}I_3|0\big) = \bbm 3-\sqrt{3}& 2&
 1&0\\0&1-\sqrt{3}&2&0\\0&1&-1-\sqrt{3}&0\ebm \sim
 \bbm 3-\sqrt{3}& 2&
 1&0\\0&1&-1-\sqrt{3}&0\\0&0&0&0\ebm,
 $$
 in row echelon form. We therefore see by back substitution that
 the general solution of the system is given by $z = \mu$, $y =
 (1+\sqrt{3})\mu$, and $${\textstyle x =
 \left[\frac{(-2(1+\sqrt{3})-1)}{(3-\sqrt{3})}\right]\mu =
 \left[\frac{(-3
 -2\sqrt{3})(3+\sqrt{3})}{(3-\sqrt{3})(3+\sqrt{3})}\right]\mu =
 \left[\frac{-15-9\sqrt{3}}{6}\right]\mu =
 \left[\frac{-5-3\sqrt{3}}{2}\right]\mu,}
 $$
 where $\mu$ can be any number; therefore the set of eigenvectors
 of $B$ corresponding to the eigenvalue $\sqrt{3}$ is
 $$
 \left\{ \mu \left[ \begin{array}{c}
 \frac{-5-3\sqrt{3}}{2}\\[2pt]1+\sqrt{3}\\[2pt]1
 \end{array} \right] : 0 \neq \mu \in \R \right\}.
 $$

 To find the eigenvectors corresponding to the eigenvalue
 $-\sqrt{3}$, we solve the system of linear equations $(B +
 \sqrt{3}I_3)\left[
 \begin{array}{c}x \\ y\\z
 \end{array} \right] = 0$. The augmented matrix of this system
 $$
 \big(B + \sqrt{3}I_3|0\big) = \bbm 3+\sqrt{3}& 2&
 1&0\\0&1+\sqrt{3}&2&0\\0&1&-1+\sqrt{3}&0\ebm \sim
 \bbm 3+\sqrt{3}& 2&
 1&0\\0&1&-1+\sqrt{3}&0\\0&0&0&0\ebm,
 $$
 in row echelon form. We therefore see by back substitution that
 the general solution of the system is given by $z = \mu$, $y =
 (1-\sqrt{3})\mu$, and $${\textstyle x =
 \left[\frac{(-2(1-\sqrt{3})-1)}{(3+\sqrt{3})}\right]\mu =
 \left[\frac{(-3
 +2\sqrt{3})(3-\sqrt{3})}{(3+\sqrt{3})(3-\sqrt{3})}\right]\mu =
 \left[\frac{-15+9\sqrt{3}}{6}\right]\mu =
 \left[\frac{-5+3\sqrt{3}}{2}\right]\mu,}
 $$
 where $\mu$ can be any number; therefore the set of eigenvectors
 of $B$ corresponding to the eigenvalue $-\sqrt{3}$ is
 $$
 \left\{ \mu \left[ \begin{array}{c}
 \frac{-5+3\sqrt{3}}{2}\\[2pt]1-\sqrt{3}\\[2pt]1
 \end{array} \right] : 0 \neq \mu \in \R \right\}.
 $$
\end{solution}

\begin{exercise}\label{ex-evectors-iii}
 Show, directly from the definition of eigenvalue, that 0 is an eigenvalue
 of the matrix
 \[ N := \bbm 
          0 & 1 & 0 & 0 \\
          0 & 0 & 1 & 0 \\
          0 & 0 & 0 & 1 \\
          0 & 0 & 0 & 0
         \ebm.
 \]
 Show, also directly from the definition of eigenvalue, that an arbitrary non-zero
 number $k$ is not an eigenvalue of $N$.
 Find all the eigenvectors of $N$.
\end{exercise}
\begin{solution}
 A real number $k$ is an eigenvalue of $N$ if and only if the
 system of linear equations \[(N-kI_4)\bbm
 x\\y\\z\\w\ebm = 0 \tag{$\dagger_k$}\] has a non-trivial
 solution.

 When $k = 0$, the augmented matrix of $(\dagger_0)$ is
 $$\big(N|0\big)= \bbm
 0&1&0&0&0\\0&0&1&0&0\\0&0&0&1&0\\0&0&0&0&0\ebm$$ and this
 is already in row echelon form. Thus we see by back substitution
 that the complete solution of $(\dagger_0)$ is $w = 0$, $z = 0$,
 $y= 0$, $x = \mu$, where $\mu$ can be any number. Thus there is a
 non-trivial solution to $(\dagger_0)$, and so $0$ is an eigenvalue
 of $N$. Also the set of eigenvectors of $N$ corresponding to the
 eigenvalue $0$ is
 $$
 \left\{ \mu\bbm 1\\0\\0\\0\ebm : 0 \neq \mu
 \in \R \right\}.
 $$

 Now consider the case where $k \neq 0$. Then the augmented matrix
 of $(\dagger_k)$ is $$\big(N-kI_4|0\big) = \bbm
 k&1&0&0&0\\0&k&1&0&0\\0&0&k&1&0\\0&0&0&k&0\ebm$$ and this
 is already in row echelon form. We see, by back substitution, that
 (because $k \neq 0$) the complete solution of $(\dagger_k)$ is $w
 = 0$, $z = 0$, $y = 0$, $x=0$. Thus the only solution of
 $(\dagger_k)$ is the trivial one, and therefore $k$ is not an
 eigenvalue of $N$.
\end{solution}

\begin{exercise}\label{ex-evectrs-iv}
 Find the characteristic polynomial, eigenvalues and eigenvectors of
 the matrix 
 \[ A = \bbm 
          3 & -5 & 5 \\
          2 & -4 & 5 \\
          2 & -2 & 3
         \ebm.
 \]
\end{exercise}
\begin{solution}
 The characteristic polynomial is
     \begin{align*} \det (A - tI_3) &=
     \det \bbm
     3-t & -5 & 5 \\
     2 & -4-t & 5 \\
     2 & -2 & 3-t
     \ebm= \left|\begin{array}{ccc}
 1-t & t-1 & 0 \\
     2 & -4-t & 5 \\
     0 & t+2 & -2-t
     \end{array}\right|\\& \quad \text{~(on subtracting the middle
     row from each of the other two rows)}\\ &
     = (1-t)(2+t)\left|\begin{array}{ccc}
 1 & -1 & 0 \\
     2 & -4-t & 5 \\
     0 & 1 & -1
     \end{array}\right| = (1-t)(2+t)\left|\begin{array}{ccc}
 1 & 0 & 0 \\
     2 & -2-t & 5 \\
     0 & 1 & -1
     \end{array}\right|\\& \quad \text{~(on adding the first column
     to the second column)}\\ &
     =(1-t)(2+t)(2+t-5) = -(t-1)(t+2)(t-3).
     \end{align*}
  Hence the eigenvalues of $A$
 are $1$, $-2$ and $3$.

 To find the eigenvectors corresponding to the eigenvalue $1$, we
 solve the system of linear equations $(A - 1I_3)\left[
 \begin{array}{c}x \\ y\\z
 \end{array} \right] = 0$. The augmented matrix of this system
 $$
 \big(A - I_3|0\big) = \bbm 2 & -5 & 5 &0\\
     2 & -5 & 5 &0\\
     2 & -2 & 2&0\ebm \sim
 \bbm 2 & -5 & 5 &0\\
     0 & 0 & 0 &0\\
     0 & 3 & -3&0\ebm \sim
 \bbm 2 & -5 & 5 &0\\
     0 & 3 & -3 &0\\
     0 & 0 & 0&0\ebm,
 $$
 in row echelon form. We therefore see by back substitution that
 the general solution of the system is given by $z = \mu$, $y =
 \mu$, $x = \frac{1}{2}(5\mu - 5\mu) = 0$ where $\mu$ can be any
 number; therefore the set of eigenvectors of $A$ corresponding to
 the eigenvalue $1$ is
 $$
 \left\{ \mu \left[ \begin{array}{c} 0\\1\\1
 \end{array} \right] : 0 \neq \mu \in \R \right\}.
 $$

 To find the eigenvectors corresponding to the eigenvalue $-2$, we
 solve the system of linear equations $(A + 2I_3)\left[
 \begin{array}{c}x \\ y\\z
 \end{array} \right] = 0$. The augmented matrix of this system
 $$
 \big(A +2 I_3|0\big) = \bbm 5 & -5 & 5 &0\\
     2 & -2 & 5 &0\\
     2 & -2 & 5&0\ebm \sim
 \bbm 1 & -1 & 1 &0\\
     0 & 0 & 3 &0\\
     0 & 0 & 0&0\ebm,
 $$
 in row echelon form. We therefore see by back substitution that
 the general solution of the system is given by $z = 0$, $y = \mu$,
 $x = \mu$ where $\mu$ can be any number; therefore the set of
 eigenvectors of $A$ corresponding to the eigenvalue $-2$ is
 $$
 \left\{ \mu \left[ \begin{array}{c} 1\\1\\0
 \end{array} \right] : 0 \neq \mu \in \R \right\}.
 $$

 To find the eigenvectors corresponding to the eigenvalue $3$, we
 solve the system of linear equations $(A - 3I_3)\left[
 \begin{array}{c}x \\ y\\z
 \end{array} \right] = 0$. The augmented matrix of this system
 $$
 \big(A - 3I_3|0\big) = \bbm 0 & -5 & 5 &0\\
     2 & -7 & 5 &0\\
     2 & -2 & 0&0\ebm \sim
 \bbm 2 & -2 & 0 &0\\
     2 & -7 & 5 &0\\
     0 & -5 & 5&0\ebm \sim
 \bbm 2 & -2 & 0 &0\\
     0 & -5 & 5 &0\\
     0 & -5 & 5&0\ebm \sim
 \bbm 2 & -2 & 0 &0\\
     0 & -5 & 5 &0\\
     0 & 0 & 0&0\ebm,
 $$
 in row echelon form. We therefore see by back substitution that
 the general solution of the system is given by $z = \mu$, $y =
 \mu$, $x = \mu$,  where $\mu$ can be any number; therefore the set
 of eigenvectors of $A$ corresponding to the eigenvalue $3$ is
 $$
 \left\{ \mu \left[ \begin{array}{c} 1\\1\\1
 \end{array} \right] : 0 \neq \mu \in \R \right\}.
 $$
\end{solution}

\begin{exercise}\label{ex-evectors-independent}
 A be an $n\tm n$ matrix, and let $\lm_1,\dotsc,\lm_h$ be $h$ distinct
 eigenvalues of $A$.   For each $i=1,\dotsc,h$, let the vectors
 $v_{i,1},\dotsc,v_{i,t_i}$ be linearly independent eigenvectors of
 $A$ all corresponding to the eigenvalue $\lm_i$.  We collect these
 lists together into a single list 
 \[ v_{1,1},\dotsc,v_{1,t_1},
    v_{2,1},\dotsc,v_{2,t_2},\dotsc,
    v_{h,1},\dotsc,v_{h,t_h}.
 \]
 Prove (as was stated in lectures) that this list is linearly independent.
\end{exercise}
\begin{solution}
 For each $i = 1, \ldots, h$, the vectors $v_{i1}, \ldots,
 v_{it_i}$ are linearly independent eigenvectors of $A$ all
 corresponding to the eigenvalue
 $\lambda_i$. We show that
 $$
 v_{11}, \ldots, v_{1t_1},v_{21}, \ldots, v_{2t_2}, \ldots, v_{h1},
 \ldots, v_{ht_h}
 $$
 (taken all together) are linearly independent by induction on $h$.

 When $h = 1$, there is nothing to prove, because we are given that
 $v_{11}, \ldots, v_{1t_1}$ are linearly independent.

 Assume now that $h>1$ and that the claim is true for $h-1$
 distinct eigenvalues of $A$.

 Let
 $$
 a_{11}, \ldots, a_{1t_1},a_{21}, \ldots, a_{2t_2}, \ldots, a_{h1},
 \ldots, a_{ht_h}
 $$
 be scalars such that
 \begin{equation}
 \sum_{i=1}^h \sum_{j=1}^{t_i} a_{ij}v_{ij} = 0.
 \end{equation}
 Then
 $$
 0 = A0 = A\left[\sum_{i=1}^h \sum_{j=1}^{t_i} a_{ij}v_{ij}\right]
 = \sum_{i=1}^h \sum_{j=1}^{t_i} a_{ij}Av_{ij}$$
 and so
 \begin{equation}
 \sum_{i=1}^h \sum_{j=1}^{t_i} a_{ij}\lambda_iv_{ij} = 0
 \end{equation}
 because $Av_{ij} = \lambda_iv_{ij}$ for all $j = 1, \ldots, t_i$ and
 $i = 1, \ldots, h$. If we
 now subtract $\lambda_h$ times (1) from (2) we get
 $$
 \sum_{i=1}^h \sum_{j=1}^{t_i} a_{ij}(\lambda_i - \lambda_h)v_{ij} = 0,
 $$
 that is
 $$
 \sum_{i=1}^{h-1} \sum_{j=1}^{t_i} a_{ij}(\lambda_i - \lambda_h)v_{ij} = 0.
 $$

 By the induction hypothesis, $$
 v_{11}, \ldots, v_{1t_1},v_{21}, \ldots, v_{2t_2}, \ldots,
 v_{h-1,1},
 \ldots, v_{h-1,t_{h-1}}
 $$
 (taken all together) are linearly independent. Therefore
 $$
 a_{ij}(\lambda_i - \lambda_h) = 0 \quad \text{~for all~} j = 1,
 \ldots, t_i
 \text{~and~} i = 1, \ldots, h-1.
 $$
 Since $\lambda_i - \lambda_h \neq 0$ for all $i = 1, \ldots, h-1$, it follows that
 $$
 a_{ij} = 0 \quad \text{~for all~} j = 1, \ldots, t_i
 \text{~and~} i = 1, \ldots, h-1.
 $$
 With this information, equation (1) now simplifies to
 $$
 \sum_{j=1}^{t_h} a_{hj}v_{hj} = 0,
 $$
 and so it follows from the fact that $v_{h1}, \ldots, v_{ht_h}$ are
 linearly independent that $a_{h1}= \cdots = a_{ht_h} = 0$.

 We have now shown that
 $$
 v_{11}, \ldots, v_{1t_1},v_{21}, \ldots, v_{2t_2}, \ldots, v_{h1},
 \ldots, v_{ht_h}
 $$
 are linearly independent. This completes the inductive step. By the
 Principle of
 Mathematical Induction, the claim is proved.

\end{solution}

\section{Lecture 9}

\begin{exercise}\label{ex-diagonal-i}
 Consider the matrix $A=\bbm 4&1\\ -6&9\ebm$.  Find an invertible 
 matrix $U$ and a diagonal matrix $D$ such that $A=UDU^{-1}$.  Check
 directly that the equation $A=UDU^{-1}$ holds.
\end{exercise}
\begin{solution}
 The characteristic polynomial is
 \[ \chi_A(t)
      = \det\bbm 4-t & 1 \\ -6 & 9-t \ebm 
      = (4-t)(9-t) - (-6) = t^2 -13t +42 = (t-6)(t-7).
 \]
 Thus, the eigenvalues are $\lm_1=6$ and $\lm_2=7$.  To find the
 corresponding eigenvectors we use the following row-reductions:
 \begin{align*}
  A-\lm_1I &= \bbm -2&1 \\ -6&3 \ebm 
            \to \bbm 1&-1/2 \\ -6&3 \ebm 
            \to \bbm 1&-1/2 \\ 0&0 \ebm 
            = B_1 \\
  A-\lm_2I &= \bbm -3&1 \\ -6&2 \ebm 
            \to \bbm 1&-1/3 \\ -6&2 \ebm 
            \to \bbm 1&-1/3 \\ 0&0 \ebm 
            = B_2
 \end{align*}
 The eigenvector $u_1$ must satisfy $B_1u_1=0$, and it is clear that
 $u_1=\bbm 1\\2\ebm$ will do.   Similarly, the eigenvector $u_2$ must
 satisfy $B_2u_2=0$, and it is clear that $u_2=\bbm 1\\3\ebm$ will do.
 We now take
 \begin{align*}
  U &= [u_1|u_2] = \bbm 1 & 1 \\ 2 & 3 \ebm \\
  D &= \diag(\lm_1,\lm_2) = \bbm 6 & 0 \\ 0 & 7 \ebm.
 \end{align*}
 The general method (Proposition~\ref{prop-diagonal-basis}) tells us
 that $A=UDU^{-1}$.  To check this directly, we need to work out
 $U^{-1}$.  The general formula
 \[ \bbm a&b\\ c&d\ebm^{-1} = \frac{1}{ad-bc}\bbm d&-b \\-c&a\ebm \]
 gives 
 \[ U^{-1}=\frac{1}{3-2}\bbm 3&-1\\-2&1\ebm = \bbm 3&-1\\-2&1\ebm. \]
 We thus have
 \[ UDU^{-1} = 
     \bbm 1 & 1 \\ 2 & 3 \ebm
     \bbm 6 & 0 \\ 0 & 7 \ebm
     \bbm 3 & -1\\ -2 & 1 \ebm =
     \bbm 1 & 1 \\ 2 & 3 \ebm
     \bbm 18 & -6 \\ -14 & 7 \ebm =
     \bbm 4 & 1 \\ -6 & 9 \ebm.
 \]
 As expected, this is the same as $A$.
\end{solution}

\begin{exercise}\label{ex-diagonal-ii}
 Show that the matrix $A=\bbm 4 & 1 \\ -1 & 2\ebm$ cannot be
 diagonalised. 
\end{exercise}
\begin{solution}
 The characteristic polynomial is 
 \[ \det(A-tI) = \det\bbm 4-t & 1 \\ -1 & 2-t \ebm =
     (4-t)(2-t)+1 = 9 - 6t + t^2 = (t-3)^2.
 \]
 This shows that the only eigenvalue is $3$.  The eigenvectors of
 eigenvalue $3$ are the vectors $u=\bbm x\\y\ebm$ satisfying
 $(A-3I)u=0$.  Here $A-3I=\bbm 1&1\\-1&-1\ebm$, so
 $(A-3I)u=\bbm x+y\\-x-y\ebm$.  This means that $u$ is an eigenvector
 iff $x+y=0$, or in other words $u=\bbm x\\-x\ebm=x\bbm 1\\-1\ebm$.
 As every eigenvector is a nonzero multiple of $\bbm 1\\ -1\ebm$, we
 see that any two eigenvectors are multiples of each other and so are
 linearly dependent.  Thus, there is no basis of eigenvectors.
 Proposition~\ref{prop-diagonal-basis} therefore tells us that $A$
 cannot be diagonalised.
\end{solution}

\begin{exercise}\label{ex-diagonal-iii}
 Consider the matrix
 \[ A = \bbm 100&10&1 \\ 100&10&1 \\ 100&10&1 \ebm. \]
 Find a basis for $\R^3$ consisting of eigenvectors for $A$.  Using
 this, find a diagonalisation $A=UDU^{-1}$.
\end{exercise}
\begin{solution}
 The characteristic polynomial is as follows.
 \begin{align*}
  \chi_A(t)
   &= \det\bbm 100-t & 10 & 1 \\ 
               100 & 10-t & 1 \\
               100 & 10 & 1-t \ebm 
    = (100-t)\det\bbm 10-t & 1 \\
                      10   & 1-t \ebm 
      -10    \det\bbm 100  & 1 \\
                      100 & 1-t \ebm 
      +      \det\bbm 100 & 10-t \\
                      100 & 10 \ebm \\
   &= (100-t)(t^2-11t) -10(-100t)+(100t) = -t^3+111t^2 = -t^2(t-111).
 \end{align*}
 It follows that the eigenvalues are $0$ and $111$.  The eigenvectors
 of eigenvalue $0$ are the vectors $u=\bbm x&y&z\ebm^T$ satisfying
 $Au=0$ or equivalently $100x+10y+z=0$.  This gives $z=-100x-10y$, so 
 \[ u
      = \bbm x\\y\\-100x-10y\ebm 
      = x \bbm 1\\0\\-100 \ebm + y \bbm 0\\1\\-10\ebm.
 \]
 Taking $x=1$ and $y=0$ gives $u_1=\bbm 1&0&-100\ebm^T$.  Taking $x=0$
 and $y=1$ gives $u_2=\bbm 0\\1\\-10\ebm$.  These are two linearly
 independent eigenvectors of eigenvalue zero.  

 Next, to find an eigenvector of eigenvalue $111$ we row-reduce the
 matrix $A-111I$.  If we row-reduce in the obvious way we get the
 following sequence:
 \[
  \bbm
  1&-10/11&-1/11\\
  100&-101&1\\
  100&10&-110\\
  \ebm
  \to
  \bbm
  1&-10/11&-1/11\\
  0&-111/11&111/11\\
  100&10&-110\\
  \ebm
  \to
  \bbm
  1&-10/11&-1/11\\
  0&-111/11&111/11\\
  0&1110/11&-1110/11\\
  \ebm
  \to
  \] \[
  \bbm
  1&-10/11&-1/11\\
  0&1&-1\\
  0&1110/11&-1110/11\\
  \ebm
  \to
  \bbm
  1&-10/11&-1/11\\
  0&1&-1\\
  0&0&0\\
  \ebm
  \to
  \bbm
  1&0&-1\\
  0&1&-1\\
  0&0&0\\
  \ebm
 \]
 If we proceed in a more creative order we can avoid fractions:
 \[ \bbm
     -11 &   10 & 1 \\
     100 & -101 & 1 \\
     100 &   10 & -110
    \ebm 
    \to
    \bbm
     -11 &   10 & 1 \\
     100 & -101 & 1 \\
       0 &  111 & -111
    \ebm 
    \to
    \bbm
     -11 &   10 & 1 \\
     100 & -101 & 1 \\
       0 &    1 & -1
    \ebm 
    \to
  \] \[
    \bbm
     -11 &    0 & 11 \\
     100 &    0 & -100 \\
       0 &    1 & -1
    \ebm 
    \to
    \bbm
       1 &    0 & -1 \\
       0 &    0 &  0 \\
       0 &    1 & -1
    \ebm 
    \to
    \bbm
       1 &    0 & -1 \\
       0 &    1 & -1 \\
       0 &    0 &  0 
    \ebm 
 \]
 Either way, we get the same final matrix $B$.  An eigenvector
 $u=\bbm x&y&z\ebm^T$ of eigenvalue $111$ must satisfy $Bu=0$, which
 means that $x=z$ and $y=z$.  Thus, we can take
 $u_3=\bbm 1&1&1\ebm^T$.  In fact, if we were sufficiently alert we
 could have seen that this vector satisfies $Au_3=111u_3$ by
 inspection, and avoided the whole row-reduction process.
 We now put 
 \begin{align*}
  U &=
   \left[\begin{array}{c|c|c} && \\ u_1 & u_2 & u_3 \\ && \end{array}\right]
   = \bbm 1 & 0 & 1 \\ 0 & 1 & 1 \\ -100 & -10 & 1 \ebm \\
  D &= \diag(\lm_1,\lm_2,\lm_3) = \diag(0,0,111) = 
       \bbm 0&0&0 \\ 0&0&0 \\ 0&0&111 \ebm.
 \end{align*}
 The general theory now tells us that $A=UDU^{-1}$.  It would not be
 hard to check this directly, but the question does not ask us to do
 so.  We just record the value of $U^{-1}$ for any students who wish
 to check their work:
 \[ U^{-1} = \frac{1}{111} \bbm
      11&-10&-1\\
      -100&101&-1\\
      100&10&1\\
      \ebm.
 \]
\end{solution}

\begin{exercise}\label{ex-diagonal-iv}
 Consider the matrix
 \[ A = \bbm 0&0&1&0 \\
             0&0&0&1 \\
             1&0&0&0 \\
             0&1&0&0 \ebm.
 \]
 Find a basis for $\R^4$ consisting of eigenvectors for $A$.  Using
 this, find a diagonalisation $A=UDU^{-1}$.  Ideally, you should do
 all this by inspection rather than using the characteristic
 polynomial and row-reduction.
\end{exercise}
\begin{solution}
 In terms of the standard basis vectors $e_i$, we have
 \[ Ae_1 = e_3 \hspace{4em}
    Ae_2 = e_4 \hspace{4em}
    Ae_3 = e_1 \hspace{4em}
    Ae_4 = e_2. 
 \]
 It follows that if we put 
 \[ u_1 = e_1+e_3 \hspace{4em}
    u_2 = e_2+e_4 \hspace{4em}
    u_3 = e_1-e_3 \hspace{4em}
    u_4 = e_2-e_4 
 \]
 then
 \[ Au_1 = u_1 \hspace{4em}
    Au_2 = u_2 \hspace{4em}
    Au_3 = u_3 \hspace{4em}
    Au_4 = u_4,
 \]
 so the vectors $u_i$ are eigenvectors, with eigenvalues
 $\lm_1=\lm_2=1$ and $\lm_3=\lm_4=-1$.  Thus, if we put 
 \[ U = [u_1|u_2|u_3|u_4] 
      = \bbm  1 &  0 &  1 &  0 \\
              0 &  1 &  0 &  1 \\
              1 &  0 & -1 &  0 \\
              0 &  1 &  0 & -1 \ebm
    \hspace{4em}
    D = \diag(\lm_1,\lm_2,\lm_3,\lm_4) 
      = \bbm  1 &  0 &  0 &  0 \\
              0 &  1 &  0 &  0 \\
              0 &  0 & -1 &  0 \\
              0 &  0 &  0 & -1 \ebm
 \]  
 then we have $A=UDU^{-1}$.  Also, it is not hard to see that
 $U^2=2I_4$, so $U^{-1}=\half U$.
\end{solution}

\begin{exercise}\label{ex-diagonal-v}
 Diagonalise the matrix $A=\bbm 1&1&1\\1&0&1\\1&1&1\ebm$. \\
 \textbf{Hint:} One of the eigenvalues, and the corresponding
 eigenvector, involves $\sqrt{3}$.  You can find another eigenvalue
 and eigenvector by just changing $\sqrt{3}$ to $-\sqrt{3}$
 everywhere.  You may also find it useful to remember the rule
 \[ \frac{1}{a+b\sqrt{3}} =
     \frac{a-b\sqrt{3}}{(a-b\sqrt{3})(a+b\sqrt{3})} = 
     \frac{a-b\sqrt{3}}{a^2-3b^2}.
 \]
\end{exercise}
\begin{solution}
 The characteristic polynomial is
 \begin{align*}
  \chi_A(t) &= 
   \det\bbm 1-t &  1 & 1 \\
            1   & -t & 1 \\
            1   &  1 & 1-t \ebm \\
   &= (1-t) \det \bbm -t &  1 \\ 1 & 1-t \ebm 
      -     \det \bbm  1 &  1 \\ 1 & 1-t \ebm 
      +     \det \bbm  1 & -t \\ 1 & 1   \ebm \\
   &= (1-t)(t^2-t-1)-(-t)+(1+t) 
    = t^2-t-1-t^3+t^2+t+t+1+t 
    = -t^3+2t^2+2t \\
   &= -t(t^2-2t-2).
 \end{align*}
 The quadratic formula tells that the roots of $t^2-2t-2$ are
 $1\pm\sqrt{3}$.  Thus, the eigenvalues are $\lm_1=0$ and
 $\lm_2=1+\sqrt{3}$ and $\lm_3=1-\sqrt{3}$.  By inspection, the vector
 $u_1=\bbm 1&0&-1\ebm^T$ satisfies $Au_1=0$, so it is an eigenvector
 of eigenvalue $0$.  To find an eigenvector of eigenvalue
 $\lm_2=1+\sqrt{3}$, we row-reduce the matrix $A-\lm_2I$:
 \[
  \bbm
  -\sqrt{3}&1&1\\
  1&-1-\sqrt{3}&1\\
  1&1&-\sqrt{3}\\
  \ebm
  \xra{1}
  \bbm
  -\sqrt{3}&1&1\\
  1&-1-\sqrt{3}&1\\
  0&2+\sqrt{3}&-1-\sqrt{3}\\
  \ebm
  \xra{2}
  \bbm
  0&-2-\sqrt{3}&1+\sqrt{3}\\
  1&-1-\sqrt{3}&1\\
  0&2+\sqrt{3}&-1-\sqrt{3}\\
  \ebm
  \xra{3}
 \] \[
  \bbm
  0&0&0\\
  1&-1-\sqrt{3}&1\\
  0&2+\sqrt{3}&-1-\sqrt{3}\\
  \ebm
  \xra{4}
  \bbm
  0&0&0\\
  1&-1-\sqrt{3}&1\\
  0&1&1-\sqrt{3}\\
  \ebm
  \xra{5}
  \bbm
  0&0&0\\
  1&0&-1\\
  0&1&1-\sqrt{3}\\
  \ebm
  \xra{6}
  \bbm
  1&0&-1\\
  0&1&1-\sqrt{3}\\
  0&0&0\\
  \ebm
 \]
 The steps are as follows:
 \begin{itemize} 
  \item[(1)] Subtract row $2$ from row $3$.
  \item[(2)] Add $\sqrt{3}$ times row $2$ to row $1$.
  \item[(3)] Add row $3$ to row $1$.
  \item[(4)] We now want to divide row $3$ by $2+\sqrt{3}$.  Taking
   $a=2$ and $b=1$ in the equation for $1/(a+b\sqrt{3})$ we get
   $1/(2+\sqrt{3})=2-\sqrt{3}$.  We therefore multiply row $3$ by
   $2-\sqrt{3}$.
  \item[(5)] Add $1+\sqrt{3}$ times row $3$ to row $2$.
  \item[(6)] Reorder the rows.
 \end{itemize}
 We conclude that an eigenvector $u_2=\bbm x\\y\\z\ebm$ of eigenvalue
 $1+\sqrt{3}$ must satisfy $x-z=0$ and $y+(1-\sqrt{3})z=0$.  Taking
 $z=1$ we get $u_2=\bbm 1&-1+\sqrt{3}&1\ebm^T$.  Finally, following
 the hint we see that the final eigenvector $u_3$ is just
 $\bbm 1&-1-\sqrt{3}&1\ebm$ (obtained by changing the $\sqrt{3}$ in
 $u_2$ to $-\sqrt{3}$).  We now have a diagonalisation 
 $A=UDU^{-1}$, where 
 \begin{align*}
  U &=
   \left[\begin{array}{c|c|c} && \\ u_1 & u_2 & u_3 \\ && \end{array}\right]
   = \bbm  1 &  1          &  1          \\
           0 & -1+\sqrt{3} & -1-\sqrt{3} \\
          -1 &  1          &  1          \ebm \\
  D &= \diag(\lm_1,\lm_2,\lm_3) = 
       \bbm 0&0&0 \\ 0&-1+\sqrt{3}&0 \\ 0&0&-1-\sqrt{3} \ebm.
 \end{align*}
\end{solution}

\section{Lecture 10}

\begin{exercise}\label{ex-powers-i}
 Let $A$ be the $5\tm 5$ matrix in which every entry is one.
% (We will stick with the $5\tm 5$ version for definiteness, but you
% should be able to see that the $d\tm d$ version works in more or less
% the same way for all $d$.)
 \begin{itemize}
  \item[(a)] Show that $A^2=5A$.
  \item[(b)] Suppose that $\lm$ is an eigenvalue of $A$, so there
   exists a nonzero vector $u$ with $Au=\lm u$.  By considering
   $A^2u$, show that $\lm^2=5\lm$, so $\lm=0$ or $\lm=5$. (You should
   not write out any matrices here, or attempt to calculate the
   characteristic polynomial; just use part~(a).)
  \item[(c)] Find an eigenvector $v$ of eigenvalue $5$, and a linearly
   independent list $w_1,\dotsc,w_4$ of eigenvectors of eigenvalue
   $0$.
  \item[(d)] Now put $B=\half I_5 + \tfrac{1}{10}A$.  Show that $B$ is
   stochastic.
  \item[(e)] Prove by induction on $k$ that
   $B^k=2^{-k}I_5+(1-2^{-k})A/5$ for all $k\geq 0$.  (You should not
   write out any matrices here; just use part~(a).)  What happens when
   $k$ is large?
 \end{itemize}
\end{exercise}
\begin{solution}
 \begin{itemize}
  \item[(a)] One way to say this is to introduce the vector
   $v=\bbm 1&1&1&1&1\ebm^T$, so $v.v=5$.  We also have
   \[
    A = \bbm 1&1&1&1& 1 \\1&1&1&1& 1 \\1&1&1&1& 1 \\1&1&1&1& 1 \\1&1&1&1& 1 \ebm
      = \left[\begin{array}{ccccc} 
         && v^T && \\ \hline
         && v^T && \\ \hline
         && v^T && \\ \hline
         && v^T && \\ \hline
         && v^T && 
        \end{array}\right] 
      = \left[\begin{array}{c|c|c|c|c}
         &&&& \\ &&&& \\ v & v & v & v & v \\ &&&& \\ &&&&
        \end{array}\right]
   \]
   so 
   \[ A^2 = \bbm v.v & v.v & v.v & v.v & v.v \\
                 v.v & v.v & v.v & v.v & v.v \\
                 v.v & v.v & v.v & v.v & v.v \\
                 v.v & v.v & v.v & v.v & v.v \\
                 v.v & v.v & v.v & v.v & v.v \ebm 
          = \bbm 5 & 5 & 5 & 5 & 5 \\
                 5 & 5 & 5 & 5 & 5 \\
                 5 & 5 & 5 & 5 & 5 \\
                 5 & 5 & 5 & 5 & 5 \\
                 5 & 5 & 5 & 5 & 5 \ebm = 5A.
   \]
  \item[(b)] Suppose we have an eigenvalue $\lm$, and an associated
   eigenvector $u$ (so $u\neq 0$ and $Au=\lm u$).  We then have
   \[ A^2u = A(Au) = A(\lm u) = \lm Au = \lm^2 u. \]
   On the other hand, we have $A^2=5A$, so 
   \[ A^2u = 5Au = 5\lm u. \]
   By comparing these two equations, we see that $\lm^2u=5\lm u$, so
   $(\lm^2-5\lm)u=0$ or $\lm(\lm-5)u=0$.  As $u\neq 0$ it follows that
   $\lm=0$ or $\lm=5$.
  \item[(c)] Put 
   \[ v   = \bbm 1 \\  1 \\  1 \\  1 \\  1 \ebm \hspace{3em}
      w_1 = \bbm 1 \\ -1 \\  0 \\  0 \\  0 \ebm \hspace{3em}
      w_2 = \bbm 0 \\  1 \\ -1 \\  0 \\  0 \ebm \hspace{3em}
      w_3 = \bbm 0 \\  0 \\  1 \\ -1 \\  0 \ebm \hspace{3em}
      w_4 = \bbm 0 \\  0 \\  0 \\  1 \\ -1 \ebm.
   \]
   It is easy to see that $Av=5v$ and $Aw_i=0$ for all $i$, so $v$ is
   an eigenvector of eigenvalue $5$, and $w_1,\dotsc,w_4$ are
   eigenvectors of eigenvalue $0$.  It is also clear that the list 
   $w_1,\dotsc,w_4$ is linearly independent.  This is not the only
   possible answer.  For example, the list 
   \[ w'_1 = \bbm 1 \\  0 \\  0 \\  0 \\ -1 \ebm \hspace{3em}
      w'_2 = \bbm 0 \\  1 \\  0 \\  0 \\ -1 \ebm \hspace{3em}
      w'_3 = \bbm 0 \\  0 \\  1 \\  0 \\ -1 \ebm \hspace{3em}
      w'_4 = \bbm 0 \\  0 \\  0 \\  1 \\ -1 \ebm
   \]
   would do equally well.
  \item[(d)] In the metrix $B$, every entry away from the diagonal
   is $\frac{1}{10}$, and every entry on the diagonal is
   $\frac{1}{10}+\frac{1}{2}=\frac{6}{10}$.  In particular, all
   entries are positive.  Moreover, each column contains four entries
   equal to $\frac{1}{10}$ and one entry equal to $\frac{6}{10}$,
   adding up to $(4\tm 1 + 6)/10=1$.  Thus, the matrix is stochastic.
  \item[(e)] We claim that for all $k\geq 0$ we have
   $B^k=2^{-k}I_5+(1-2^{-k})A/5$.  When $k=0$ the left hand side is
   $B^0=I_5$, whereas the right hand side is $2^0I_5+(1-2^0)A=I_5$, as
   required.  When $k=1$ the left hand side is
   $B^1=B=\half I_5+\tfrac{1}{10}A$.  We also have
   $2^{-1}=1-2^{-1}=\half$ so on the right hand side we have
   $\half I_5+\frac{1}{10}A$ again, as required.

   Now suppose that the claim is true for a particular value of $k$.
   We can the equation $B=\half I_5+\tfrac{1}{10}A$ by the equation
   $B^k=2^{-k}I_5+(1-2^{-k})A/5$ and expand out to get
   \begin{align*}
    B^{k+1} &= (\half I_5+\tfrac{1}{10}A)(2^{-k}I_5+(1-2^{-k})A/5) \\
     &= \half 2^{-k}I_5 + \half\tfrac{1}{5}(1-2^{-k})A 
         + \tfrac{1}{10} 2^{-k}A
         + \tfrac{1}{10}\tfrac{1}{5}(1-2^{-k})A^2.
   \end{align*}
   Using $A^2=5A$ this becomes
   \begin{align*}
    B^{k+1} 
     &= \half 2^{-k}I_5 + \half\tfrac{1}{5}(1-2^{-k})A 
         + \tfrac{1}{10} 2^{-k}A
         + \tfrac{1}{10}(1-2^{-k})A \\
     &= 2^{-k-1}I_5 +
         \left(\half(1-2^{-k})+\half 2^{-k}+\half(1-2^{-k})\right)A/5 \\
     &= 2^{-k-1}I_5 +
         \left(\half-2^{-k-1}+2^{-k-1}+\half-2^{-k-1})\right)A/5 \\
     &= 2^{-k-1}I_5 + (1-2^{-k-1})A/5.
   \end{align*}
   This is the case $k+1$ of our claim.  It follows by induction that
   the claim holds for all $k$.
 \end{itemize}
\end{solution}

\begin{exercise}\label{ex-diagonal-vi}
 Show that the matrix $A=\bbm 1&1&0\\ -1&2&1\\ -1&0&3\ebm$ cannot be
 diagonalised.\\
 \textbf{Hint:} the eigenvalues are small integers.
\end{exercise}
\begin{solution}
 The characteristic polynomial is
 \begin{align*}
  \chi_A(t) &= 
   \det\bbm 1-t & 1   & 0 \\
           -1   & 2-t & 1 \\
           -1   & 0   & 3-t \ebm 
   = (1-t) \det\bbm 2-t&1 \\ 0&3-t \ebm - \det\bbm -1&1 \\ -1&3-t \ebm \\
   &= (1-t)(2-t)(3-t) - (t-2) 
    = (2-t)((1-t)(3-t)+1) \\
   &= (2-t)(4-4t+t^2) = (2-t)^3.
 \end{align*}
 (If we had not spotted that $2-t$ was a common factor and had just
 expanded everything out, we would have found that
 $\chi_A(t)=-t^3+6t^2-12t+8$.  Using the hint we could have tried
 various small integers and found that $\chi_A(2)=0$, then we could
 have divided $\chi_A(t)$ by $t-2$ to get $-t^2+4t-4$, then we could
 have used the quadratic formula to see that $2$ is the only root.)

 We now see that $2$ is the only eigenvalue of $A$.  To find the
 eigenvectors, we row-reduce $A-2I$:
 \[ \bbm -1&1&0 \\ -1&0&1 \\ -1&0&1 \ebm 
    \to 
    \bbm 1&-1&0 \\ 0&-1&1 \\ 0&-1&1 \ebm
    \to
    \bbm 1&-1&0 \\ 0&1&-1 \\ 0&0&0 \ebm
    \to
    \bbm 1&0&-1 \\ 0&1&-1 \\ 0&0&0 \ebm
 \]
 From this we see that the eigenvectors of eigenvalue $2$ are just the
 nonzero vectors of the form $u=\bbm x&x&x\ebm^T$.  In particular, any
 two eigenvectors are multiples of each other, and so are linearly
 dependent.  It follows that there is no basis of eigenvectors, so the
 matrix cannot be diagonalised.
\end{solution}

\begin{exercise}\label{ex-powers-ii}
 Consider the matrix
 \[ A = \frac{1}{16} \bbm 10 & 2 & 2 \\ 3 & 11 & 7 \\ 3 & 3 & 7 \ebm.
 \]
 For this matrix it turns out that the powers $A^n$ converge to a
 limit as $n\to\infty$.  Use Maple to find a diagonalisation
 $A=UDU^{-1}$, then find the limit of $D^n$ as $n\to\infty$, then find
 the limit of $A^n$.
\end{exercise}
\begin{solution}
 We enter the definition of $A$ and find the eigenvectors as follows:
 \begin{verbatim}
  with(LinearAlgebra):

  A := <<10|2|2>,<3|11|7>,<3|3|7>>/16;

  L,U := Eigenvectors(A);
 \end{verbatim}
 Maple responds by printing
 \[ L,U := \bbm 1\\ 1/4\\ 1/2\ebm,
           \bbm 1 & 0 & -1 \\ 2 & -1 & 1 \\ 1 & 1 & 0 \ebm
 \]
 This indicates that the eigenvalues are $\lm_1=1$,
 $\lm_2=\tfrac{1}{4}$ and $\lm_3=\tfrac{1}{2}$, and the corresponding
 eigenvectors are the columns of the above matrix, namely
 \[ u_1 = \bbm 1 \\ 2 \\ 1 \ebm\qquad
    u_2 = \bbm 0 \\ -1 \\ 1 \ebm\qquad
    u_3 = \bbm -1 \\ 1 \\ 0 \ebm.
 \]
 We therefore have a diagonalisation $A=UDU^{-1}$, where 
 \[ U = \bbm 1 & 0 & -1 \\ 2 & -1 & 1 \\ 1 & 1 & 0 \ebm
    \hspace{4em}
    D = \bbm 1 & 0 & 0 \\ 0 & 1/4 & 0 \\ 0 & 0 & 1/2 \ebm.
 \]
 We can calculate the inverse of $U$ by entering \verb+U^(-1)+ in
 Maple; we find that 
 \[ U^{-1} = \frac{1}{4} \bbm 1&1&1 \\ -1&-1&3 \\ -3&1&1 \ebm \]
 This gives
 \[ \lim_{n\to\infty} D^n = 
    \lim_{n\to\infty} 
     \bbm 1 & 0 & 0 \\ 0 & 1/4^n & 0 \\ 0 & 0 & 1/2^n \ebm = 
     \bbm 1 & 0 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & 0 \ebm.
 \]
 We will call this matrix $D^\infty$. As $A^n=UD^nU^{-1}$ we see that 
 \begin{align*}
  \lim_{n\to\infty} A^n &= UD^\infty U^{-1} = 
     \frac{1}{4}
     \bbm 1 & 0 & -1 \\ 2 & -1 & 1 \\ 1 & 1 & 0 \ebm
     \bbm 1 & 0 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & 0 \ebm 
     \bbm 1&1&1 \\ -1&-1&3 \\ -3&1&1 \ebm \\
   &=
   \bbm 0.25 & 0 & -0.25 \\ 0.5 & -0.25 & 0.25 \\ 0.25 & 0.25 & 0 \ebm
   \bbm 1&1&1 \\ 0&0&0 \\ 0&0&0 \ebm = 
   \bbm 0.25 & 0.25 & 0.25 \\ 0.5 & 0.5 & 0.5 \\ 0.25 & 0.25 & 0.25 \ebm.
 \end{align*}
 As a check, we can enter \verb+evalf(A^10)+ in Maple to calculate a
 numerical approximation to $A^{10}$, which is
 \[ \bbm 0.2507324219 & 0.2497558594 & 0.2497558594 \\
         0.4992678165 & 0.5002443790 & 0.5002434254 \\
         0.2499997616 & 0.2499997616 & 0.2500007153 \ebm.
 \]
 This is already quite close to the limiting value.
\end{solution}

\begin{exercise}\label{ex-powers-iii}
 Consider the matrix 
 \[ A = \bbm 1&1&1 \\ 0&1&1 \\ 0&0&1 \ebm. \]
 You may assume that this matrix cannot be diagonalised.  Nonetheless,
 the powers $A^n$ follow a simple pattern.  Calculate $A^n$ for some
 small values of $n$, then see if you can find the general rule, then
 prove it by induction.
\end{exercise}
\begin{solution}
 The first few powers are as follows:
 \begin{align*}
  A^0 &= \bbm 1&0& 0 \\ 0&1&0 \\ 0&0&1 \ebm &
  A^1 &= \bbm 1&1& 1 \\ 0&1&1 \\ 0&0&1 \ebm \\
  A^2 &= \bbm 1&2& 3 \\ 0&1&2 \\ 0&0&1 \ebm &
  A^3 &= \bbm 1&3& 6 \\ 0&1&3 \\ 0&0&1 \ebm \\
  A^4 &= \bbm 1&4&10 \\ 0&1&4 \\ 0&0&1 \ebm &
  A^5 &= \bbm 1&5&15 \\ 0&1&5 \\ 0&0&1 \ebm.
 \end{align*}
 From this it is at least clear that 
 \[ A^n = \bbm 1 & n & p_n \\ 0 & 1 & n \\ 0 & 0 & 1 \ebm \]
 for some number $p_n$.  The remaining problem is to find a formula
 for $p_n$.  The first few cases are 
 \[ p_0 = 0  \qquad
    p_1 = 1  \qquad
    p_2 = 3  \qquad
    p_3 = 6  \qquad
    p_4 = 10 \qquad
    p_5 = 15. 
 \]
 You might recognise these numbers as coming from Pascal's triangle,
 or you might notice that $p_n-p_{n-1}=n$ and work from there, or you
 might notice that $p_n$ is approximately $n^2/2$ and so study
 $p_n-n^2/2$, or you might enter the above numbers in the Online
 Encyclopedia of Integer Sequences at \verb+http://oeis.org+ and see
 what it finds.  By any of these means you can arrive at the formula 
 \[ p_n = \left(\begin{array}{c} n+1 \\ 2 \end{array}\right) 
        = (n^2+n)/2.
 \] 
 We thus conclude that
 \[ A^n = \bbm 1 & n & (n^2+n)/2 \\ 0 & 1 & n \\ 0 & 0 & 1 \ebm. \]
 We can prove this formally by induction.  The claim is clearly true
 for $n=0$.  If it holds for a particular value of $n$, then we have 
 \begin{align*}
  A^{n+1}
   &= AA^n 
    = \bbm 1&1&1 \\ 0&1&1 \\ 0&0&1 \ebm
      \bbm 1 & n & (n^2+n)/2 \\ 0 & 1 & n \\ 0 & 0 & 1 \ebm \\
   &= \bbm 1 & n+1 & (n^2+n)/2 + n + 1 \\ 0 & 1 & n+1 \\ 0 & 0 & 1 \ebm.
 \end{align*}
 Here 
 \[ (n^2+n)/2 + n + 1 = n^2/2 + 3n/2 + 1 = ((n+1)^2+(n+1))/2, \]
 so we see that the claim also holds for $n+1$.  Thus, by induction,
 it holds for all natural numbers $n$.
\end{solution}

\section{Lecture 11}

\begin{exercise}\label{ex-ode-i}
 Solve the following system of differential equations using the method
 in Section~\ref{sec-ode}:
 \begin{align*}
  \dot{x}_1 &= 0.2 x_1 + 0.5 x_2 + 0.3 x_3 \\
  \dot{x}_2 &= 0.6 x_1 + 0.6 x_2 + 0.7 x_3 \\
  \dot{x}_3 &= 0.1 x_1 + 0.4 x_2 + 0.8 x_3,
 \end{align*}
 with $x=\bbm 1 & 0 & 0 \ebm^T$ at $t=0$.  You should use Maple to
 calculate the relevant eigenvalues and eigenvectors.  Unlike most
 examples in this course, this one has not been fine-tuned to work out
 with nice round numbers.
\end{exercise}
\begin{solution}
 We have $\dot{x}=Ax$ and $x=c$ at $t=0$, where 
 \[ A = \bbm 0.2 & 0.5 & 0.3 \\
             0.6 & 0.6 & 0.7 \\
             0.1 & 0.4 & 0.8 \ebm 
    \hspace{5em}
    c = \bbm 1 \\ 0 \\ 0 \ebm.
 \]
 The general method is to diagonalise $A$ as $UDU^{-1}$ with
 $D=\diag(\lm_1,\lm_2,\lm_3)$ say, then $x=UEU^{-1}c$, where
 $E=\diag(e^{\lm_1 t},e^{\lm_2 t},e^{\lm_3 t})$.  We can do this in
 Maple as follows:
 \begin{verbatim}
  with(LinearAlgebra):
  unprotect('D'):

  A := <<0.2|0.5|0.3>,<0.6|0.6|0.7>,<0.1|0.4|0.8>>;
  L,U := Eigenvectors(A);
  D := DiagonalMatrix(L);
  E := DiagonalMatrix([exp(L[1]*t),exp(L[2]*t),exp(L[3]*t)]);
  c := <1,0,0>;
  x := U . E . U^(-1);
 \end{verbatim}
 Maple responds with 
 \[ x := 
     \bbm 
      0.1471732926\,{{\rm e}^{ 1.442698079\,t}}+
      0.5641142246\,{{\rm e}^{-0.2096633632\,t}}+ 
      0.2887124828\,{{\rm e}^{ 0.3669652806\,t}} \\ 
      0.2563257383\,{{\rm e}^{ 1.442698079\,t}}
    - 0.5623411149\,{{\rm e}^{-0.2096633632\,t}} +
      0.3060153766\,{{\rm e}^{ 0.3669652806\,t}}\\
      0.1824303322\,{{\rm e}^{ 1.442698079\,t}}+
      0.1669120914\,{{\rm e}^{-0.2096633632\,t}}
    - 0.3493424236\,{{\rm e}^{ 0.3669652806\,t}}
     \ebm
 \]
 which is the solution for $x$.  Some comments on these commands:
 \begin{itemize}
  \item Maple usually uses the symbol $D$ for differentiataion, so if
   we want to use $D$ as the name of a matrix, we need to enter
   \verb+unprotect('D')+ first.  The quotation marks are important
   here.
  \item The line \verb+L,U := Eigenvectors(A)+ sets $L$ to be the
   vector $\bbm \lm_1 & \lm_2 & \lm_3\ebm^T$, whose entries are the
   eigenvalues.  It also sets $U$ to be the usual matrix whose columns
   are the corresponding eigenvectors.
 \end{itemize}
\end{solution}

\begin{exercise}\label{ex-diffeq-i}
 Consider the matrix $A=\bbm 1 & 1 \\ 2 & 1 \ebm$.
 \begin{itemize}
  \item[(a)] Find the eigenvalues of $A$.
  \item[(b)] For each eigenvalue, find a corresponding eigenvector of
   $A$. 
  \item[(c)] Define recursively a sequence of vectors $\bbm
   u_n\\v_n\ebm$ as follows: we have $u_0=1$ and $v_0=0$, and for all
   $n>0$ we have
   \begin{align*}
    u_n &= u_{n-1} + v_{n-1} \\
    v_n &= 2u_{n-1} + v_{n-1}.
   \end{align*}
   Use your eigenvectors of $A$ to find expressions for $u_n$ and
   $v_n$ (for a general positive integer $n$).
 \end{itemize}
\end{exercise}
\begin{solution}
 \def\Fiba{\textstyle 1+\sqrt{2}}
 \def\Fibb{\textstyle 1-\sqrt{2}}

 We have
 $$
 \chi_A(t) = \left|  \begin{array}{cc}1-t & 1\\2& 1-t
 \end{array} \right| = (t-1)^2 -2 = t^2 - 2t - 1 =
 \left[t - 1-\sqrt{2}\right]\left[t -
 1+\sqrt{2}\right].
 $$

 We thus see that the eigenvalues of $A$ are $\lambda_1 = \Fiba$ and
 $\lambda_2 = \Fibb$. There are two distinct eigenvalues, and so it
 is possible to find a basis for $\R^2$ consisting of eigenvectors of
 $A$. Notice that $(\Fiba)(\Fibb) = -1$ and
 $(\Fiba) + (\Fibb) = 2$.

 To find an eigenvector corresponding to $\lambda_1$, we consider
 $(A - \lambda_1I_2)\left[
 \begin{array}{cc}x & y
 \end{array} \right]^T = 0$:
 \begin{align*}
 \big(A - \lambda_1I_2|0\big) &= \left[ \begin{array}{cc|c} 1 -
 (\Fiba) & 1& 0\\ 2 &1 -(\Fiba) & 0\end{array} \right] = \left[
 \begin{array}{cc|c} -\sqrt{2} & 1 & 0\\ 2 & -\sqrt{2} & 0
 \end{array} \right]\\ & \to \left[ \begin{array}{cc|c}
 -\sqrt{2} & 1 & 0\\0 & 0 & 0
 \end{array} \right],
 \end{align*}
 so that $w_1 := \left[ \begin{array}{c}1\\ \sqrt{2}
 \end{array} \right]$ is an eigenvector of $A$ corresponding to
 $\lambda_1$.

 To find an eigenvector corresponding to $\lambda_2$, we consider
 $(A - \lambda_2I_2)\left[
 \begin{array}{cc}x & y
 \end{array} \right]^T = 0$:
 \begin{align*}
 \big(A - \lambda_2I_2|0\big) &= \left[ \begin{array}{cc|c} 1 -
 (\Fibb) & 1& 0\\ 2 & 1-(\Fibb) & 0\end{array} \right] = \left[
 \begin{array}{cc|c} \sqrt{2} & 1 & 0\\ 2 & \sqrt{2} & 0
 \end{array} \right]\\ & \to \left[ \begin{array}{cc|c}
  \sqrt{2} & 1 & 0\\0 & 0 & 0
 \end{array} \right],
 \end{align*}
 so that $w_2 := \left[ \begin{array}{c}1\\ -\sqrt{2}
 \end{array} \right]$ is an eigenvector of $A$ corresponding to
 $\lambda_2$.

 We have
 $$
 \left[ \begin{array}{c}u_{0}\\ v_0 \end{array} \right] =
 \left[ \begin{array}{c}1\\ 0 \end{array} \right]
 \quad \mbox{~and~} \quad
 \left[ \begin{array}{c}u_{n}\\ v_{n} \end{array} \right] =
 \left[ \begin{array}{cc} 1 & 1 \\ 2 & 1 \end{array} \right]
 \left[ \begin{array}{c}u_{n-1}\\ v_{n-1} \end{array} \right] =
 A\left[ \begin{array}{c}u_{n-1}\\ v_{n-1} \end{array} \right]
 \quad\textrm{\ for\ } n>0.
 $$
 Therefore
 $$
 \left[ \begin{array}{c}u_{n}\\ v_{n} \end{array} \right] =
 A^{n} \left[\begin{array}{c}1\\ 0 \end{array} \right]
 \quad\textrm{\ for\ } n>0.
 $$
 We calculate this by using the above eigenvectors $w_1$ and $w_2$ of
 $A$. Since $w_1$ and $w_2$ are eigenvectors of $A$ corresponding to
 different eigenvalues, they are linearly independent, and so form a
 basis for $\R^2$.
 We express
 $\left[ \begin{array}{cc}1&0 \end{array} \right]^T$
 as a linear combination of $w_1$ and $w_2$:
 $$
 \left[ \begin{array}{c}1\\ 0 \end{array} \right] =
 {\textstyle \half \left[ \begin{array}{c}1\\ \sqrt{2} \end{array} \right] +
 \half \left[ \begin{array}{c}1\\ -\sqrt{2} \end{array} \right] =
 \half w_1 + \half w_2}.
 $$
 Therefore, for all $n > 0$,
 \begin{align*}
 \left[ \begin{array}{c}u_{n}\\ v_{n} \end{array} \right] &=
   A^{n} \left[ \begin{array}{c}1\\ 0 \end{array} \right] =
   A^{n}\left({\textstyle \half w_1 + \half w_2}\right) \\
  & = {\textstyle \half A^nw_1 + \half A^nw_2 } =
      {\textstyle \half\lambda_1^nw_1 + \half\lambda_2^nw_2 }\\
  & = {\textstyle \half}\left(\Fiba\right)^{n}
       \left[ \begin{array}{c}1 \\ \sqrt{2} \end{array} \right] +
      {\textstyle \half}\left(\Fibb\right)^{n}
       \left[ \begin{array}{c}1 \\ -\sqrt{2} \end{array} \right]
  \\& =
       \left[ \begin{array}{c} {\textstyle \half}\left(\left(\Fiba\right)^{n} + \left(\Fibb\right)^{n}\right)  \\
       {\textstyle \frac{\sqrt{2}}{2}}\left(\left(\Fiba\right)^{n} - \left(\Fibb\right)^{n}\right)  \\
       \end{array} \right].
 \end{align*}
 Thus
 $$ u_n={\textstyle \half}\left( (1+\sqrt{2})^n + (1-\sqrt{2})^n\right)
    \quad \text{~and~} \quad
    v_n={\textstyle \frac{1}{\sqrt{2}}}\left((1+\sqrt{2})^n-(1-\sqrt{2})^n\right)
 $$
 for all $n > 0$.
\end{solution}

\begin{exercise}\label{ex-diffeq-ii}
 The sequence $(a_n)_{n=0}^\infty$ is given by 
 $a_0=1001001$, $a_1=1010100$, $a_2=1110000$ and 
 \[ a_{n+3} = 111 a_{n+2} - 1110 a_{n+1} + 1000 a_n
   \hspace{4em} (\text{ for } n>2)
 \] 
 \begin{itemize}
  \item[(a)] Write down a matrix equation relating the vector $u_n$ to
   $u_{n+1}$, where $u_n=\bbm a_{n+2}\\ a_{n+1}\\ a_n\ebm$.
  \item[(b)] Find the eigenvalues and eigenvectors of the matrix
   occuring in~(a).  (If you have done this correctly, the answers
   will be integers with a nice pattern.)
  \item[(c)] Express $u_0$ as a linear combination of the eigenvectors
   in~(b).
  \item[(d)] Give a general formula for $a_n$.
  \item[(e)] Check directly that your formula satisfies
   $a_{n+3}=111a_{n+2}-1110 a_{n+1}+1000a_n$ and that $a_0$, $a_1$ and
   $a_2$ are as they should be.
 \end{itemize}
\end{exercise}
\begin{solution}
 \begin{itemize}
  \item[(a)] We have
   \[ u_{n+1} = 
      \bbm a_{n+3} \\ a_{n+2} \\ a_{n+1} \ebm = 
      \bbm 111 a_{n+2} - 1110 a_{n+1} + 1000 a_n \\ a_{n+2} \\ a_{n+1} \ebm = 
      \bbm 111 & -1110 & 1000 \\ 1 & 0 & 0 \\ 0 & 1 & 0 \ebm 
       \bbm a_{n+2} \\ a_{n+1} \\ a_n \ebm.
   \]
   In other words, if we put 
   \[ A = \bbm 111 & -1110 & 1000 \\ 1 & 0 & 0 \\ 0 & 1 & 0 \ebm  \]
   then $u_{n+1}=Au_n$.  It follows that for all $n\geq 0$ we have
   \[ u_n = A^nu_0 = A^n\bbm 1110000 \\ 1010100 \\ 1001001\ebm. \]
  \item[(b)] The characteristic polynomial is 
   \begin{align*}
    \chi_A(t)
     &= \det\bbm 111-t & -1110 & 1000 \\ 1 & -t & 0 \\ 0 & 1 & -t \ebm
      = (111-t) \det\bbm -t & 0 \\ 1 & -t \ebm 
        +1110 \det\bbm 1 & 0 \\ 0 & -t \ebm 
        +1000 \det\bbm 1 & -t \\ 0 & 1\ebm \\
     &= (111-t)t^2 -1110t + 1000 
      = 1000 - 1110t + 111t^2 - t^3 \\
     &= (1-t)(10-t)(100-t).
   \end{align*}
   Thus, the eigenvalues are $1$, $10$ and $100$.  To find the
   corresponding eigenvectors, we perform the following
   row-reductions:
   \begin{align*}
    A-I    &= \bbm 110 & -1110 & 1000 \\ 1 & -1 & 0 \\ 0 & 1 & -1 \ebm 
            \to \bbm 1 & 0 & -1 \\ 0 & 1 & -1 \\ 0 & 0 & 0 \ebm =: B_1 \\
    A-10I  &= \bbm 101 & -1110 & 1000 \\ 1 & -10 & 0 \\ 0 & 1 & -10 \ebm 
            \to \bbm 1 & 0 & -100 \\ 0 & 1 & -10 \\ 0 & 0 & 0 \ebm =: B_2\\
    A-100I &= \bbm 10 & -1110 & 1000 \\ 1 & -100 & 0 \\ 0 & 1 & -100 \ebm 
            \to \bbm 1 & 0 & -10000 \\ 0 & 1 & -100 \\ 0 & 0 & 0 \ebm =: B_3.
   \end{align*}
   To find an eigenvector $w_2=\bbm x & y & z\ebm^T$ of eigenvalue
   $10$, we need to solve $(A-10I)w_2=0$, or equivalently $B_2w_2=0$,
   which just reduces to $x=100z$ and $y=10z$ with $z$ arbitrary.
   Taking $z=1$, we see that $\bbm 100 & 10 & 1\ebm^T$ is an
   eigenvector of eigenvalue $10$.  Treating the other two eigenvalues
   in the same way, we find that the vectors
   \[ w_1 = \bbm 1 \\ 1 \\ 1 \ebm \hspace{5em}
      w_2 = \bbm 100 \\ 10 \\ 1 \ebm \hspace{5em}
      w_3 = \bbm 10000 \\ 100 \\ 1 \ebm
   \]
   are eigenvectors of eigenvalues $1$, $10$ and $100$ respectively.
  \item[(c)] By inspection we have
   \begin{align*}
    u_0 &= \bbm 1110000 \\ 1010100 \\ 1001001\ebm 
       = \bbm 1000000 \\ 1000000 \\ 1000000\ebm +
         \bbm  100000 \\   10000 \\    1000\ebm +
         \bbm   10000 \\     100 \\       1\ebm  \\
      &= 1000000 \bbm 1 \\ 1 \\ 1\ebm +
         1000 \bbm 100 \\ 10 \\ 1\ebm +
         \bbm 10000 \\ 100 \\ 1\ebm 
       = 10^6 w_1 + 10^3 w_2 + w_3.
   \end{align*}
  \item[(d)] Recall that $Aw_1=w_1$ and $Aw_2=10w_2$ and
   $Aw_3=100w_3$.  It follows that for all $n\geq 0$ we have
   $A^nw_1=w_1$ and $A^nw_2=10^nw_2$ and
   $A^nw_3=100^nw_3=10^{2n}w_3$.  This gives
   \begin{align*}
    u_n &= A^nu_0 
         = A^n(10^6 w_1 + 10^3 w_2 + w_3) 
         = 10^6 A^nw_1 + 10^3 A^nw_2 + A^nw_3 \\
        &= 10^6w_1 + 10^{n+3}w_2 + 10^{2n}w_3 
         = 10^6     \bbm 1    \\ 1    \\ 1 \ebm + 
           10^{n+3} \bbm 10^2 \\ 10   \\ 1 \ebm + 
           10^{2n}  \bbm 10^4 \\ 10^2 \\ 1 \ebm \\
        &= \bbm 10^6 + 10^{n+5} + 10^{2n+4} \\
                10^6 + 10^{n+4} + 10^{2n+2} \\
                10^6 + 10^{n+3} + 10^{2n} \ebm.
   \end{align*}
   In particular, $a_n$ is the bottom entry in $u_n$, which is
   \[ a_n = 10^6 + 10^{n+3} + 10^{2n}. \]
  \item[(e)] Our formula gives
   \begin{align*}
    a_0 &= 10^6 + 10^{3} + 10^{0} = 1001001 \\
    a_1 &= 10^6 + 10^{4} + 10^{2} = 1010100 \\
    a_0 &= 10^6 + 10^{5} + 10^{4} = 1110000 
   \end{align*}
   as it should.  We also have
   \begin{align*}
      & 111a_{n+2}-1110 a_{n+1}+1000a_n \\
     =& 111(10^6 + 10^{n+5} + 10^{2n+4}) - 
        1110(10^6 + 10^{n+4} + 10^{2n+2}) +
        1000(10^6 + 10^{n+3} + 10^{2n}) \\
     =& 10^6(111-1110+1000) + 
        10^{n+3}(11100-11100+1000) +
        10^{2n}(1110000-111000+1000) \\
     =& 10^6 + 1000\tm 10^{n+3} + 1000000\tm 10^{2n} 
     =  10^6 + 10^{n+6} + 10^{2n+6} = a_{n+3}.
   \end{align*}
 \end{itemize}
\end{solution}

\begin{exercise}\label{ex-diffeq-iii}
 Let $(a_n)$ be the sequence given by $a_0=2$ and $a_1=4$ and
 $a_{n+2}=4a_{n+1}-a_n$ for $n\geq 0$.  Give a general formula for
 $a_n$. 
\end{exercise}
\begin{solution}
 The vectors $v_n=\bbm a_n\\a_{n+1}\ebm$ satisfy $v_0=\bbm 2\\4\ebm$
 and 
 \[ v_{n+1}
     = \bbm a_{n+1}\\a_{n+2}\ebm
     = \bbm a_{n+1}\\4a_{n+1}-a_n\ebm 
     = \bbm 0&1 \\ -1&4\ebm \bbm a_n\\ a_{n+1}\ebm 
     = A v_n,
 \]
 where $A=\bbm 0&1\\ -1&4\ebm$.  It follows that $v_k=A^kv_0$ for all
 $k\geq 0$.  To understand this more explicitly, we need to find the
 eigenvalues and eigenvectors of $A$.  The characteristic polynomial
 is 
 \[ \chi_A(t) = \det\bbm -t&1\\-1&4-t \ebm =
     -t(4-t)-(-1) = t^2-4t+1.
 \]
 The eigenvalues of $A$ are the roots of $\chi_A(t)$, which are
 $\lm_1=(4+\sqrt{16-4})/2=2+\sqrt{3}$ and $\lm_2=2-\sqrt{3}$.  We next
 want to find an eigenvector $u_1=\bbm x\\ y\ebm$ with
 $Au_1=\lm_1u_1$, or in other words
 \[ \bbm 0 & 1 \\ -1 & 4 \ebm \bbm x \\ y \ebm =
     \bbm \lm_1 x\\ \lm_1 y\ebm
 \]
 or $y=\lm_1x$ and $4y-x=\lm_1y$.  If we substitute $y=\lm_1x$ then
 the equation $4y-x=\lm_1y$ becomes $4\lm_1x-x=\lm_1^2x$ or
 $(\lm_1^2-4\lm_1+1)x=0$, which holds automatically because $\lm_1$ is
 a root of $t^2-4t+1=0$.  It follows that we can take
 $u_1=\bbm 1\\\lm_1\ebm$.  Similarly, the vector
 $u_2=\bbm 1\\\lm_2\ebm$ is an eigenvector of $A$ with eigenvalue
 $\lm_2$.  

 We next need to express the vector $v_0=\bbm 2\\4\ebm$ as a linear
 combination of $u_1$ and $u_2$.  Equivalently, we must find $\al_1$
 and $\al_2$ such that 
 \[ \bbm 2\\ 4\ebm = \al_1 \bbm 1\\\lm_1\ebm + \al_2\bbm 1\\\lm_2\ebm.
 \]
 Looking on the top line gives $\al_2=2-\al_1$, and then the second
 line gives $4=\lm_1\al_1+\lm_2(2-\al_1)$ and so
 $\al_1(\lm_1-\lm_2)=4-2\lm_2$.  Here $\lm_1-\lm_2=2\sqrt{3}$ and
 $4-2\lm_2=2\sqrt{3}$ as well so $\al_1=1$.  It follows that
 $\al_2=2-\al_1=1$, so $v_0=u_1+u_2$.  (This can also be seen by
 inspection.) 

 We now have $A^nu_i=\lm_i^nu_i$, so
 \begin{align*}
  v_n &= A^nv_0 = A^n(u_1+u_2) = A^nu_1+A^nu_2 = \lm_1^nu_1+\lm_2^nu_2 \\
      &= \bbm \lm_1^n + \lm_2^n \\ \lm_1^{n+1} + \lm_2^{n+1} \ebm.      
 \end{align*}
 On the other hand, we have $v_n=\bbm a_n\\ a_{n+1}\ebm$, so we
 conclude that $a_n=\lm_1^n+\lm_2^n=(2+\sqrt{3})^n+(2-\sqrt{3})^n$.
\end{solution}

\section{Lecture 12}

\begin{exercise}\label{ex-markov-i}
 Over a period of 5 minutes, in a typical MAS201 lecture, 90\% of
 students who are awake at the beginning of the 5-minute period will
 still be so at the end of it (but the other 10\% will fall asleep)
 and 90\% of students who are asleep at the beginning of the 5-minute
 period will still be so at the end of it (and the other 10\% will
 wake up). If all the students are awake at the beginning of the
 lecture, what percentage will be awake at the end of the lecture, 50
 minutes later?
\end{exercise}
\begin{solution}
 For each $k = 0, \ldots, 10$, let $a_k$ and $b_k$ be the
 proportions of students who are awake and who are asleep after
 $5k$ minutes of the lecture, respectively, and set $v_k
 =(\begin{array}{cc} a_k & b_k \end{array})^T$. We have
     $$
     \left[\begin{array}{cc} a_0\\ b_0 \end{array}\right]
     =\left[\begin{array}{cc} 1\\ 0 \end{array}\right]
    \quad \text{~and~} \quad \left[\begin{array}{cc} a_{k+1}\\ b_{k+1}
    \end{array}\right]=
     \left[\begin{array}{cc} \frac{9}{10} & \frac{1}{10}\\[3pt]
     \frac{1}{10} & \frac{9}{10}
     \end{array}\right] \left[\begin{array}{cc} a_k\\ b_k \end{array}\right] $$
 for $k = 0, \ldots, 9$. Set
 $$ A =
       \left[\begin{array}{cc} \frac{9}{10} & \frac{1}{10}\\[3pt]
     \frac{1}{10} & \frac{9}{10}
     \end{array}\right].$$
 We are thus considering the difference equation $v_{k+1} = Av_k$, so
 that $v_k =
 A^kv_0$ for $k = 0, \ldots, 10$, and we wish to find $v_{10} = A^{10}v_0$.

 The matrix $A$ is stochastic, and so has $1$ as an eigenvalue. The characteristic polynomial of $A$ is
 $$
 \chi_A(t) = \left| \begin{array}{cc} \frac{9}{10} - t & \frac{1}{10}\\[3pt] \frac{1}{10}
 & \frac{9}{10} - t \end{array} \right| =
 {\textstyle \left(t - \frac{9}{10}\right)^2 -
 \left(\frac{1}{10}\right)^2 = (t - 1)\left( t - \frac{8}{10}\right)}.
 $$
 Thus the eigenvalues of $A$ are $1$ and $\frac{8}{10}$. Since $A$ has $2$ distinct
 eigenvalues, we can find a basis for $\R^2$ consisting of
 eigenvectors of $A$.

 To find an eigenvector of $A$ corresponding to the eigenvalue $\lambda_1 :=1$,
 we need to solve the system of linear equations
 $(A - I_2)(\begin{array}{cc} x & y \end{array})^T = 0$. This has augmented matrix
 $$
 \big(A - I_2|0\big) = \left[\begin{array}{cc|c}
 - \frac{1}{10} & \frac{1}{10} & 0\\[3pt]
 \frac{1}{10}
 &- \frac{1}{10} & 0\end{array} \right] \to
  \left[\begin{array}{cc|c}- \frac{1}{10} & \frac{1}{10} & 0\\[2pt]
 0
 & 0 & 0\end{array} \right],
 $$
 and so $w_1 :=\bbm 1 & 1\ebm^T$ is an eigenvector of $A$ corresponding to
 the eigenvalue $1$.

 To find an eigenvector of $A$ corresponding to the eigenvalue
 $\lambda_2 := \frac{8}{10}$, we need to solve the system of linear
 equations
 $\left[A -{\textstyle \frac{8}{10}} I_2\right]\bbm x & y \ebm^T = 0$. This has
 augmented matrix
 $$
 \big(A -{\textstyle \frac{8}{10}} I_2|0\big) =
 \left[\begin{array}{cc|c} \frac{1}{10} & \frac{1}{10} & 0\\[3pt]
 \frac{1}{10}
 & \frac{1}{10} & 0\end{array} \right] \to
  \left[\begin{array}{cc|c} \frac{1}{10} & \frac{1}{10} & 0\\[2pt]
 0
 & 0 & 0\end{array} \right],
 $$
 and so $w_2 :=\bbm 1 & -1\ebm^T$ is an eigenvector of $A$ corresponding to
 the eigenvalue $\frac{8}{10}$.

 Now, $w_1 = \left[\begin{array}{c} 1 \\ 1 \end{array}\right]$
 and $w_2 = \left[\begin{array}{c} 1 \\ -1 \end{array}\right]$, being
 eigenvectors corresponding to distinct eigenvalues of $A$, form a
 basis for $\R^2$. We express $v_0$ as a linear combination of these
 two eigenvectors:
 $$
 v_0 =\left[\begin{array}{cc} 1\\ 0 \end{array}\right] = {\textstyle
 \half}\left[\begin{array}{c} 1 \\ 1 \end{array}\right] +
 {\textstyle \half}\left[\begin{array}{c} 1 \\ -1
 \end{array}\right] = {\textstyle \half w_1 + \half w_2}.
 $$

 We have
 $${\textstyle v_k = A^kv_0 =
    A^k\left(\half w_1 + \half w_2\right) =
    \half A^kw_1 + \half A^kw_2 =
    \half\lambda_1^kw_1 + \half\lambda_2^kw_2 =
    \half 1^kw_1 + \half(0.8)^kw_2}.
 $$ In particular,
 $$
  v_{10} = \left[\begin{array}{cc} a_{10}\\ b_{10} \end{array}\right] =
 {\textstyle \half w_1 + \half(0.8)^{10}w_2} =
 {\textstyle \half}
    \left[\begin{array}{c} 1 \\ 1 \end{array}\right] +
 {\textstyle \half}(0.8)^{10}
    \left[\begin{array}{c} 1 \\ -1 \end{array}\right].
 $$
 Since $(0.8)^{10} \approx 0.107374$, we conclude that
 approximately $55.37\%$ of students are awake at the end of the
 lecture.
\end{solution}

\begin{exercise}\label{ex-stochastic-i}
 Put $d=\bbm 1 &\cdots&1\ebm^T\in\R^n$.
 \begin{itemize}
  \item[(a)] If $P\in M_n(\R)$ is a stochastic matrix, show that $d^TP=d^T$.
  \item[(b)] Deduce that if $q\in\R^n$ is a probability vector, then
   $Pq$ is also a probability vector.
  \item[(c)] Deduce that if $Q\in M_n(\R)$ is another stochastic
   matrix, then $PQ$ is also a stochastic matrix.\\
   (\textbf{Hint:} how are the columns of $PQ$ related to the columns
   of $Q$?)
 \end{itemize}
\end{exercise}
\begin{solution}
 \begin{itemize}
  \item[(a)] Let the columns of $P$ be $v_1,\dotsc,v_n$.  As $P$ is
   stochastic, we know that the sum of the entries in $v_i$  is equal
   to one, so $d.v_i=1$.  This means that
   \[ d^TP =
      \bbm 1 &\cdots& 1\ebm 
       \left[\begin{array}{c|c|c} 
       v_1 & \cdots & v_n 
      \end{array}\right] = 
      \bbm d.v_1 & \cdots & d.v_n \ebm = 
      \bbm 1 & \cdots & 1 \ebm = d^T.
   \]
  \item[(b)] Now let $q$ be a probability vector.  Then all entries in
   $P$ and $q$ are nonnegative, and the entries in $Pq$ are sums of
   entries in $P$ multiplied by entries in $q$, so they are again
   nonnegative.  Moreover, the sum of the entries in $Pq$ is
   $d.Pq=d^TPq$, but $d^TP=d$, so this is the same as $d^Tq=d.q$,
   which is one by assumption.  This proves that $Pq$ is a probability
   vector.
  \item[(c)] Now let $Q$ be another $n\tm n$ stochastic matrix.  Let
   $w_1,\dotsc,w_n$ be the columns of $Q$, which are probability
   vectors.  We then have
   \[ PQ 
       = P \left[\begin{array}{c|c|c} 
            w_1 & \cdots & w_n 
           \end{array}\right] 
       =  \left[\begin{array}{c|c|c} 
            Pw_1 & \cdots & Pw_n 
           \end{array}\right].
   \]
   The vectors $Pw_1,\dotsc,Pw_n$ are probability vectors by part~(b),
   and it follows that $PQ$ is a stochastic matrix.
 \end{itemize}
\end{solution}

\begin{exercise}\label{ex-stochastic-ii}
 Suppose that $0<p,q<1$, and put $P=\bbm p& 1-q \\ 1-p & q\ebm$ (so
 $P$ is a stochastic matrix).  Find the eigenvalues and eigenvectors
 of $P$ in terms of $p$ and $q$.  

 (\textbf{Hint:} a general theorem from lectures tells you one of the
 eigenvalues.) 
\end{exercise}
\begin{solution}
 The characteristic polynomial is
 \[ \chi_P(t) = \det\bbm p-t & 1-q \\ 1-p & q-t \ebm =
     (p-t)(q-t)-(1-p)(1-q) = t^2-(p+q)t + (p+q-1).
 \]
 Every stochastic matrix has $1$ as an eigenvalue, so one of the roots
 of $\chi_P(t)$ is at $t=1$.  We can divide $t^2-(p+q)t - (1-p-q)$ by
 $t-1$ to obtain the factorisation $\chi_P(t)=(t-1)(t-(p+q-1))$, so
 the other eigenvalue is $r=p+q-1$.  To find an eigenvector
 $u_1=\bbm x\\ y\ebm$ of eigenvalue $1$, we must solve 
 \[ (P-I)u_1 = \bbm p-1 & 1-q \\ 1-p & q-1\ebm \bbm x\\ y\ebm 
     = \bbm 0\\ 0\ebm.
 \]
 This reduces to $(1-p)x=(1-q)y$ so we can take $y=1/(1-q)$ to get
 $x=1/(1-p)$ and $u_1=\bbm 1/(1-p)\\ 1/(1-q)\ebm$.

 Next, to find an eigenvector of eigenvalue $r$ we note that 
 \[ P-rI = \bbm p & 1-q \\ 1-p & q \ebm -
           \bbm p+q-1 & 0 \\ 0 & p+q-1 \ebm 
    = \bbm 1-q & 1-q \\ 1-p & 1-p \ebm.
 \]
 It follows that the vector $u_2=\bbm 1\\ -1\ebm$ satisfies
 $(P-rI)u_2=0$, so this is the required eigenvector.
\end{solution}

\begin{exercise}\label{ex-markov-ii}
 Consider the following Markov chain:
 \begin{center}
  \begin{tikzpicture}[scale=2,auto,shorten >= 1pt]
   \node (state1) at (1,0) [circle,draw] {$1$}; 
   \node (state2) at (2,0) [circle,draw] {$2$}; 
   \node (state3) at (3,0) [circle,draw] {$3$}; 
   \draw[->,bend right] (state1) to node [swap] {$\frac{2}{3}$} (state2);
   \draw[->,bend left]  (state3) to node {$\frac{2}{3}$} (state2);
   \draw[->,bend right] (state2) to node [swap] {$\frac{1}{3}$} (state1);
   \draw[->,bend left]  (state2) to node {$\frac{1}{3}$} (state3);
   \draw[->,loop left]  (state1) to node {$\frac{1}{3}$} (state1);
   \draw[->,loop right] (state3) to node {$\frac{1}{3}$} (state3);
   \draw[->,loop above] (state2) to node {$\frac{1}{3}$} (state2);
  \end{tikzpicture}
 \end{center}
 Write down the transition matrix and find its eigenvalues and
 eigenvectors.  What is the stationary distribution?
\end{exercise}
\begin{solution}
 The transition matrix is
 \[ P = \bbm 
      p_{1\xla{}1} & p_{1\xla{}2} & p_{1\xla{}3} \\
      p_{2\xla{}1} & p_{2\xla{}2} & p_{2\xla{}3} \\
      p_{3\xla{}1} & p_{3\xla{}2} & p_{3\xla{}3} 
     \ebm = 
     \bbm 
      1/3 & 1/3 & 0   \\
      2/3 & 1/3 & 2/3 \\
      0   & 1/3 & 1/3
     \ebm.
 \]
 For the characteristic polynomial, we have
 \begin{align*}
   \chi_P(t) &=
     \det\bbm 1/3-t & 1/3 & 0 \\
              2/3 & 1/3-t & 2/3 \\
              0 & 1/3 & 1/3-t \ebm \\
    &= (1/3-t)\det\bbm 1/3-t & 2/3 \\ 1/3 & 1/3-t \ebm 
       -(1/3) \det\bbm 2/3 & 2/3 \\ 0 & 1/3-t \ebm \\
    \det\bbm 1/3-t & 2/3 \\ 1/3 & 1/3-t \ebm 
     &= (1/3-t)^2 - 2/9 = t^2 - (2/3)t - 1/9 \\
    \det\bbm 2/3 & 2/3 \\ 0 & 1/3-t \ebm 
     &= 2/9 - (2/3)t \\
   \chi_P(t) &= (1/3-t)(t^2 - (2/3)t - 1/9) - (1/3)(2/9-(2/3)t)\\
     &= -1/9 + (1/9)t + t^2 -t^3 
      = (1-t)(t^2-1/9) = (1-t)(t-1/3)(t+1/3).
 \end{align*}
 From this we see that the eigenvalues are $1/3$, $-1/3$ and $1$.  To
 find an eigenvector $u_1$ of eigenvalue $1/3$ we row-reduce
 $P-\frac{1}{3}I$:
 \[ \bbm 0 & 1/3 & 0 \\ 2/3 & 0 & 2/3 \\ 0 & 1/3 & 0 \ebm 
    \to 
    \bbm 1 & 0 & 1 \\ 0 & 1 & 0 \\ 0 & 0 & 0 \ebm.
 \]
 This means that if $u_1=\bbm x&y&z\ebm^T$ we must have
 \[ \bbm 1 & 0 & 1 \\ 0 & 1 & 0 \\ 0 & 0 & 0 \ebm
     \bbm x \\ y \\ z\ebm = \bbm 0 \\ 0 \\ 0 \ebm 
 \]
 which gives $x=-z$ with $y=0$.  Taking $z=1$ we get
 $u_1=\bbm -1&0&1\ebm^T$.  Next, to find an eigenvector $u_2$ of
 eigenvalue $-1/3$ we row-reduce $P+\frac{1}{3}I$:
 \[ \bbm 2/3 & 1/3 & 0 \\ 2/3 & 2/3 & 2/3 \\ 0 & 1/3 & 2/3 \ebm 
    \to 
    \bbm 1 & 1/2 & 0 \\ 1 & 1 & 1 \\ 0 & 1 & 2 \ebm
    \to
    \bbm 1 & 0 & -1 \\ 0 & 1 & 2 \\ 0 & 0 & 0 \ebm.
 \]
 This means that if $u_2=\bbm x&y&z\ebm^T$ we must have
 \[ \bbm 1 & 0 & -1 \\ 0 & 1 & 2 \\ 0 & 0 & 0 \ebm
     \bbm x \\ y \\ z\ebm = \bbm 0 \\ 0 \\ 0 \ebm 
 \]
 which gives $x=z$ and $y=-2z$.  Taking $z=1$ we get
 $u_2=\bbm 1&-2&1\ebm^T$.  Finally, to find an eigenvector of
 eigenvalue $1$ we row-reduce $P-I$:
 \[ \bbm -2/3 & 1/3 & 0 \\ 2/3 & -2/3 & 2/3 \\ 0 & 1/3 & -2/3 \ebm 
    \to 
    \bbm 1 & -1/2 & 0 \\ 1 & -1 & 1 \\ 0 & 1 & -2 \ebm
    \to
    \bbm 1 & 0 & -1 \\ 0 & 1 & -2 \\ 0 & 0 & 0 \ebm.
 \]
 This means that if $u_3=\bbm x&y&z\ebm^T$ we must have
 \[ \bbm 1 & 0 & -1 \\ 0 & 1 & -2 \\ 0 & 0 & 0 \ebm
     \bbm x \\ y \\ z\ebm = \bbm 0 \\ 0 \\ 0 \ebm 
 \]
 which gives $x=z$ and $y=2z$.  Taking $z=1$, we get
 $u_3=\bbm 1&2&1\ebm^T$.  

 We are also asked for a stationary distribution, which should be an
 eigenvector of eigenvalue $1$ that is also a probability vector.  To
 make $u_3$ into a probability vctor we need to divide it by $4$,
 giving $\bbm \frac{1}{4}&\frac{1}{2}&\frac{1}{4}\ebm^T$ as the
 stationary distribution.
\end{solution}

\begin{exercise}\label{ex-markov-iii}
 Consider the following Markov chain:
 \begin{center}
  \begin{tikzpicture}[scale=2,auto,shorten >= 1pt]
   \node (state1) at (0,0) [circle,draw] {$1$}; 
   \node (state2) at (1,0) [circle,draw] {$2$}; 
   \node (state3) at (1,1) [circle,draw] {$3$}; 
   \node (state4) at (0,1) [circle,draw] {$4$}; 
   \draw[->,bend left] (state1) to node {$\frac{1}{2}$} (state2);
   \draw[->,bend left] (state2) to node {$\frac{1}{2}$} (state3);
   \draw[->,bend left] (state3) to node {$\frac{1}{2}$} (state4);
   \draw[->,bend left] (state4) to node {$\frac{1}{2}$} (state1);
   \draw[->,bend left] (state1) to node {$\frac{1}{2}$} (state4);
   \draw[->,bend left] (state2) to node {$\frac{1}{2}$} (state1);
   \draw[->,bend left] (state3) to node {$\frac{1}{2}$} (state2);
   \draw[->,bend left] (state4) to node {$\frac{1}{2}$} (state3);
  \end{tikzpicture}
 \end{center}
 Write down the transition matrix $P$ and check that $P^3=P$.  Deduce
 that $P^{2k+1}=P$ for all $k\geq 0$.  If we start in state $1$ at
 $t=0$, what is the probability of being in state $3$ at $t=1111$?

 \textbf{Note:} you do not need to calculate any eigenvalues or
 eigenvectors for this question.
\end{exercise}
\begin{solution}
 The transition matrix is
 \[ P =
     \bbm 
      p_{1\xla{}1} & p_{1\xla{}2} & p_{1\xla{}3} & p_{1\xla{}4} \\
      p_{2\xla{}1} & p_{2\xla{}2} & p_{2\xla{}3} & p_{2\xla{}4} \\
      p_{3\xla{}1} & p_{3\xla{}2} & p_{3\xla{}3} & p_{3\xla{}4} \\
      p_{4\xla{}1} & p_{4\xla{}2} & p_{4\xla{}3} & p_{4\xla{}4}
     \ebm = 
     \frac{1}{2}
     \bbm 
      0 & 1 & 0 & 1 \\
      1 & 0 & 1 & 0 \\
      0 & 1 & 0 & 1 \\
      1 & 0 & 1 & 0
     \ebm.
 \]
 This gives
 \begin{align*}
  P^2 &= \frac{1}{4}
     \bbm 
      0 & 1 & 0 & 1 \\
      1 & 0 & 1 & 0 \\
      0 & 1 & 0 & 1 \\
      1 & 0 & 1 & 0
     \ebm
     \bbm 
      0 & 1 & 0 & 1 \\
      1 & 0 & 1 & 0 \\
      0 & 1 & 0 & 1 \\
      1 & 0 & 1 & 0
     \ebm 
     = \frac{1}{2} 
     \bbm
      1 & 0 & 1 & 0 \\
      0 & 1 & 0 & 1 \\
      1 & 0 & 1 & 0 \\
      0 & 1 & 0 & 1
     \ebm \\
  P^3 &= \frac{1}{4}
     \bbm 
      0 & 1 & 0 & 1 \\
      1 & 0 & 1 & 0 \\
      0 & 1 & 0 & 1 \\
      1 & 0 & 1 & 0
     \ebm
     \bbm
      1 & 0 & 1 & 0 \\
      0 & 1 & 0 & 1 \\
      1 & 0 & 1 & 0 \\
      0 & 1 & 0 & 1
     \ebm 
     = \frac{1}{2}
     \bbm 
      0 & 1 & 0 & 1 \\
      1 & 0 & 1 & 0 \\
      0 & 1 & 0 & 1 \\
      1 & 0 & 1 & 0
     \ebm = P.
 \end{align*}
 We can now multiply both sides of the equation $P^3=P$ by $P^2$ to
 get $P^5=P^3$, but $P^3=P$ so $P^5=P$.  We now multiply both sides by
 $P^2$ again to get $P^7=P^3=P$, and again to get $P^9=P^3=P$ and so
 on.  Ths shows that $P^{2k+1}=P$ for all $k\geq 0$.

 Now suppose we are definitely in state $1$ at $t=0$, so the
 distribution vector $r_0$ is $\bbm 1 & 0 & 0 & 0 \ebm^T$.  The
 distribution at $t=1111$ is then $r_{1111}=P^{1111}r_0$, but we have
 just seen that $P^{1111}=P$, so 
 \[ r_{1111} = Pr_0 =
    \frac{1}{2}
     \bbm 
      0 & 1 & 0 & 1 \\
      1 & 0 & 1 & 0 \\
      0 & 1 & 0 & 1 \\
      1 & 0 & 1 & 0
     \ebm
     \bbm 1 \\ 0 \\ 0 \\ 0 \ebm =
     \bbm 0 \\ 1/2 \\ 0 \\ 1/2 \ebm.
 \]
 By looking at the third entry, we see that the probability of being
 in state $3$ at $t=1111$ is zero.  In fact, this can be seen even
 more directly.  From the diagram we see that every second we switch
 from an odd-numbered state to an even numbered state or
 \emph{vice-versa}.  We start in state $1$ at $t=0$, and at $t=1111$ we
 have switched over an odd number of times, so we must be in an
 even-numbered state, and in particular we cannot be in state $3$.
\end{solution}

\section{Lecture 13}

\begin{exercise}\label{eg-page-rank-i}
 Consider the following web of pages and links.
 \begin{center}
  \begin{tikzpicture}[scale=3,auto,shorten >= 1pt]
   \node (state1) at (  0:1) [circle,draw] {$1$};
   \node (state2) at ( 45:1) [circle,draw] {$2$};
   \node (state3) at ( 90:1) [circle,draw] {$3$};
   \node (state4) at (135:1) [circle,draw] {$4$};
   \node (state5) at (180:1) [circle,draw] {$5$};
   \node (state6) at (225:1) [circle,draw] {$6$};
   \node (state7) at (270:1) [circle,draw] {$7$};
   \node (state8) at (315:1) [circle,draw] {$8$};
   \node (state9) at (0,0)   [circle,draw] {$9$};
   \draw[->]           (state1) to (state2);
   \draw[->]           (state2) to (state3);
   \draw[->]           (state3) to (state4);
   \draw[->]           (state4) to (state5);
   \draw[->]           (state5) to (state6);
   \draw[->]           (state6) to (state7);
   \draw[->]           (state7) to (state8);
   \draw[->]           (state8) to (state1);
   \draw[->,bend left=10] (state1) to (state9);
   \draw[->,bend left=10] (state2) to (state9);
   \draw[->,bend left=10] (state3) to (state9);
   \draw[->,bend left=10] (state4) to (state9);
   \draw[->,bend left=10] (state5) to (state9);
   \draw[->,bend left=10] (state6) to (state9);
   \draw[->,bend left=10] (state7) to (state9);
   \draw[->,bend left=10] (state8) to (state9);
   \draw[->,bend left=10] (state9) to (state1);
   \draw[->,bend left=10] (state9) to (state2);
   \draw[->,bend left=10] (state9) to (state3);
   \draw[->,bend left=10] (state9) to (state4);
   \draw[->,bend left=10] (state9) to (state5);
   \draw[->,bend left=10] (state9) to (state6);
   \draw[->,bend left=10] (state9) to (state7);
   \draw[->,bend left=10] (state9) to (state8);
  \end{tikzpicture}
 \end{center}
 Let $a$ be the PageRank of page $1$, and let $b$ be the PageRank of
 page $9$.  By symmetry, pages $2$ to $8$ must also have rank $a$.
 Use the consistency and normalisation conditions to find $a$ and $b$
 (without writing down any $9\tm 9$ matrices).
\end{exercise}
\begin{solution}
 First, the normalisation condition says that $\sum_{i=1}^9r_i=1$.  As
 $r_1=\dotsb=r_8=a$ and $r_9=b$, this means that $8a+b=1$.

 Next, note that the numbers of outgoing links are $N_1=\dotsb=N_8=2$
 and $N_9=8$.  As page $1$ has links from pages $8$ and $9$, the
 consistency condition says that $r_1=r_8/N_8+r_9/N_9$, or in other
 words $a=a/2+b/8$.  By symmetry, pages $2$ to $8$ have the same
 consistency condition as page $1$.  On the other hand, page $9$ has
 links from pages $1$ to $8$, so the consistency condition there is 
 \[ b = r_9 = r_1/N_1 + \dotsb + r_8/N_8 =
     a/2 + \dotsb + a/2 = 4a.
 \]
 Solving the equations $8a+b=1$, $a=a/2+b/8$ and $b=4a$ gives $a=1/12$
 and $b=1/3$.  
\end{solution}

\section{Lecture 14}

\begin{exercise}\label{ex-subspace-i}
 Consider the following sets
 \begin{align*}
  P_0 &= \{\bbm x&y\ebm^T \in\R^2\st x^2\geq 1\} \\
  P_1 &= \{\bbm x&y\ebm^T\in\R^2\st xy\geq 0\} \\
  P_2 &= \{\bbm x&y\ebm^T\in\R^2\st y\leq x^2\} \\
  P_3 &= \{\bbm x&y\ebm^T\in\R^2\st x+y \text{ is an integer }\} \\
  P_4 &= \{\bbm x&y\ebm^T\in\R^2\st x^2+y^2\leq 1\} \\
 \end{align*}
 The set $P_0$ is not closed under addition, because the vectors
 $u_0=\bbm 1\\0\ebm$ and $u_1=\bbm -1\\0\ebm$ both lie in $P_0$, but
 the sum $u_0+u_1=\bbm 0\\0\ebm$ does not lie in $P_0$.  Moreover, the
 set $P_0$ is not closed under scalar multiplication, because the
 vector $u_2=\bbm 1\\1\ebm$ lies in $P_0$, but the product
 $0.5 u_2=\bbm 0.5\\0.5\ebm$ does not lie in $P_0$.  Give similarly
 specific examples to show that
 \begin{itemize}
  \item[(a)] $P_1$ is not closed under addition. 
  \item[(b)] $P_2$ is not closed under addition. 
  \item[(c)] $P_2$ is not closed under scalar multiplication. 
  \item[(d)] $P_3$ is not closed under scalar multiplication. 
  \item[(e)] $P_4$ is not closed under scalar multiplication. 
 \end{itemize}
\end{exercise}
\begin{solution}
 \begin{itemize}
  \item[(a)] $P_1$ contains the vectors $u_3=\bbm 0\\1\ebm$ and
   $u_4=\bbm -1\\0\ebm$ but not the sum $u_3+u_4=\bbm -1\\1\ebm$.
  \item[(b)] $P_2$ contains the vectors $u_5=\bbm 1\\1\ebm$ and
   $u_6=\bbm -1\\1\ebm$ but not the sum $u_5+u_6=\bbm 0\\2\ebm$.
  \item[(c)] $P_2$ contains the vector $u_7=\bbm 0\\-1\ebm$ but not
   the vector $(-1)u_7=\bbm 0\\ 1\ebm$.
  \item[(d)] $P_3$ contains the vector $u_8=\bbm 1\\0\ebm$ but not the
   vector $0.5u_8=\bbm 0.5\\0\ebm$.
  \item[(e)] $P_4$ contains the vector $u_8$ as above, but not the
   vector $2u_8=\bbm 2\\0\ebm$.
 \end{itemize}
\end{solution}

\begin{exercise}\label{ex-subspace-ii}
 Which of the following sets is a subspace of $\R^4$?
 \begin{itemize}
  \item[(a)] $V_1$ is the set of vectors of the form
   $\bbm s&t+s&t-s&t\ebm^T$ (for some $s,t\in\R$).
  \item[(b)] $V_2$ is the set of vectors of the form
   $\bbm t&t^2&t^3&t^4\ebm^T$ (for some $t\in\R$).
  \item[(c)] $V_3$ is the set of vectors
   $v=\bbm w&x&y&z\ebm^T$ that satisfy $w+10x+100y+1000z=1$.
  \item[(d)] $V_4$ is the set of vectors
   $v=\bbm w&x&y&z\ebm^T$ that satisfy $w-x+y-z=0$.
  \item[(e)] $V_5$ is the set of vectors
   $v=\bbm w&x&y&z\ebm^T$ that satisfy $(w-x)^2+(y-z)^2=0$.
 \end{itemize}
\end{exercise}
\begin{solution}
 \begin{itemize}
  \item[(a)] The set $V_1$ is a subspace of $\R^4$.  Indeed, if $v,v'\in V_1$
   then we have $v=\bbm s&t+s&t-s&t\ebm^T$ and $v'=\bbm
   s'&t'+s'&t'-s'&t'\ebm^T$ for some $s,t,s',t'in\R$.  This means that
   $v+v'=\bbm s''&t''+s''&t''-s''&t''\ebm^T$, where $s''=s+s'$ and
   $t''=t+t'$.  It follows that $v+v'\in V_1$, so $V_1$ is closed under
   addition.  Similarly, if $a$ is any scalar, we have
   $av=\bbm s^*&t^*+s^*&t^*-s^*&t^*\ebm^T$, where $s^*=as$ and
   $t^*=at$.  This shows that $av\in V_1$, so $V_1$ is closed under
   scalar multiplication.  Finally, by taking $s=t=0$ we see that the
   zero vector lies in $V_1$.
  \item[(b)] The set $V_2$ is not a subspace of $\R^4$.  Indeed, by
   taking $t=1$ we see that the vector $v=\bbm 1&1&1&1\ebm^T$ lies in
   $V_2$, but the vector $2v=\bbm 2&2&2&2\ebm^T$ does not lie in
   $V_2$, so $V_2$ is not closed under scalar multiplication.
  \item[(c)] The set $V_3$ is not a subspace of $\R^4$, because the
   zero vector does not satisfy $w+10x+100y+1000z=1$ and so is not an
   element of $V_3$.
  \item[(d)] The set $V_4$ is a subspace of $\R^4$.  Indeed, the zero
   vector $\bbm w&x&y&z\ebm^T=\bbm 0&0&0&0\ebm^T$ satisfies $w-x+y-z$
   and so $0\in V_4$.  If we have elements $v=\bbm w&x&y&z\ebm^T$ and
   $v'=\bbm w'&x'&y'&z'\ebm^T$ in $V_4$ then the we have $w-x+y-z=0$
   and $w'-x'+y'-z'=0$.  By adding these equations we see that
   $(w+w')-(x+x')+(y+y')-(z+z')=0$, which shows that the sum $v+v'$ is
   again an element of $V_4$, so $V_4$ is closed under addition.  A
   similar argument shows that it is closed under scalar
   multiplication.
  \item[(e)] The set $V_5$ is also a subspace of $\R^4$, although this
   fact is slightly disguised by the way that we have defined it.
   Because all squares are nonnegative, we see that the only way
   $(w-x)^2+(y-z)^2$ can be zero is if $w=x$ and $y=z$.  This means
   that $V_5$ is the set of vectors of the form $\bbm s&s&t&t\ebm^T$,
   which is a subspace by the same method that we used in part~(a).
 \end{itemize}
\end{solution}

\begin{exercise}\label{ex-subspace-iii}
 \begin{itemize}
  \item[(a)] Give an example of a subset $U_0\sse\R^2$ that contains
   zero and is closed under addition but is not closed under scalar
   multiplication.
  \item[(b)] Give an example of a subset $U_1\sse\R^2$ that contains
   zero and is closed under scalar multiplication but is not closed
   under addition.
  \item[(c)] Suppose that $U_2$ is a nonempty subset of $\R^2$ that is
   closed under addition and scalar multiplication.  Show that $U_2$
   contains the zero vector.
  \item[(d)] Let $U_3$ be a subspace of $\R^1=\R$.  Show that $U_3$ is
   either $\{0\}$ or all of $\R$.
 \end{itemize}
\end{exercise}
\begin{solution}
 \begin{itemize}
  \item[(a)] The simplest example is 
   \[ U_0=\left\{\bbm x \\ y\ebm \in\R^2\st x,y\geq 0\right\}. \]
   This is not closed under scalar multiplication, because
   $\bbm 1\\ 1\ebm\in U_0$ but $(-1)\bbm 1 \\ 1\ebm\not\in U_0$.
  \item[(b)] The simplest example is 
   \[ U_1=\left\{\bbm x \\ y\ebm \in\R^2\st xy=0\right\}. \]
   This is not closed under addition, because
   $\bbm 1\\ 0\ebm\in U_1$ and $\bbm 0\\1\ebm\in U_1$ but
   $\bbm 1\\0\ebm+\bbm 0\\1\ebm=\bbm 1\\1\ebm\not\in U_1$.
  \item[(c)] As $U_2$ is nonempty, we can choose a vector $u\in U_2$.
   As $U_2$ is closed under scalar multiplication, we can multiply the
   vector $u\in U_2$ by the scalar $0\in\R$, and the result $0u$ will
   again be an element of $U_2$.  Of course $0u$ is just the zero
   vector, so the zero vector is an element of $U_2$.
  \item[(d)] Let $U_3$ be a subspace of $\R$.  As it is a subspace, it
   must contain zero.  If it does not contain anything else, then
   $U_3=\{0\}$.  Suppose instead that it does contain something else,
   so there is a nonzero element $u\in U_3$.  Consider another element
   $v\in\R$.  As we are working with elements of $\R^1$ which are just
   numbers, we can make sense of multiplication and division (which
   are not defined for vectors in $\R^2$ and beyond).  We can thus
   express $v$ as the product of the scalar $v/u$ with the vector
   $u\in U_3$.  (There is no problem with dividing by $u$, because we
   have assumed that $u\neq 0$.)  As $U_3$ is closed under scalar
   multiplication, the product $(v/u)u$ lies in $U_3$, or in other
   words $v\in U_3$.  This works for all vectors $v\in\R^1$, so we
   have $U_3=\R^1$.
 \end{itemize}
\end{solution}

\section{Lecture 15}

\begin{exercise}\label{ex-span-form-i}
 Let $V$ be the set of vectors of the form
 \[ v = \bbm 2p-q  & q+r+s & 3p+2s & r-s \ebm \]
 (where $p$, $q$, $r$ and $s$ are arbitrary real numbers).  Find a
 list of vectors whose span is $V$.
\end{exercise}
\begin{solution}
 This is similar to examples~\ref{eg-span-form-i}
 and~\ref{eg-span-form-ii}.  The general form for elements of $V$ is 
 \[ v = \bbm 2p-q  & q+r+s & 3p+2s & r-s \ebm
      = p \bbm  2\\0\\3\\ 0 \ebm + 
        q \bbm -1\\1\\0\\ 0 \ebm +
        r \bbm  0\\1\\0\\ 1 \ebm +
        s \bbm  0\\1\\0\\-1 \ebm.
 \]
 In other words, the elements of $V$ are all the possible linear
 combinations of the four vectors occuring in the above formula.  In
 other words, we have 
 \[ V = \spn\left(
        \bbm  2\\0\\3\\ 0 \ebm ,\quad
        \bbm -1\\1\\0\\ 0 \ebm ,\quad
        \bbm  0\\1\\0\\ 1 \ebm ,\quad
        \bbm  0\\1\\0\\-1 \ebm 
        \right).
 \]
\end{solution}

\begin{exercise}\label{ex-ann-eqs-i}
 Put 
 \[ A = \bbm 1 & 6 & 8 \\ 7 & 2 & 3 \ebm \]
 and $V=\{v\in\R^3\st Av=0\}$.  Find a list of vectors whose
 annihilator is $V$.
\end{exercise}
\begin{solution}
 This is an instance of Proposition~\ref{prop-ker-ann}: the space $V$
 is by definition the kernel of $A$, and that proposition tells us
 that the kernel is the annihilator of the transposed rows.  Thus, if
 we put $a_1=\bbm 1&6&8\ebm^T$ and $a_2=\bbm 7&2&3\ebm^T$ then 
 $V=\ann(a_1,a_2)$.  This can also be seen quite easily without
 reference to Proposition~\ref{prop-ker-ann}.  If $v=\bbm x&y&z\ebm^T$
 then 
 \[ Av = \bbm 1 & 6 & 8 \\ 7 & 2 & 3 \ebm \bbm x \\ y \\ z \ebm =
    \bbm x+6y+8z \\ 7x+2y+3z \ebm = 
    \bbm a_1.v \\ a_2.v \ebm,
 \]
 so $v$ lies in $V$ iff $Av=0$ iff $a_1.v=a_2.v=0$ iff $v$ lies in
 $\ann(a_1,a_2)$; this means that $V=\ann(a_1,a_2)$ as before.
\end{solution}

\begin{exercise}\label{ex-check-ann-span}
 Put 
 \[ a_1 = \bbm 1 \\ 2 \\ 3 \\ 4 \ebm \hspace{3em}
    a_2 = \bbm 4 \\ 3 \\ 2 \\ 1 \ebm \hspace{3em}
    u = \bbm 1 \\ -1 \\ -1 \\ 1 \ebm \hspace{3em}
    v = \bbm 1 \\ 1 \\ 1 \\ 1 \ebm.
 \]
 \begin{itemize}
  \item[(a)] Does $u$ lie in $\ann(a_1,a_2)$?
  \item[(b)] Does $v$ lie in $\ann(a_1,a_2)$?
  \item[(c)] Does $u$ lie in $\spn(a_1,a_2)$?
  \item[(d)] Does $v$ lie in $\spn(a_1,a_2)$?
 \end{itemize}
\end{exercise}
\begin{solution}
 \begin{itemize}
  \item[(a)] Yes, we have $u.a_1=1-2-3+4=0$ and $u.a_2=4-3-2+1=0$, so
   $u\in\ann(a_1,a_2)$.
  \item[(b)] No, we have $v.a_1=1+2+3+4=10\neq 0$, so
   $v\not\in\ann(a_1,a_2)$.  (We also have $v.a_2\neq 0$, but the fact
   that $v.a_1\neq 0$ is already enough to show that
   $v\not\in\ann(a_1,a_2)$, so we do not really need to consider
   $v.a_2$.)
  \item[(c)] No, $u$ cannot be written as a linear combination of
   $a_1$ and $a_2$, so it does not lie in $\spn(a_1,a_2)$.  One way to
   check this is to use Method~\ref{meth-find-lincomb}, which involves
   row-reducing the matrix $[a_1|a_2|u]$:
   \[ 
     \bbm 1 & 4 & 1 \\
          2 & 3 & -1 \\
          3 & 2 & -1 \\
          4 & 1 &  1 \ebm 
     \to
     \bbm 1 &   4 &  1 \\
          0 &  -5 & -3 \\
          0 & -10 & -4 \\
          0 & -15 & -5 \ebm 
     \to
     \bbm 1 &   4 &  1 \\
          0 &   1 & 0.6 \\
          0 & -10 & -4 \\
          0 & -15 & -5 \ebm 
     \to
     \bbm 1 &   0 & -1.4 \\
          0 &   1 &  0.6 \\
          0 &   0 &  2 \\
          0 &   0 &  4 \ebm 
     \to
     \bbm 1 &   0 &  0 \\
          0 &   1 &  0 \\
          0 &   0 &  1 \\
          0 &   0 &  0 \ebm. 
   \]
   We end up with a pivot in the last column, which indicates that the
   equation $\lm_1a_1+\lm_2a_2=u$ cannot be solved for $\lm_1$ and
   $\lm_2$, or equivalently that $u$ is not a linear combination of
   $a_1$ and $a_2$.
  \item[(d)] Yes, it is easy to see by inspection that
   $v=(a_1+a_2)/5=0.2a_1+0.2a_2$, so $v$ is a linear combination of
   $a_1$ and $a_2$, or in other words $v\in\spn(a_1,a_2)$.
 \end{itemize}
\end{solution}

\begin{exercise}\label{ex-span-in-ann}
 Put 
 \[ a_1 = \bbm 1 \\ 1 \\ 2 \\ 2 \ebm \hspace{4em}
    a_2 = \bbm 2 \\ 2 \\ 1 \\ 1 \ebm \hspace{4em}
    b_1 = \bbm 3 \\ -3 \\ 4 \\ -4 \ebm \hspace{4em}
    b_2 = \bbm 4 \\ -4 \\ 3 \\ -3 \ebm.
 \] 
 Show that $\spn(a_1,a_2)\sse\ann(b_1,b_2)$.
\end{exercise}
\begin{solution}
 First, we have
 \begin{align*}
  a_1.b_1 &= 3-3+8-8 = 0 \\
  a_1.b_2 &= 4-4+6-6 = 0 \\
  a_2.b_1 &= 6-6+4-4 = 0 \\
  a_2.b_2 &= 8-8+3-3 = 0.
 \end{align*}
 Now consider an arbitrary element $v\in\spn(a_1,a_2)$.  By the
 definition of $\spn(a_1,a_2)$, this means that $v$ can be expressed
 as $v=\lm_1a_1+\lm_2a_2$ for some scalars $\lm_1$ and $\lm_2$.  This
 gives 
 \begin{align*}
  v.b_1 &= (\lm_1a_1+\lm_2a_2).b_1 = 
           \lm_1(a_1.b_1) + \lm_2 (a_2.b_1) = 
           \lm_1\tm 0 + \lm_2\tm 0 = 0 \\
  v.b_2 &= (\lm_1a_1+\lm_2a_2).b_2 = 
           \lm_1(a_1.b_2) + \lm_2 (a_2.b_2) = 
           \lm_1\tm 0 + \lm_2\tm 0 = 0.
 \end{align*}
 As $v.b_1=v.b_2=0$, we have $v\in\ann(b_1,b_2)$.  As this holds for
 every element of $\spn(a_1,a_2)$, we have
 $\spn(a_1,a_2)\sse\ann(b_1,b_2)$ as claimed.
\end{solution}

\begin{exercise}\label{ex-dim-span}
 Consider the vectors
 \[ v_1 = \bbm -1 \\  2 \\ -1 \\  3 \ebm \qquad
    v_2 = \bbm  1 \\ -1 \\  2 \\ -2 \ebm \qquad
    v_3 = \bbm  1 \\  0 \\  3 \\ -1 \ebm \qquad
    w_1 = \bbm -1 \\  5 \\  2 \\  6 \ebm \qquad
    w_2 = \bbm  1 \\  1 \\  4 \\  0 \ebm
 \]
 \begin{itemize}
  \item[(a)] Show that
   $\spn(v_1,v_2,v_3)=\spn(v_1,v_2)=\spn(w_1,w_2)$.
  \item[(b)] Find $\dim(\spn(v_1,v_2,v_3,w_1,w_2))$.
 \end{itemize}
\end{exercise}
\begin{solution}
 We will first give a solution that involves observing various
 identities between the given vectors, then a longer but more
 systematic solution by row-reduction.

 First, we observe that $v_3=v_1+2v_2$.  This allows us to rewrite any
 linear combination of $v_1$, $v_2$ and $v_3$ as a linear combination
 of $v_1$ and $v_2$ alone.  Thus, we have
 $\spn(v_1,v_2,v_3)=\spn(v_1,v_2)$.  

 Next, we observe that $w_1=4v_1+3v_2$ and $w_2=2v_1+3v_2$.  This
 shows that $w_1,w_2\in\spn(v_1,v_2)$ and so
 $\spn(w_1,w_2)\sse\spn(v_1,v_2)$.  In the opposite direction, we have 
 $v_1=(w_1-w_2)/2$ and $v_2=(2w_2-w_1)/3$, which shows that
 $v_1,v_2\in\spn(w_1,w_2)$ and so $\spn(v_1,v_2)\sse\spn(w_1,w_2)$.

 We now see that all of the given vectors are linear combinations of
 $v_1$ and $v_2$, so the space $V=\spn(v_1,v_2,v_3,w_1,w_2)$ is just
 the same as $\spn(v_1,v_2)$.  Recall that a list of two nonzero
 vectors is only linearly dependent if the vectors are scalar
 multiples of each other.  This is clearly not the case for $v_1$ and
 $v_2$, so we see that the list $v_1,v_2$ is a basis for $V$, so
 $\dim(V)=2$.  

 The more systematic approach is just to find the canonical bases for
 all the spaces involved.  We have
 \[ 
  [v_1|v_2|v_3]^T 
  = 
  \bbm 
   -1 &  2 & -1 &  3 \\
    1 & -1 &  2 & -2 \\
    1 &  0 &  3 & -1 
  \ebm
  \to 
  \bbm 
    1 & -2 &  1 & -3 \\
    0 &  1 &  1 &  1 \\
    0 &  2 &  2 &  2 
  \ebm
  \to
  \bbm 
    1 &  0 &  3 & -1 \\
    0 &  1 &  1 &  1 \\
    0 &  0 &  0 &  0 
  \ebm
 \]
 It follows that the vectors $a_1=\bbm 1&0&3&-1\ebm^T$ and
 $a_2=\bbm 0&1&1&1\ebm^T$ form the canonical basis for
 $\spn(v_1,v_2,v_3)$.  We can perform the same row-reduction leaving
 out the last row to see that $a_1$ and $a_2$ also form the canonical
 basis for $\spn(v_1,v_2)$, so $\spn(v_1,v_2,v_3)=\spn(v_1,v_2)$.
 Similarly, we have
 \[ [w_1|w_2]^T =
     \bbm
      -1 &  5 &  2 &  6 \\
       1 &  1 &  4 &  0 
     \ebm 
     \to
     \bbm
       1 & -5 & -2 & -6 \\
       0 &  6 &  6 &  6 
     \ebm 
     \bbm
       1 &  0 &  3 & -1 \\
       0 &  1 &  1 &  1 
     \ebm 
     =
     [a_1|a_2]^T
 \]
 This shows that $a_1$ and $a_2$ also form the canonical
 basis for $\spn(w_1,w_2)$, so
 $\spn(v_1,v_2,v_3)=\spn(v_1,v_2)=\spn(w_1,w_2)$.  From this it
 follows as before that $\spn(v_1,v_2,v_3,w_1,w_2)$ is yet another
 description of the same space, and the canonical basis has two
 vectors so the dimension is two.
\end{solution}

\section{Lecture 16}

\begin{exercise}\label{ex-jumps-i}
 Put $V=\spn(v_1,v_2,v_3)$, where
 \begin{align*}
  v_1 &= \bbm 0&2&6&10&1&0 \ebm^T \\
  v_2 &= \bbm 0&1&3&5&1&-3 \ebm^T \\
  v_3 &= \bbm 0&3&9&15&1&3 \ebm^T.
 \end{align*}
 \begin{itemize}
  \item[(a)] What is the dimension of $V$?
  \item[(b)] What is the canonical basis for $V$?
  \item[(c)] What is the set $J(V)$ of jumps for $V$?
 \end{itemize}
\end{exercise}
\begin{solution}
 We can row-reduce the matrix $A=[v_1|v_2|v_3]^T$ as follows:
 \[ A =
    \bbm 
     0&2&6&10&1&0 \\
     0&1&3&5&1&-3 \\
     0&3&9&15&1&3
    \ebm
    \to
    \bbm 
     0&0&0&0&-1&6 \\
     0&1&3&5&1&-3 \\
     0&0&0&0&-2&12
    \ebm
    \to
    \bbm 
     0&1&3&5&0& 3 \\
     0&0&0&0&1&-6 \\
     0&0&0&0&0&0
    \ebm
    = B.
 \]
 According to Method~\ref{meth-span-canonical}, the canonical basis
 for $V$ consists of the transposes of the nonzero rows in $B$, or in
 other words the vectors
 \[ u_1=\bbm 0&1&3&5&0&3\ebm^T \hspace{4em}
    u_2=\bbm 0&0&0&0&1&-6 \ebm.
 \]
 As this basis consists of two vectors, we have $\dim(V)=2$.
 According to Lemma~\ref{lem-jumps-pivots}, the jumps for $V$ are the
 pivot columns for the above matrix $B$.  There are pivots in columns
 $2$ and $5$, so $J(V)=\{2,5\}$.
\end{solution}

\begin{exercise}\label{ex-span-canonical-i}
 Let $V$ be the set of all vectors of the form 
 \[ v = \bbm p+q & p+2q & p+r & p+3r \ebm^T. \]
 You may assume that this is a subspace.  Find a list of vectors that
 spans $V$, and then find the canonical basis for $V$.
\end{exercise}
\begin{solution}
 A general element of $V$ has the form
 \[ v = 
     \bbm p+q & p+2q & p+r & p+3r \ebm = 
     p \bbm 1\\1\\1\\1\ebm + 
     q \bbm 1\\2\\0\\0\ebm +
     r \bbm 0\\0\\1\\3\ebm.
 \]
 In other words, the elements of $V$ are precisely the linear
 combinations of the vectors
 \[ v_1 = \bbm 1 \\ 1 \\ 1 \\ 1 \ebm \hspace{4em} 
    v_2 = \bbm 1 \\ 2 \\ 0 \\ 0 \ebm \hspace{4em} 
    v_3 = \bbm 0 \\ 0 \\ 1 \\ 3 \ebm.
 \]
 For the canonical basis, we perform the following row-reduction:
 \[ 
  \bbm 
   1 & 1 & 1 & 1 \\
   1 & 2 & 0 & 0 \\
   0 & 0 & 1 & 3
  \ebm
  \to 
  \bbm 
   1 & 1 & 1 & 1 \\
   0 & 1 &-1 &-1 \\
   0 & 0 & 1 & 3
  \ebm
  \to 
  \bbm 
   1 & 0 & 2 & 2 \\
   0 & 1 &-1 &-1 \\
   0 & 0 & 1 & 3
  \ebm
  \to 
  \bbm 
   1 & 0 & 0 &-4 \\
   0 & 1 & 0 & 2 \\
   0 & 0 & 1 & 3
  \ebm.
 \]
 We conclude that the canonical basis consists of the vectors
 \[ 
   w_1 = \bbm 1 & 0 & 0 & -4 \ebm^T \hspace{2em}
   w_2 = \bbm 0 & 1 & 0 &  2 \ebm^T \hspace{2em}
   w_3 = \bbm 0 & 0 & 1 &  3 \ebm^T.
 \]
\end{solution}

\begin{exercise}\label{ex-jumps-ii}
 Put $V=\spn(e_1-e_2,e_2-e_3,\dotsc,e_{n-1}-e_n)\sse\R^n$, where $e_i$
 is the $i$'th standard basis vector for $\R^n$.
 \begin{itemize}
  \item[(a)] What is the dimension of $V$?
  \item[(b)] What is the canonical basis for $V$?
  \item[(c)] What is the set $J(V)$ of jumps for $V$?
 \end{itemize}
 (You can start by doing the case $n=5$ by row-reduction if you like,
 but ideally you should give an answer for the general case, together
 with a more abstract proof that your answer is correct.)
\end{exercise}
\begin{solution}
 Put $v_i=e_i-e_{i+1}$, so $V=\spn(v_1,\dotsc,v_{n-1})$.  
 For the case $n=5$ we have can row-reduce the matrix
 $A=[v_1|v_2|v_3|v_4]^T$ as follows:
 \[ 
   \bbm 
    1&-1&0&0&0 \\
    0&1&-1&0&0 \\
    0&0&1&-1&0 \\
    0&0&0&1&-1
   \ebm
   \to 
   \bbm 
    1&-1&0&0&0 \\
    0&1&-1&0&0 \\
    0&0&1&0&-1 \\
    0&0&0&1&-1
   \ebm
   \to 
   \bbm 
    1&-1&0&0&0 \\
    0&1&0&0&-1 \\
    0&0&1&0&-1 \\
    0&0&0&1&-1
   \ebm
   \to 
   \bbm 
    1&0&0&0&-1 \\
    0&1&0&0&-1 \\
    0&0&1&0&-1 \\
    0&0&0&1&-1
   \ebm
 \]
 The final matrix $B$ can be described as $[w_1|w_2|w_3|w_4]^T$, where
 $w_i=e_i-e_4$.  It follows that these vectors $w_i$ form the
 canonical basis for $V$, so $\dim(V)=4$.  Moreover, the set of jumps
 for $V$ is the set of pivot columns for $B$, namely $\{1,2,3,4\}$.

 The same pattern works for general $n$.  In more detail, we can
 define vectors $w_1,\dotsc,w_{n-1}$ by $w_i=e_i-e_n$ and
 $W=\spn(w_1,\dotsc,w_{n-1})$.  For $i<n-1$ we have 
 \[ v_i = e_i-e_{i+1} = (e_i-e_n) - (e_{i+1}-e_n) = w_i-w_{i+1}, \]
 whereas $v_{n-1}$ is just equal to $w_{n-1}$.  This shows that
 $v_i\in W$ for all $i$, and it follows that $V\sse W$.  In the
 opposite direction, we have
 \[ v_i + v_{i+1} + \dotsb + v_{n-1} =
    (e_i-e_{i+1}) + (e_{i+1}-e_{i+2}) + \dotsb + (e_{n-1}-e_n) = 
    e_i - e_n = w_i,
 \]
 which shows that $w_i\in V$ for all $i$, and thus that $W\sse V$.  It
 follows that $W=V$, so the list $\CW=w_1,\dotsc,w_{n-1}$ spans $V$.
 The corresponding matrix $B=[w_1|\dotsb|w_{n-1}]^T$ is clearly in
 RREF, so $\CW$ is in fact the canonical basis for $V$.  It follows
 that $\dim(V)=n-1$ and $J(V)=\{1,2,\dotsc,n-1\}$.
\end{solution}

\begin{exercise}\label{ex-ann-canonical-i}
 Put $V=\ann(a_1,a_2,a_3)\sse\R^6$, where 
 \begin{align*}
  a_1 &= \bbm 1&1&2&3&3&2 \ebm^T \\
  a_2 &= \bbm 3&3&2&1&1&2 \ebm^T \\
  a_3 &= \bbm 0&0&1&1&1&1 \ebm^T.
 \end{align*}
 Find the canonical basis for $V$.
\end{exercise}
\begin{solution}
 The equations $a_3.x=a_2.x=a_1.x=0$ can be written as
 \begin{align*}
  x_6+x_5+x_4+x_3 &= 0 \\
  2x_6+x_5+x_4+2x_3+3x_2+x_1 &= 0 \\
  2x_6+3x_5+3x_4+2x_3+x_2+x_1 &= 0.
 \end{align*}
 The matrix $A$ on the left below is $[a_1|a_2|a_3]^T$; the matrix
 $A^*$ on the right is obtained by turning $A$ through $180^\circ$ and
 is the matrix of coefficients in the above system of equations.
 \[ 
  A   = \bbm 1&1&2&3&3&2 \\
             3&3&2&1&1&2 \\
             0&0&1&1&1&1 \ebm \hspace{4em}
  A^* = \bbm 1&1&1&1&0&0 \\
             2&1&1&2&3&3 \\
             2&3&3&2&1&1 \ebm.
 \]
 We can row-reduce $A^*$ as follows:
 \[ A^* \to 
     \bbm 1&1&1&1&0&0 \\
          0&-1&-1&0&3&3 \\
          0&1&1&0&1&1 \ebm
     \to 
     \bbm 1&0&0&1&3&3 \\
          0&1&1&0&-3&-3 \\
          0&0&0&0&4&4 \ebm
     \to 
     \bbm 1&0&0&1&0&0 \\
          0&1&1&0&0&0 \\
          0&0&0&0&1&1 \ebm
    = B^*
 \]
 The matrix $B^*$ corresponds to the system of equations 
 \begin{align*}
  x_6+x_3 &= 0 \\
  x_5+x_4 &= 0 \\
  x_2+x_1 &= 0,
 \end{align*}
 which can be rewritten as $x_6=-x_3$ and $x_5=-x_4$ and $x_2=-x_1$.
 This gives
 \[ x = \bbm x_1\\ x_2 \\ x_3 \\ x_4 \\ x_5 \\ x_6 \ebm 
      = \bbm x_1\\ -x_1 \\ x_3 \\ x_4 \\ -x_4 \\ -x_3 \ebm
      = x_1 \bbm 1 \\ -1 \\ 0 \\ 0 \\  0 \\  0 \ebm + 
        x_3 \bbm 0 \\  0 \\ 1 \\ 0 \\  0 \\ -1 \ebm + 
        x_4 \bbm 0 \\  0 \\ 0 \\ 1 \\ -1 \\  0 \ebm. 
 \]
 It follows that the vectors 
 \begin{align*}
  v_1 &= \bbm 1 & -1 & 0 & 0 &  0 &  0 \ebm^T \\
  v_2 &= \bbm 0 &  0 & 1 & 0 &  0 & -1 \ebm^T \\
  v_3 &= \bbm 0 &  0 & 0 & 1 & -1 &  0 \ebm^T 
 \end{align*}
 form the canonical basis for $V$.

 The calculation can be written more compactly in terms of
 Method~\ref{meth-ann-basis-matrix}.  The matrix $B^*$ has pivot 
 columns $1$, $2$ and $5$, and non-pivot columns $3$, $4$ and $6$.
 Deleting the pivot columns leaves the matrix
 \[ C^* =
    \left[\begin{array}{ccc}
     & c_1^T & \\ \hline
     & c_2^T & \\ \hline
     & c_3^T & 
    \end{array}\right]
    =
    \bbm 0&1&0 \\ 1&0&0 \\ 0&0&1 \ebm.
 \]
 We then construct the matrix
 \[ D^* = \bbm -c_1 & -c_2 & e_1 & e_2 & -c_3 & e_3 \ebm 
     = \bbm 
         0 & -1 &  1 &  0 &  0 &  0 \\
        -1 &  0 &  0 &  1 &  0 &  0 \\
         0 &  0 &  0 &  0 & -1 &  1
       \ebm 
 \]
 and rotate it to get 
 \[ D = 
     \bbm
      1 & -1 & 0 & 0 & 0 & 0 \\
      0 & 0 & 1 & 0 & 0 & -1 \\
      0 & 0 & 0 & 1 & -1 & 0
     \ebm.
 \]
 The canonical basis vectors $v_i$ appear as the rows of $D$.
\end{solution}

\begin{exercise}\label{ex-img-ker-canonical}
 Put 
 \[ A = \bbm 1&1&1&1 \\ 1&2&2&1 \\ 1&3&3&1 \\ 1&4&4&1 \ebm. \]
 Find the canonical basis for $\img(A)$, and the canonical basis for
 $\ker(A)$.  
\end{exercise}
\begin{solution}
 First, let $a_1,\dotsc,a_4$ be the columns of $A$.
 Proposition~\ref{prop-img-span} tellus us that
 $\img(A)=\spn(a_1,\dotsc,a_4)$.  To find the canonical basis for this
 space, Method~\ref{meth-span-canonical} tells us that we should form
 the matrix whose rows are $a_1^T,\dotsc,a_4^T$, but that matrix is
 just $A^T$.  We can row-reduce $A^T$ as follows:
 \[ 
   \bbm 1 & 1 & 1 & 1 \\
        1 & 2 & 3 & 4 \\
        1 & 2 & 3 & 4 \\
        1 & 1 & 1 & 1 \ebm \to
   \bbm 1 & 1 & 1 & 1 \\
        0 & 1 & 2 & 3 \\
        0 & 1 & 2 & 3 \\
        0 & 0 & 0 & 0 \ebm \to
   \bbm 1 & 0 &-1 &-2 \\
        0 & 1 & 2 & 3 \\
        0 & 0 & 0 & 0 \\
        0 & 0 & 0 & 0 \ebm
 \]
 By looking at the transposed rows of the final matrix, we see that
 the canonical basis for $\img(A)$ consists of the vectors
 \[ u_1 = \bbm 1 \\ 0 \\ -1 \\ -2 \ebm 
    \hspace{3em} \text{and} \hspace{3em}
    u_2 = \bbm 0 \\ 1 \\  2 \\  3 \ebm. 
 \]
 Next, we recall that $\ker(A)$ is the set of vectors $x$ that satisfy
 $Ax=0$.  After noting that 
 \[ \bbm 1&1&1&1 \\ 1&2&2&1 \\ 1&3&3&1 \\ 1&4&4&1 \ebm
    \bbm x_1\\ x_2\\ x_3\\ x_4 \ebm = 
    \bbm x_1+x_2+x_3+x_4 \\
         x_1+2x_2+2x_3+x_4 \\
         x_1+3x_2+3x_3+x_4 \\
         x_1+4x_2+4x_3+x_4 \ebm, 
 \]
 we see that $\ker(A)$ is the set of solutions to the equations
 \begin{align*}
  x_1+ x_2+ x_3+x_4   &= 0 \\
  x_1+2x_2+2x_3+x_4 &= 0 \\
  x_1+3x_2+3x_3+x_4 &= 0 \\
  x_1+4x_2+4x_3+x_4 &= 0.
 \end{align*}
 These are easily solved to give $x_4=-x_1$ and $x_3=-x_2$ with $x_1$
 and $x_2$ arbitrary.  (In order to get the canonical basis rather
 than any other basis, we need to write things this way around, with
 the higher-numbered variables on the left written in terms of the
 lower-numbered variables on the right.)  This gives 
 \[ x = \bbm x_1 \\ x_2 \\ -x_2 \\ -x_1 \ebm 
      = x_1 \bbm 1 \\ 0 \\ -1 \\  0 \ebm + 
        x_2 \bbm 0 \\ 1 \\  0 \\ -1 \ebm.
 \]
 From this we see that the canonical basis for $\ker(A)$ consists of
 the vectors 
 \[ v_1 = \bbm 1 \\ 0 \\ -1 \\ 0 \ebm 
    \hspace{3em} \text{and} \hspace{3em}
    v_2 = \bbm 0 \\ 1 \\  0 \\ -1 \ebm. 
 \]
\end{solution}

\section{Lecture 17}

\begin{exercise}\label{ex-plus-cap-i}
 Put 
 \[ v_1 = \bbm 1 \\ 3 \\ 5 \\ 3 \ebm 
    v_2 = \bbm 1 \\ 1 \\ 1 \\ -3 \ebm 
    w_1 = \bbm 1 \\ 2 \\ 3 \\ 4 \ebm
    w_2 = \bbm 3 \\ 2 \\ 1 \\ 0 \ebm
 \]
 and $V=\spn(v_1,v_2)$ and $W=\spn(w_1,w_2)$.
 \begin{itemize}
  \item[(a)] Find the canonical basis for $V+W$.
  \item[(b)] Find vectors $a_1$ and $a_2$ such that
   $V=\ann(a_1,a_2)$. 
  \item[(c)] Find vectors $b_1$ and $b_2$ such that
   $W=\ann(b_1,b_2)$. 
  \item[(d)] Find the canonical basis for $V\cap W$.
 \end{itemize}
\end{exercise}
\begin{solution}
 \begin{itemize}
  \item[(a)] We can row-reduce the matrix $[v_1|v_2|w_1|w_2]^T$ as
   follows: 
   \[ 
    \bbm 
     1 & 3 & 5 &  3 \\
     1 & 1 & 1 & -3 \\
     1 & 2 & 3 &  4 \\
     3 & 2 & 1 &  0
    \ebm
    \to
    \bbm 
     0 & 2 & 4 &  6 \\
     1 & 1 & 1 & -3 \\
     0 & 1 & 2 &  7 \\
     0 &-1 &-2 &  9
    \ebm
    \to
    \bbm 
     1 & 1 & 1 & -3 \\
     0 & 1 & 2 &  7 \\
     0 & 0 & 0 & -8 \\
     0 & 0 & 0 &  2
    \ebm
    \to
    \bbm 
     1 & 0 &-1 &  0 \\
     0 & 1 & 2 &  0 \\
     0 & 0 & 0 &  1 \\
     0 & 0 & 0 &  0
    \ebm
   \]
   We deduce that the vectors
   \[ 
    p_1 = \bbm 1&0&-1&0\ebm^T \hspace{3em}
    p_2 = \bbm 0&1&2&0\ebm^T \hspace{3em}
    p_3 = \bbm 0&0&0&1\ebm^T
   \]
   form the canonical basis for $V+W$.
  \item[(b)] The equations $x.v_2=x.v_1=0$ can be written as
   \begin{align*}
    -3x_4+x_3+x_2+x_1 &= 0 \\
    3x_4+5x_3+3x_2+x_1 &= 0.
   \end{align*}
   These can be solved in the usual way to give 
   $x_4=x_2/9+2x_1/9$ and $x_3=-2x_2/3-x_1/3$.  This in turn gives
   \[ x = \bbm x_1\\x_2\\x_3\\x_4 \ebm 
      = \bbm x_1\\x_2\\-2x_2/3-x_1/3 \\ x_2/9+2x_1/9 \ebm
      = x_1 \bbm 1 \\ 0 \\ -1/3 \\ 2/9 \ebm + 
        x_2 \bbm 0 \\ 1 \\ -2/3 \\ 1/9 \ebm.
   \]
   It follows that $V=\ann(a_1,a_2)$, where 
   \[ a_1 = \bbm 1&0&-1/3&2/9\ebm^T \hspace{4em}
      a_2 = \bbm 0&1&-2/3&1/9\ebm^T. 
   \]
  \item[(c)] The method is the same as for part~(b).  The equations
   $x.w_2=x.w_1=0$ can be written as
   \begin{align*}
    x_3 + 2x_2 + 3x_1 &= 0 \\
    4x_4+ 3x_3 + 2x_2 + x_1 &= 0 
   \end{align*}
   and these can be solved to give $x_4=x_2+2x_1$ and
   $x_3=-2x_2-3x_1$.  This in turn gives
   \[ x = \bbm x_1\\x_2\\x_3\\x_4 \ebm 
      = \bbm x_1 \\ x_2 \\ -2x_2-3x_1 \\ x_2+2x_1 \ebm 
      = x_1 \bbm 1 \\ 0 \\ -3 \\ 2 \ebm +
        x_2 \bbm 0 \\ 1 \\ -2 \\ 1 \ebm.
   \]
   It follows that $W=\ann(b_1,b_2)$, where 
   \[ b_1 = \bbm 1&0&-3&2\ebm^T \hspace{4em}
      b_2 = \bbm 0&1&-2&1\ebm^T. 
   \]
  \item[(d)] Now
   $V\cap W=\ann(a_1,a_2)\cap\ann(b_1,b_2)=\ann(a_1,a_2,b_1,b_2)$.  
   To save writing we will use the pure matrix method to calculate
   this.  The relevant matrix $A^*$ has rows consisting of the vectors
   $b_2$, $b_1$, $a_2$ and $a_1$ written backwards:
   \[ A^* =
       \bbm
        1 & -2 & 1 & 0 \\
        2 & -3 & 0 & 1 \\
        1/9 & -2/3 & 1 & 0 \\
        2/9 & -1/3 & 0 & 1
       \ebm
   \]
   This can be row-reduced as follows:
   \[ A^* \to 
       \bbm
        1 & -2 & 1 & 0 \\
        2 & -3 & 0 & 1 \\
        1 & -6 & 9 & 0 \\
        2 & -3 & 0 & 9
       \ebm
       \to
       \bbm
        1 & -2 & 1 & 0 \\
        0 &  1 &-2 & 1 \\
        0 & -4 & 8 & 0 \\
        0 &  1 &-2 & 9
       \ebm
       \to
       \bbm
        1 &  0 &-3 & 0 \\
        0 &  1 &-2 & 0 \\
        0 &  0 & 0 & 1 \\
        0 &  0 & 0 & 0
       \ebm  =B^*
   \]
   The matrix $B^*$ corresponds to the system of equations $x_4=3x_2$
   and $x_3=2x_2$ and $x_1=0$, so $x=x_2\bbm 0&1&2&3\ebm$.  It follows
   that $V\cap W$ is the set of multiples of the vector
   $q=\bbm 0&1&2&3\ebm^T$, so $q$ on its own is the canonical basis
   for $V\cap W$.
 \end{itemize}
\end{solution}

\begin{exercise}\label{ex-plus-cap-ii}
 Put 
 \begin{align*}
  U &= \{x\in\R^3 \st x_1+2x_2+2x_3 = 0 \} \\
  V &= \{x\in\R^3 \st 4x_1-x_2-x_3 = 0 \}.
 \end{align*}
 Find the canonical bases for $U$, $V$, $U+V$ and $U\cap V$.  
\end{exercise}
\begin{solution}
 First, we put $a=\bbm 1&2&2\ebm$ and $b=\bbm 4&-1&-1\ebm$.  We have
 $a.x=x_1+2x_2+2x_3$, so $U$ can be described as $U=\{x\st x.a=0\}$ or
 equivalently $U=\ann(a)$.  Similarly, we have $V=\ann(b)$.  

 For $x\in U$ we have $x_3=-x_1/2-x_2$, so 
 \[ x = \bbm x_1 \\ x_2 \\ -x_1/2-x_2 \ebm 
      = x_1\bbm 1\\0\\-1/2\ebm + x_2 \bbm 0\\ 1\\ -1\ebm.
 \]
 It follows that the vectors $u_1=\bbm 1&0&-1/2\ebm^T$ and
 $u_2=\bbm 0&1&-1\ebm^T$ form the canonical basis for $U$.

 Similarly, for $x\in V$ we have $x_3=4x_1-x_2$ so 
 \[ x = \bbm x_1 \\ x_2 \\ 4x_1-x_2 \ebm
      = x_1 \bbm 1 \\ 0 \\ 4 \ebm + x_2 \bbm 0 \\ 1 \\ -1 \ebm,
 \]
 so the vectors $v_1=\bbm 1&0&4\ebm^T$ and $v_2=\bbm 0&1&-1\ebm^T$
 form the canonical basis for $V$.

 It now follows that $U+V=\spn(u_1,u_2,v_1,v_2)$.  However, we can
 omit $v_2$ because it is the same as $u_2$, so
 $U+V=\spn(u_1,u_2,v_1)$.  To find the canonical basis for this space
 we row-reduce the matrix $[u_1|u_2|v_1]^T$:
 \[
   \bbm 
    1 & 0 & -1/2 \\
    0 & 1 & -1 \\
    1 & 0 & 4 
   \ebm
   \to 
   \bbm 
    1 & 0 & -1/2 \\
    0 & 1 & -1 \\
    0 & 0 & 9/2 
   \ebm
   \to 
   \bbm 
    1 & 0 & -1/2 \\
    0 & 1 & -1 \\
    0 & 0 &  1 
   \ebm
   \to 
   \bbm 
    1 & 0 &  0 \\
    0 & 1 &  0 \\
    0 & 0 &  1 
   \ebm
   = 
   \left[\begin{array}{ccc}
    & e_1^T & \\ \hline
    & e_2^T & \\ \hline
    & e_3^T & 
   \end{array}\right]
 \]
 It follows that $e_1,e_2,e_3$ is the canonical basis for $U+V$ and so
 $U+V=\R^3$.  

 The dimension formula now gives
 \[ \dim(U\cap V) = \dim(U)+\dim(V)-\dim(U+V) = 2+2-3 = 1. \]
 It follows that any nonzero vector in $U\cap V$ (considered as a list
 of length one) forms a basis for $U\cap V$.  We have seen that the
 vector $w=\bbm 0&1&-1\ebm^T=u_2=v_2$ lies in both $U$ and $V$, so it
 forms a basis for $U\cap V$.  The first nonzero entry in $w$ is one,
 so this is the canonical basis.

 For a more direct approach, we can use the fact that 
 \[ U\cap V = \ann(a)\cap\ann(b) = \ann(a,b). \]
 The equations $x.b=x.a=0$ can be written with the variables in
 decreasing order as
 \begin{align*}
  2x_3+2x_2+x_1 &= 0 \\
  -x_3-x_2+4x_1 &= 0.
 \end{align*}
 These equations can be solved to give $x_3=-x_2$ and $x_1=0$, so
 \[ x = \bbm 0 \\ x_2 \\ -x_2 \ebm =
      x_2 \bbm 0 \\ 1 \\ -1 \ebm = x_2w.
 \]
 Form this we again see that $w$ is the canonical basis for $U\cap V$.
\end{solution}

\begin{exercise}\label{ex-span-canonical-ii}
 Let $V$ be the set of all vectors of the form 
 \[ v = \bbm p+q & 2p-2q & 3p+3q & 4p-4q \ebm^T. \]
 \begin{itemize}
  \item[(a)] Find vectors $v_1$ and $v_2$ such that
   $V=\spn(v_1,v_2)$. 
  \item[(b)] Find vectors $w_1$ and $w_2$ such that
   $V=\ann(w_1,w_2)$. 
 \end{itemize}
\end{exercise}
\begin{solution}
 \begin{itemize}
  \item[(a)] A general element $v\in V$ can be written as
   \[ v = \bbm p+q \\ 2p-2q \\ 3p+3q \\ 4p-4q \ebm
        = p \bbm 1\\2\\3\\4 \ebm + q \bbm 1\\-2\\3\\-4\ebm. 
   \]
   It follows that if we put $v_1=\bbm 1&2&3&4\ebm^T$ and
   $v_2=\bbm 1&-2&3&-4\ebm^T$ then the elements of $V$ are precisely
   the linear combinations of $v_1$ and $v_2$, or in other words
   $V=\spn(v_1,v_2)$.  

   If we want we can tidy this up by row-reduction:
   \[ [v_1|v_2]^T = 
      \bbm 1 & 2 & 3 & 4 \\ 1 & -2 & 3 & -4 \ebm \to
      \bbm 1 & 2 & 3 & 4 \\ 0 & -4 & 0 & -8 \ebm \to
      \bbm 1 & 2 & 3 & 4 \\ 0 &  1 & 0 &  2 \ebm \to
      \bbm 1 & 0 & 3 & 0 \\ 0 &  1 & 0 &  2 \ebm.
   \]
   It follows that $V$ can also be described as $\spn(v'_1,v'_2)$,
   where $v'_1=\bbm 1&0&3&0\ebm^T$ and $v'_2=\bbm 0&1&0&2\ebm^T$.
   (In fact, $v'_1$ and $v'_2$ form the canonical basis for $V$.)
  \item[(b)] The equations $x.v_2=0$ and $x.v_1=0$ can be written as 
   \begin{align*}
    -4x_4+3x_3-2x_2+x_1 &= 0 \\
     4x_4+3x_3+2x_2+x_1 &= 0.
   \end{align*}
   By adding the above equations we get $6x_2+2x_1=0$ or
   $x_3=-x_1/3$.  By subtracting the above equations we get
   $8x_4+4x_2=0$ or $x_4=-x_2/2$.  This gives
   \[ x = \bbm x_1\\x_2\\x_3\\x_4\ebm 
        = \bbm x_1\\x_2\\-x_1/3\\-x_2/2\ebm 
        = x_1 \bbm 1\\0\\-1/3\\0\ebm + 
          x_2 \bbm 0\\1\\0\\-1/2\ebm.
   \]
   It follows that $V=\ann(w_1,w_2)$, where
   $w_1=\bbm 1&0&-1/3&0\ebm^T$ and $w_2=\bbm 0&1&0&-1/2\ebm$.

   Note that we could also have started with the equations
   $x.v'_2=x.v'_1=0$ instead of $x.v_2=x.v_1=0$ and we would still
   have obtained the same vectors $w_i$.
 \end{itemize}
\end{solution}

\begin{exercise}\label{ex-dim-formula-i}
 For each of the following configurations, either find an example, or
 show that no example is possible.
 \begin{itemize}
  \item[(a)] Subspaces $U,V\leq\R^4$ with $\dim(U)=\dim(V)=3$ and
   $\dim(U\cap V)=1$.
  \item[(b)] Subspaces $U,V\leq\R^4$ with $\dim(U)=\dim(V)=3$ and
   $\dim(U\cap V)=2$.
  \item[(c)] Subspaces $U,V\leq\R^5$ with $\dim(U)=\dim(V)=2$ and
   $\dim(U+V)=5$.
  \item[(d)] Subspaces $U,V\leq\R^3$ with
   $\dim(U)=\dim(V)=\dim(U+V)=\dim(U\cap V)$.
 \end{itemize}
\end{exercise}
\begin{solution}
 We will repeatedly use the dimension formula 
 \[ \dim(U)+\dim(V) = \dim(U+V) + \dim(U\cap V). \]
 \begin{itemize}
  \item[(a)] This is not possible.  Indeed, the dimension formula can
   be rearranged to give
   $\dim(U+V)=\dim(U)+\dim(V)-\dim(U\cap V)=3+3-1=5$, but $U+V$ is a
   subspace of $\R^4$, so it cannot have dimension greater than $4$.
  \item[(b)] The simplest example is  
    \begin{align*}
     U &= \spn(e_1,e_2,e_3)
        = \{\bbm w & x & y & 0 \ebm^T\st w,x,y\in\R\} \\
     V &= \spn(e_1,e_2,e_4)
        = \{\bbm w & x & 0 & z \ebm^T\st w,x,z\in\R\} \\
     U\cap V &= \spn(e_1,e_2)
        = \{\bbm w & x & 0 & 0 \ebm^T\st w,x\in\R\}.
    \end{align*}
   \item[(c)] This is not possible.  Indeed, the dimension formula can
    be rearranged to give
    $\dim(U\cap V)=\dim(U)+\dim(V)-\dim(U+V)=2+2-5=-1$, but no subspace
    can have negative dimension.
   \item[(d)] The minimal example here is to take $U=V=\{0\}$, so
    $U+V=U\cap V=\{0\}$ and $\dim(U)=\dim(V)=\dim(U+V)=\dim(U\cap V)=0$.
    More generally, we can choose $U$ to be any subspace of $\R^3$ (of
    dimension $d$, say) and take $V=U$.  We then have $U+V=U+U=U$ and
    $U\cap V=U\cap U=U$ so $\dim(U)=\dim(V)=\dim(U+V)=\dim(U\cap V)=d$.
 \end{itemize}
\end{solution}

\section{Lecture 18}

\begin{exercise}\label{ex-rank-i}
 Find the ranks of the following matrices:
 \[ 
  A = \bbm
   0& 1& 2\\
  -1& 0& 3\\
  -2&-3& 0\\
  \ebm \qquad
  B = \bbm
   1&2&3&4&5&6\\
   2&3&4&5&6&7\\
   3&4&5&6&7&8\\
  \ebm \qquad
  C = \bbm
   1&10&100\\
   10&100&1000\\
   100&1000&10000\\
  \ebm \qquad
  D = \bbm
   1&0&1&1&1\\
   1&0&0&0&1\\
   0&0&1&1&0\\
   0&1&0&0&0\\
   0&1&1&1&0\\
  \ebm
 \]
\end{exercise}
\begin{solution}
 The rank of a matrix $M$ is the number of nonzero rows in the
 row-reduced form of $M$.  We have row-reductions as follows:
 \[ A = 
   \bbm
   0&1&2\\
   -1&0&3\\
   -2&-3&0\\
   \ebm
   \to
   \bbm
   0&1&2\\
   1&0&-3\\
   -2&-3&0\\
   \ebm
   \to
   \bbm
   0&1&2\\
   1&0&-3\\
   0&-3&-6\\
   \ebm
   \to
   \bbm
   1&0&-3\\
   0&1&2\\
   0&-3&-6\\
   \ebm
   \to
   \bbm
   1&0&-3\\
   0&1&2\\
   0&0&0\\
   \ebm
 \]
 \[ B = 
  \bbm
  1&2&3&4&5&6\\
  2&3&4&5&6&7\\
  3&4&5&6&7&8\\
  \ebm
  \to
  \bbm
  1&2&3&4&5&6\\
  0&-1&-2&-3&-4&-5\\
  0&-2&-4&-6&-8&-10\\
  \ebm
  \to
 \] \[
  \bbm
  1&2&3&4&5&6\\
  0&1&2&3&4&5\\
  0&-2&-4&-6&-8&-10\\
  \ebm
  \to
  \bbm
  1&0&-1&-2&-3&-4\\
  0&1&2&3&4&5\\
  0&0&0&0&0&0\\
  \ebm
 \]
 \[ C = 
  \bbm
   1   &   10 &   100 \\
   10  &  100 &  1000 \\
   100 & 1000 & 10000
  \ebm
  \to
  \bbm
   1   &   10 &   100 \\
   0   &    0 &     0 \\
   100 & 1000 & 10000
  \ebm
  \to
  \bbm
   1 & 10 & 100 \\
   0 &  0 & 0   \\
   0 &  0 & 0
  \ebm
 \]
 \[ D = 
  \bbm
  1&0&1&1&1\\
  1&0&0&0&1\\
  0&0&1&1&0\\
  0&1&0&0&0\\
  0&1&1&1&0\\
  \ebm
  \to
  \bbm
  1&0&1&1&1\\
  0&0&-1&-1&0\\
  0&0&1&1&0\\
  0&1&0&0&0\\
  0&1&1&1&0\\
  \ebm
  \to
  \bbm
  1&0&1&1&1\\
  0&0&-1&-1&0\\
  0&0&1&1&0\\
  0&1&0&0&0\\
  0&0&1&1&0\\
  \ebm
  \to
  \bbm
  1&0&0&0&1\\
  0&0&0&0&0\\
  0&0&1&1&0\\
  0&1&0&0&0\\
  0&0&0&0&0\\
  \ebm
  \to
  \bbm
  1&0&0&0&1\\
  0&1&0&0&0\\
  0&0&1&1&0\\
  0&0&0&0&0\\
  0&0&0&0&0\\
  \ebm
 \]
 From this we see that $\rnk(A)=\rnk(B)=2$ and $\rnk(C)=1$ and $\rnk(D)=3$.
\end{solution}

\begin{exercise}\label{ex-rank-ii}
 Give examples as follows, or explain why no such examples are
 possible. 
 \begin{itemize}
  \item[(a)] A $3\tm 5$ matrix of rank $4$.
  \item[(b)] A $3\tm 3$ matrix of rank $1$, in which none of the
   entries are zero.
  \item[(c)] A $2\tm 4$ matrix $A$ such that $A$ has rank $1$ and
   $A^T$ has rank $2$.
  \item[(d)] A $3\tm 3$ matrix $A$ such that $A+A^T=0$ and $A$ has
   rank $2$.
  \item[(e)] An invertible $3\tm 3$ matrix of rank $2$.
  \item[(f)] A matrix in RREF with rank $1$ and $4$ nonzero columns. 
 \end{itemize}
\end{exercise}
\begin{solution}
 \begin{itemize}
  \item[(a)] This is not possible, because the rank of any $m\tm n$
   matrix is at most the minimum of $n$ and $m$, so a $3\tm 5$ matrix
   cannot have rank larger than $3$.
  \item[(b)] The simplest example is $A=\bbm 1&1&1\\1&1&1\\1&1&1\ebm$.
  \item[(c)] This is not possible, because $A$ and $A^T$ always have
   the same rank.
  \item[(d)] The simplest example is $A=\bbm 0&0&1\\0&0&0\\-1&0&0\ebm$.
  \item[(e)] This is not possible.  If $A$ is an \emph{invertible}
   $n\tm n$ matrix, then the columns form a basis for $\R^n$, which
   means that the rank must be $n$.
  \item[(f)] One example is the matrix $\bbm 1&1&1&1 \\ 0&0&0&0\ebm$.
 \end{itemize}
\end{solution}

\begin{exercise}\label{ex-rank-iii}
 Consider the following matrices, which depend on a parameter $t$. 
 \[ A = \bbm 1 & 0 \\ 
             0 & (t-3)(t-4) \ebm \hspace{3em}
    B = \bbm 1 & t \\
             t & 2t-1 \ebm \hspace{3em}
    C = \bbm 1 & 1 & 1 \\
             1 & 2 & t \\
             1 & 4 & t^2 \ebm \hspace{3em}
    D = \bbm 1 & 1 & 1   & 1 & 0 \\
             1 & 2 & t   & 3 & t \\
             1 & 4 & t^2 & 7 & 3 \ebm
 \]
 It should be clear that $A$ usually has rank two, except that when
 $t=3$ or $t=4$ the second row becomes zero and so the rank is only
 one.  In the same way, for each of the other matrices, there is a
 usual value for the rank, but the rank drops for some exceptional
 values of $t$.
 \begin{itemize}
  \item[(1)] Simplify $B$ by row and column operations.  Do not divide
   any row or column by anything that depends on $t$, but make $B$ as
   simple as you can without such divisions.
  \item[(2)] What is the usual rank of $B$?
  \item[(3)] What is the exceptional value of $t$ for which the rank
   of $B$ is lower?  What is the rank in that case?
  \item[(4)] What is the usual rank of $C$, and what are the
   exceptional cases?  (Use the same method as for $B$.)
  \item[(5)] What is the usual rank of $D$, and what are the
   exceptional cases?  (\textbf{Hint:} how is $D$ related to $C$?)
 \end{itemize}
\end{exercise}
\begin{solution}
 \begin{itemize}
  \item[(1)] Subtract $t$ times the first row from the second row,
   then subtract $t$ times the first column from the second column:
   \[ B =
      \bbm 1&t \\ t&2t-1 \ebm \to 
      \bbm 1&t \\ 0 & -t^2+2t-1 \ebm \to
      \bbm 1&0 \\ 0 & -t^2+2t-1 \ebm = 
      \bbm 1&0 \\ 0 & -(t-1)^2 \ebm = B'.
   \]
   We might now be tempted to divide the second row by $-(t-1)^2$ to
   get the identity matrix.  However, that would not be valid when
   $t=1$, because then we would be dividing by zero.  It is for this
   reason that the question tells you not to divide by anyhing that
   depends on $t$.
  \item[(2)] As row and column operations do not affect the rank, we
   have $\rnk(B)=\rnk(B')$.  If $t\neq 1$ then it is clear that the
   two rows in $B'$ are linearly independent and so
   $\rnk(B)=\rnk(B')=2$; this is the usual case.
  \item[(3)] In the exceptional case where $t=1$ we have
   $B'=\bbm 1&0\\0&0\ebm$ and it is clear that $\rnk(B)=\rnk(B')=1$.
  \item[(4)] We can simplify $C$ by row and column operations as
   follows. 
   \[ C = 
      \bbm 1 & 1 & 1 \\
           1 & 2 & t \\
           1 & 4 & t^2 \ebm \xra{1}
      \bbm 1 & 1 & 1 \\
           0 & 1 & t-1 \\
           0 & 3 & t^2-1 \ebm \xra{2}
      \bbm 1 & 0 & 0 \\
           0 & 1 & t-1 \\
           0 & 3 & t^2-1 \ebm \xra{3}
      \bbm 1 & 0 & 0 \\
           0 & 1 & 0 \\
           0 & 3 & t^2-3t+2 \ebm \xra{4}
      \bbm 1 & 0 & 0 \\
           0 & 1 & 0 \\
           0 & 0 & t^2-3t+2 \ebm = C'
   \]
   (Step $1$: subtract row $1$ from the other two rows; Step $2$:
   subtract column $1$ from the other two columns; Step $3$: add
   $1-t$ times column $2$ to column $3$; Step $4$: subtract $3$ times
   row $2$ from row $3$.)  Note also that $t^2-3t+2=(t-1)(t-2)$.  For
   most values of $t$ this will be nonzero, so $\rnk(C)=\rnk(C')=3$.
   The exceptional cases are where $t=1$ or $t=2$, in which case
   $C'=\bsm 1&0&0 \\ 0&1&0\\ 0&0&0\esm$ and $\rnk(C)=\rnk(C')=2$.
  \item[(5)] $C$ consists of the first three columns of $D$.  If
   $t\neq 1,2$ then $\rnk(C)=3$ so the columns of $C$ span $\R^3$, so
   the columns of $D$ certainly span $\R^3$, so $\rnk(D)=3$.  In the
   case $t=1$ we can write down $D$ and simplify by column operations
   as follows:
   \[ D = \bbm 1 & 1 & 1 & 1 & 0 \\
               1 & 2 & 1 & 3 & 1 \\
               1 & 4 & 1 & 7 & 3 \ebm \to
          \bbm 1 & 0 & 0 & 0 & 0 \\
               1 & 1 & 0 & 2 & 1 \\
               1 & 3 & 0 & 6 & 3 \ebm \to
          \bbm 1 & 0 & 0 & 0 & 0 \\
               0 & 1 & 0 & 0 & 0 \\
              -2 & 3 & 0 & 0 & 0 \ebm
        = D'.
   \]
   It is clear that in this case we have $\rnk(D)=\rnk(D')=2$.  In the
   other exceptional case where $t=2$ we can write down $D$ and
   simplify by column operations as follows:
   \[ D = \bbm 1 & 1 & 1 & 1 & 0 \\
               1 & 2 & 2 & 3 & 2 \\
               1 & 4 & 4 & 7 & 3 \ebm \to
          \bbm 1 & 0 & 0 & 0 & 0 \\
               1 & 1 & 0 & 2 & 2 \\
               1 & 3 & 0 & 6 & 3 \ebm \to
          \bbm 1 & 0 & 0 & 0 & 0 \\
               1 & 1 & 0 & 0 & 0 \\
               1 & 3 & 0 & 0 &-3 \ebm \to
          \bbm 1 & 0 & 0 & 0 & 0 \\
               0 & 1 & 0 & 0 & 0 \\
               0 & 0 & 1 & 0 & 0 \ebm
        = D''.
   \]
   It is clear from this that the case $t=2$ is not in fact
   exceptional for $D$, because we have $\rnk(D)=\rnk(D'')=3$ in that
   case (which is the same answer as for every other value of $t$
   except $t=1$).
 \end{itemize}
\end{solution}



\end{document}